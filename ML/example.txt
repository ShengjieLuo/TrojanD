2017-05-04 08:53:32,862 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Running Spark version 2.1.0
2017-05-04 08:53:33,291 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-05-04 08:53:33,404 WARN  [main] spark.SparkConf (Logging.scala:logWarning(66)) - 
SPARK_CLASSPATH was detected (set to ':/usr/local/hive/lib/mysql-connector-java-5.1.41-bin.jar:/usr/local/spark/lib/drools/*:/usr/local/spark/lib/protobuf/*:/usr/local/hive/lib/mysql-connector-java-5.1.41-bin.jar:/usr/local/spark/lib/drools/*:/usr/local/spark/lib/protobuf/*').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
2017-05-04 08:53:33,405 WARN  [main] spark.SparkConf (Logging.scala:logWarning(66)) - Setting 'spark.executor.extraClassPath' to ':/usr/local/hive/lib/mysql-connector-java-5.1.41-bin.jar:/usr/local/spark/lib/drools/*:/usr/local/spark/lib/protobuf/*:/usr/local/hive/lib/mysql-connector-java-5.1.41-bin.jar:/usr/local/spark/lib/drools/*:/usr/local/spark/lib/protobuf/*' as a work-around.
2017-05-04 08:53:33,405 WARN  [main] spark.SparkConf (Logging.scala:logWarning(66)) - Setting 'spark.driver.extraClassPath' to ':/usr/local/hive/lib/mysql-connector-java-5.1.41-bin.jar:/usr/local/spark/lib/drools/*:/usr/local/spark/lib/protobuf/*:/usr/local/hive/lib/mysql-connector-java-5.1.41-bin.jar:/usr/local/spark/lib/drools/*:/usr/local/spark/lib/protobuf/*' as a work-around.
2017-05-04 08:53:33,456 INFO  [main] spark.SecurityManager (Logging.scala:logInfo(54)) - Changing view acls to: root
2017-05-04 08:53:33,457 INFO  [main] spark.SecurityManager (Logging.scala:logInfo(54)) - Changing modify acls to: root
2017-05-04 08:53:33,458 INFO  [main] spark.SecurityManager (Logging.scala:logInfo(54)) - Changing view acls groups to: 
2017-05-04 08:53:33,458 INFO  [main] spark.SecurityManager (Logging.scala:logInfo(54)) - Changing modify acls groups to: 
2017-05-04 08:53:33,459 INFO  [main] spark.SecurityManager (Logging.scala:logInfo(54)) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2017-05-04 08:53:33,849 INFO  [main] util.Utils (Logging.scala:logInfo(54)) - Successfully started service 'sparkDriver' on port 40766.
2017-05-04 08:53:33,874 INFO  [main] spark.SparkEnv (Logging.scala:logInfo(54)) - Registering MapOutputTracker
2017-05-04 08:53:33,906 INFO  [main] spark.SparkEnv (Logging.scala:logInfo(54)) - Registering BlockManagerMaster
2017-05-04 08:53:33,910 INFO  [main] storage.BlockManagerMasterEndpoint (Logging.scala:logInfo(54)) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-05-04 08:53:33,910 INFO  [main] storage.BlockManagerMasterEndpoint (Logging.scala:logInfo(54)) - BlockManagerMasterEndpoint up
2017-05-04 08:53:33,934 INFO  [main] storage.DiskBlockManager (Logging.scala:logInfo(54)) - Created local directory at /tmp/blockmgr-cd53068b-4eee-4c4f-80d5-3e293ab4c720
2017-05-04 08:53:34,027 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - MemoryStore started with capacity 366.3 MB
2017-05-04 08:53:34,123 INFO  [main] spark.SparkEnv (Logging.scala:logInfo(54)) - Registering OutputCommitCoordinator
2017-05-04 08:53:34,218 INFO  [main] util.log (Log.java:initialized(186)) - Logging initialized @2299ms
2017-05-04 08:53:34,335 INFO  [main] server.Server (Server.java:doStart(327)) - jetty-9.2.z-SNAPSHOT
2017-05-04 08:53:34,364 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@39c11e6c{/jobs,null,AVAILABLE}
2017-05-04 08:53:34,365 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@324dcd31{/jobs/json,null,AVAILABLE}
2017-05-04 08:53:34,365 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@503d56b5{/jobs/job,null,AVAILABLE}
2017-05-04 08:53:34,366 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@72bca894{/jobs/job/json,null,AVAILABLE}
2017-05-04 08:53:34,366 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@433ffad1{/stages,null,AVAILABLE}
2017-05-04 08:53:34,367 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@1fc793c2{/stages/json,null,AVAILABLE}
2017-05-04 08:53:34,368 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@2575f671{/stages/stage,null,AVAILABLE}
2017-05-04 08:53:34,368 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@329a1243{/stages/stage/json,null,AVAILABLE}
2017-05-04 08:53:34,369 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@ecf9fb3{/stages/pool,null,AVAILABLE}
2017-05-04 08:53:34,369 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@2d35442b{/stages/pool/json,null,AVAILABLE}
2017-05-04 08:53:34,370 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@27f9e982{/storage,null,AVAILABLE}
2017-05-04 08:53:34,370 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@4593ff34{/storage/json,null,AVAILABLE}
2017-05-04 08:53:34,371 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@37d3d232{/storage/rdd,null,AVAILABLE}
2017-05-04 08:53:34,371 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@30c0ccff{/storage/rdd/json,null,AVAILABLE}
2017-05-04 08:53:34,372 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@581d969c{/environment,null,AVAILABLE}
2017-05-04 08:53:34,372 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@22db8f4{/environment/json,null,AVAILABLE}
2017-05-04 08:53:34,373 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@2b46a8c1{/executors,null,AVAILABLE}
2017-05-04 08:53:34,373 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@1d572e62{/executors/json,null,AVAILABLE}
2017-05-04 08:53:34,374 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@29caf222{/executors/threadDump,null,AVAILABLE}
2017-05-04 08:53:34,374 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@46cf05f7{/executors/threadDump/json,null,AVAILABLE}
2017-05-04 08:53:34,385 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@5851bd4f{/static,null,AVAILABLE}
2017-05-04 08:53:34,385 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@7cd1ac19{/,null,AVAILABLE}
2017-05-04 08:53:34,386 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@2f40a43{/api,null,AVAILABLE}
2017-05-04 08:53:34,387 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@3caa4757{/jobs/job/kill,null,AVAILABLE}
2017-05-04 08:53:34,387 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@69c43e48{/stages/stage/kill,null,AVAILABLE}
2017-05-04 08:53:34,396 INFO  [main] server.ServerConnector (AbstractConnector.java:doStart(266)) - Started ServerConnector@1a38ba58{HTTP/1.1}{0.0.0.0:4040}
2017-05-04 08:53:34,397 INFO  [main] server.Server (Server.java:doStart(379)) - Started @2479ms
2017-05-04 08:53:34,397 INFO  [main] util.Utils (Logging.scala:logInfo(54)) - Successfully started service 'SparkUI' on port 4040.
2017-05-04 08:53:34,400 INFO  [main] ui.SparkUI (Logging.scala:logInfo(54)) - Bound SparkUI to 0.0.0.0, and started at http://192.168.0.101:4040
2017-05-04 08:53:34,442 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Added JAR file:/usr/local/TrojanD/ML/target/scala-2.11/simple-project_2.11-1.0.jar at spark://192.168.0.101:40766/jars/simple-project_2.11-1.0.jar with timestamp 1493902414441
2017-05-04 08:53:34,534 INFO  [main] executor.Executor (Logging.scala:logInfo(54)) - Starting executor ID driver on host localhost
2017-05-04 08:53:34,558 INFO  [main] util.Utils (Logging.scala:logInfo(54)) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42487.
2017-05-04 08:53:34,558 INFO  [main] netty.NettyBlockTransferService (Logging.scala:logInfo(54)) - Server created on 192.168.0.101:42487
2017-05-04 08:53:34,560 INFO  [main] storage.BlockManager (Logging.scala:logInfo(54)) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-05-04 08:53:34,562 INFO  [main] storage.BlockManagerMaster (Logging.scala:logInfo(54)) - Registering BlockManager BlockManagerId(driver, 192.168.0.101, 42487, None)
2017-05-04 08:53:34,565 INFO  [dispatcher-event-loop-10] storage.BlockManagerMasterEndpoint (Logging.scala:logInfo(54)) - Registering block manager 192.168.0.101:42487 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.101, 42487, None)
2017-05-04 08:53:34,572 INFO  [main] storage.BlockManagerMaster (Logging.scala:logInfo(54)) - Registered BlockManager BlockManagerId(driver, 192.168.0.101, 42487, None)
2017-05-04 08:53:34,573 INFO  [main] storage.BlockManager (Logging.scala:logInfo(54)) - Initialized BlockManager: BlockManagerId(driver, 192.168.0.101, 42487, None)
2017-05-04 08:53:34,783 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@3703bf3c{/metrics/json,null,AVAILABLE}
2017-05-04 08:53:35,482 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_0 stored as values in memory (estimated size 236.5 KB, free 366.1 MB)
2017-05-04 08:53:35,534 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
2017-05-04 08:53:35,537 INFO  [dispatcher-event-loop-12] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_0_piece0 in memory on 192.168.0.101:42487 (size: 22.9 KB, free: 366.3 MB)
2017-05-04 08:53:35,543 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 0 from textFile at KmeansMLlibTrain.scala:20
2017-05-04 08:53:35,696 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(249)) - Total input paths to process : 1
2017-05-04 08:53:35,714 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: takeSample at KMeans.scala:353
2017-05-04 08:53:35,799 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 0 (takeSample at KMeans.scala:353) with 2 output partitions
2017-05-04 08:53:35,800 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 0 (takeSample at KMeans.scala:353)
2017-05-04 08:53:35,801 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:35,808 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:35,823 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224), which has no missing parents
2017-05-04 08:53:35,894 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_1 stored as values in memory (estimated size 4.1 KB, free 366.0 MB)
2017-05-04 08:53:35,896 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 366.0 MB)
2017-05-04 08:53:35,897 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_1_piece0 in memory on 192.168.0.101:42487 (size: 2.4 KB, free: 366.3 MB)
2017-05-04 08:53:35,898 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 1 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:35,903 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at KMeans.scala:224)
2017-05-04 08:53:35,906 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 0.0 with 2 tasks
2017-05-04 08:53:35,984 INFO  [dispatcher-event-loop-16] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6372 bytes)
2017-05-04 08:53:35,989 INFO  [dispatcher-event-loop-16] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6372 bytes)
2017-05-04 08:53:35,999 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 0.0 (TID 0)
2017-05-04 08:53:35,999 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 0.0 (TID 1)
2017-05-04 08:53:36,011 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Fetching spark://192.168.0.101:40766/jars/simple-project_2.11-1.0.jar with timestamp 1493902414441
2017-05-04 08:53:36,114 INFO  [Executor task launch worker-0] client.TransportClientFactory (TransportClientFactory.java:createClient(250)) - Successfully created connection to /192.168.0.101:40766 after 49 ms (0 ms spent in bootstraps)
2017-05-04 08:53:36,127 INFO  [Executor task launch worker-0] util.Utils (Logging.scala:logInfo(54)) - Fetching spark://192.168.0.101:40766/jars/simple-project_2.11-1.0.jar to /tmp/spark-c0c6e657-a403-4866-9dfd-f94efbc27ff4/userFiles-724794a1-5e9c-4d86-b01a-9ec69fbde586/fetchFileTemp6717433856562675445.tmp
2017-05-04 08:53:36,214 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Adding file:/tmp/spark-c0c6e657-a403-4866-9dfd-f94efbc27ff4/userFiles-724794a1-5e9c-4d86-b01a-9ec69fbde586/simple-project_2.11-1.0.jar to class loader
2017-05-04 08:53:36,291 INFO  [Executor task launch worker-1] rdd.HadoopRDD (Logging.scala:logInfo(54)) - Input split: file:/usr/local/TrojanD/sample/MLlib.txt:1382329+1382330
2017-05-04 08:53:36,291 INFO  [Executor task launch worker-0] rdd.HadoopRDD (Logging.scala:logInfo(54)) - Input split: file:/usr/local/TrojanD/sample/MLlib.txt:0+1382329
2017-05-04 08:53:36,314 INFO  [Executor task launch worker-1] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2017-05-04 08:53:36,314 INFO  [Executor task launch worker-0] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2017-05-04 08:53:36,315 INFO  [Executor task launch worker-1] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2017-05-04 08:53:36,315 INFO  [Executor task launch worker-0] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2017-05-04 08:53:36,315 INFO  [Executor task launch worker-1] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2017-05-04 08:53:36,572 INFO  [Executor task launch worker-1] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_2_1 stored as values in memory (estimated size 952.4 KB, free 365.1 MB)
2017-05-04 08:53:36,573 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_2_1 in memory on 192.168.0.101:42487 (size: 952.4 KB, free: 365.3 MB)
2017-05-04 08:53:36,575 INFO  [Executor task launch worker-0] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_2_0 stored as values in memory (estimated size 952.4 KB, free 364.2 MB)
2017-05-04 08:53:36,575 INFO  [dispatcher-event-loop-29] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_2_0 in memory on 192.168.0.101:42487 (size: 952.4 KB, free: 364.4 MB)
2017-05-04 08:53:36,579 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:36,579 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:36,603 INFO  [Executor task launch worker-0] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_3_0 stored as values in memory (estimated size 83.0 KB, free 364.0 MB)
2017-05-04 08:53:36,603 INFO  [Executor task launch worker-1] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_3_1 stored as values in memory (estimated size 83.0 KB, free 364.0 MB)
2017-05-04 08:53:36,604 INFO  [dispatcher-event-loop-32] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_3_0 in memory on 192.168.0.101:42487 (size: 83.0 KB, free: 364.3 MB)
2017-05-04 08:53:36,605 INFO  [dispatcher-event-loop-32] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_3_1 in memory on 192.168.0.101:42487 (size: 83.0 KB, free: 364.3 MB)
2017-05-04 08:53:36,647 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 0.0 (TID 0). 1979 bytes result sent to driver
2017-05-04 08:53:36,647 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 0.0 (TID 1). 1979 bytes result sent to driver
2017-05-04 08:53:36,663 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 0.0 (TID 0) in 714 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:36,663 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 0.0 (TID 1) in 677 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:36,665 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-05-04 08:53:36,669 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 0 (takeSample at KMeans.scala:353) finished in 0.747 s
2017-05-04 08:53:36,678 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 0 finished: takeSample at KMeans.scala:353, took 0.963828 s
2017-05-04 08:53:36,712 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: takeSample at KMeans.scala:353
2017-05-04 08:53:36,713 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 1 (takeSample at KMeans.scala:353) with 2 output partitions
2017-05-04 08:53:36,713 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 1 (takeSample at KMeans.scala:353)
2017-05-04 08:53:36,714 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:36,717 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:36,718 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353), which has no missing parents
2017-05-04 08:53:36,723 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 364.0 MB)
2017-05-04 08:53:36,727 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 364.0 MB)
2017-05-04 08:53:36,728 INFO  [dispatcher-event-loop-38] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_2_piece0 in memory on 192.168.0.101:42487 (size: 2.7 KB, free: 364.3 MB)
2017-05-04 08:53:36,729 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 2 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:36,729 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 1 (PartitionwiseSampledRDD[7] at takeSample at KMeans.scala:353)
2017-05-04 08:53:36,729 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 1.0 with 2 tasks
2017-05-04 08:53:36,740 INFO  [dispatcher-event-loop-39] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 6481 bytes)
2017-05-04 08:53:36,743 INFO  [dispatcher-event-loop-39] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 6481 bytes)
2017-05-04 08:53:36,743 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 1.0 (TID 3)
2017-05-04 08:53:36,744 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 1.0 (TID 2)
2017-05-04 08:53:36,754 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:36,754 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:36,754 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:36,754 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:36,781 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 1.0 (TID 2). 2264 bytes result sent to driver
2017-05-04 08:53:36,781 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 1.0 (TID 3). 2264 bytes result sent to driver
2017-05-04 08:53:36,783 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 1.0 (TID 2) in 48 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:36,784 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 1.0 (TID 3) in 43 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:36,784 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-05-04 08:53:36,785 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 1 (takeSample at KMeans.scala:353) finished in 0.051 s
2017-05-04 08:53:36,785 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 1 finished: takeSample at KMeans.scala:353, took 0.073368 s
2017-05-04 08:53:36,792 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_3 stored as values in memory (estimated size 176.0 B, free 364.0 MB)
2017-05-04 08:53:36,796 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 385.0 B, free 364.0 MB)
2017-05-04 08:53:36,796 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_3_piece0 in memory on 192.168.0.101:42487 (size: 385.0 B, free: 364.3 MB)
2017-05-04 08:53:36,797 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 3 from broadcast at KMeans.scala:367
2017-05-04 08:53:36,817 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: sum at KMeans.scala:373
2017-05-04 08:53:36,818 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 2 (sum at KMeans.scala:373) with 2 output partitions
2017-05-04 08:53:36,818 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 2 (sum at KMeans.scala:373)
2017-05-04 08:53:36,818 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:36,853 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:36,854 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370), which has no missing parents
2017-05-04 08:53:36,858 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_4 stored as values in memory (estimated size 5.0 KB, free 364.0 MB)
2017-05-04 08:53:36,860 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:36,863 INFO  [dispatcher-event-loop-8] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_4_piece0 in memory on 192.168.0.101:42487 (size: 2.8 KB, free: 364.2 MB)
2017-05-04 08:53:36,864 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 4 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:36,865 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at map at KMeans.scala:370)
2017-05-04 08:53:36,865 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 2.0 with 2 tasks
2017-05-04 08:53:36,868 INFO  [dispatcher-event-loop-11] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 6398 bytes)
2017-05-04 08:53:36,869 INFO  [dispatcher-event-loop-11] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 6398 bytes)
2017-05-04 08:53:36,870 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 2.0 (TID 4)
2017-05-04 08:53:36,870 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 2.0 (TID 5)
2017-05-04 08:53:36,877 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:36,877 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:36,878 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:36,878 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:36,878 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:36,878 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:36,879 INFO  [dispatcher-event-loop-16] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_1_piece0 on 192.168.0.101:42487 in memory (size: 2.4 KB, free: 364.2 MB)
2017-05-04 08:53:36,879 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:36,879 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:36,883 INFO  [dispatcher-event-loop-19] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_2_piece0 on 192.168.0.101:42487 in memory (size: 2.7 KB, free: 364.3 MB)
2017-05-04 08:53:36,895 WARN  [Executor task launch worker-0] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2017-05-04 08:53:36,895 WARN  [Executor task launch worker-0] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
2017-05-04 08:53:36,932 INFO  [Executor task launch worker-0] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_9_0 stored as values in memory (estimated size 83.0 KB, free 363.9 MB)
2017-05-04 08:53:36,933 INFO  [Executor task launch worker-1] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_9_1 stored as values in memory (estimated size 83.0 KB, free 363.9 MB)
2017-05-04 08:53:36,933 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_9_0 in memory on 192.168.0.101:42487 (size: 83.0 KB, free: 364.2 MB)
2017-05-04 08:53:36,934 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_9_1 in memory on 192.168.0.101:42487 (size: 83.0 KB, free: 364.1 MB)
2017-05-04 08:53:36,942 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 2.0 (TID 4). 1847 bytes result sent to driver
2017-05-04 08:53:36,942 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 2.0 (TID 5). 1847 bytes result sent to driver
2017-05-04 08:53:36,944 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 2.0 (TID 5) in 76 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:36,944 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 2.0 (TID 4) in 78 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:36,945 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-05-04 08:53:36,946 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 2 (sum at KMeans.scala:373) finished in 0.079 s
2017-05-04 08:53:36,946 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 2 finished: sum at KMeans.scala:373, took 0.129039 s
2017-05-04 08:53:36,949 INFO  [main] rdd.MapPartitionsRDD (Logging.scala:logInfo(54)) - Removing RDD 6 from persistence list
2017-05-04 08:53:36,953 INFO  [block-manager-slave-async-thread-pool-1] storage.BlockManager (Logging.scala:logInfo(54)) - Removing RDD 6
2017-05-04 08:53:36,972 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collect at KMeans.scala:381
2017-05-04 08:53:36,973 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 3 (collect at KMeans.scala:381) with 2 output partitions
2017-05-04 08:53:36,973 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 3 (collect at KMeans.scala:381)
2017-05-04 08:53:36,974 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:36,975 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:36,976 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
2017-05-04 08:53:36,979 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_5 stored as values in memory (estimated size 5.6 KB, free 363.8 MB)
2017-05-04 08:53:36,982 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.0 KB, free 363.8 MB)
2017-05-04 08:53:36,982 INFO  [dispatcher-event-loop-31] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_5_piece0 in memory on 192.168.0.101:42487 (size: 3.0 KB, free: 364.1 MB)
2017-05-04 08:53:36,983 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 5 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:36,984 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at mapPartitionsWithIndex at KMeans.scala:378)
2017-05-04 08:53:36,984 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 3.0 with 2 tasks
2017-05-04 08:53:36,986 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 6434 bytes)
2017-05-04 08:53:36,988 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 6434 bytes)
2017-05-04 08:53:36,988 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 3.0 (TID 6)
2017-05-04 08:53:36,988 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 3.0 (TID 7)
2017-05-04 08:53:36,993 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:36,993 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:36,993 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:36,993 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:36,994 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_9_0 locally
2017-05-04 08:53:36,994 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_9_1 locally
2017-05-04 08:53:37,008 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 3.0 (TID 7). 2352 bytes result sent to driver
2017-05-04 08:53:37,009 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 3.0 (TID 6). 2177 bytes result sent to driver
2017-05-04 08:53:37,010 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 3.0 (TID 6) in 25 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,010 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 3.0 (TID 7) in 23 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,010 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,012 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 3 (collect at KMeans.scala:381) finished in 0.026 s
2017-05-04 08:53:37,012 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 3 finished: collect at KMeans.scala:381, took 0.039908 s
2017-05-04 08:53:37,015 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_6 stored as values in memory (estimated size 2.6 KB, free 363.8 MB)
2017-05-04 08:53:37,029 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1665.0 B, free 363.8 MB)
2017-05-04 08:53:37,030 INFO  [dispatcher-event-loop-36] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_6_piece0 in memory on 192.168.0.101:42487 (size: 1665.0 B, free: 364.1 MB)
2017-05-04 08:53:37,031 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 6 from broadcast at KMeans.scala:367
2017-05-04 08:53:37,043 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: sum at KMeans.scala:373
2017-05-04 08:53:37,045 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 4 (sum at KMeans.scala:373) with 2 output partitions
2017-05-04 08:53:37,045 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 4 (sum at KMeans.scala:373)
2017-05-04 08:53:37,046 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:37,048 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:37,049 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370), which has no missing parents
2017-05-04 08:53:37,052 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_7 stored as values in memory (estimated size 5.2 KB, free 363.8 MB)
2017-05-04 08:53:37,054 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KB, free 363.8 MB)
2017-05-04 08:53:37,056 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_7_piece0 in memory on 192.168.0.101:42487 (size: 2.9 KB, free: 364.1 MB)
2017-05-04 08:53:37,057 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 7 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,057 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at map at KMeans.scala:370)
2017-05-04 08:53:37,057 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 4.0 with 2 tasks
2017-05-04 08:53:37,061 INFO  [dispatcher-event-loop-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 6430 bytes)
2017-05-04 08:53:37,062 INFO  [dispatcher-event-loop-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 6430 bytes)
2017-05-04 08:53:37,063 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 4.0 (TID 9)
2017-05-04 08:53:37,063 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 4.0 (TID 8)
2017-05-04 08:53:37,068 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:37,068 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:37,068 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:37,068 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:37,069 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_9_1 locally
2017-05-04 08:53:37,069 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_9_0 locally
2017-05-04 08:53:37,106 INFO  [Executor task launch worker-1] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_13_0 stored as values in memory (estimated size 83.0 KB, free 363.8 MB)
2017-05-04 08:53:37,106 INFO  [Executor task launch worker-0] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_13_1 stored as values in memory (estimated size 83.0 KB, free 363.7 MB)
2017-05-04 08:53:37,106 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_13_0 in memory on 192.168.0.101:42487 (size: 83.0 KB, free: 364.0 MB)
2017-05-04 08:53:37,107 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_13_1 in memory on 192.168.0.101:42487 (size: 83.0 KB, free: 363.9 MB)
2017-05-04 08:53:37,110 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 4.0 (TID 8). 1847 bytes result sent to driver
2017-05-04 08:53:37,111 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 4.0 (TID 8) in 53 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,111 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 4.0 (TID 9). 1847 bytes result sent to driver
2017-05-04 08:53:37,112 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 4.0 (TID 9) in 51 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,113 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,113 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 4 (sum at KMeans.scala:373) finished in 0.055 s
2017-05-04 08:53:37,114 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 4 finished: sum at KMeans.scala:373, took 0.070097 s
2017-05-04 08:53:37,116 INFO  [main] rdd.MapPartitionsRDD (Logging.scala:logInfo(54)) - Removing RDD 9 from persistence list
2017-05-04 08:53:37,119 INFO  [block-manager-slave-async-thread-pool-0] storage.BlockManager (Logging.scala:logInfo(54)) - Removing RDD 9
2017-05-04 08:53:37,147 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collect at KMeans.scala:381
2017-05-04 08:53:37,149 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 5 (collect at KMeans.scala:381) with 2 output partitions
2017-05-04 08:53:37,149 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 5 (collect at KMeans.scala:381)
2017-05-04 08:53:37,149 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:37,151 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:37,152 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378), which has no missing parents
2017-05-04 08:53:37,154 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_8 stored as values in memory (estimated size 5.9 KB, free 363.8 MB)
2017-05-04 08:53:37,157 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.1 KB, free 363.8 MB)
2017-05-04 08:53:37,158 INFO  [dispatcher-event-loop-17] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_8_piece0 in memory on 192.168.0.101:42487 (size: 3.1 KB, free: 364.1 MB)
2017-05-04 08:53:37,158 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 8 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,159 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at mapPartitionsWithIndex at KMeans.scala:378)
2017-05-04 08:53:37,159 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 5.0 with 2 tasks
2017-05-04 08:53:37,162 INFO  [dispatcher-event-loop-18] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 6466 bytes)
2017-05-04 08:53:37,163 INFO  [dispatcher-event-loop-18] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 5.0 (TID 11, localhost, executor driver, partition 1, PROCESS_LOCAL, 6466 bytes)
2017-05-04 08:53:37,164 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 5.0 (TID 10)
2017-05-04 08:53:37,164 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 5.0 (TID 11)
2017-05-04 08:53:37,169 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:37,169 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:37,169 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:37,169 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:37,169 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_13_0 locally
2017-05-04 08:53:37,170 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_13_1 locally
2017-05-04 08:53:37,183 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 5.0 (TID 10). 2868 bytes result sent to driver
2017-05-04 08:53:37,184 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 5.0 (TID 11). 2352 bytes result sent to driver
2017-05-04 08:53:37,184 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 5.0 (TID 10) in 24 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,185 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 5.0 (TID 11) in 23 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,185 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,185 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 5 (collect at KMeans.scala:381) finished in 0.025 s
2017-05-04 08:53:37,186 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 5 finished: collect at KMeans.scala:381, took 0.038545 s
2017-05-04 08:53:37,188 INFO  [main] rdd.MapPartitionsRDD (Logging.scala:logInfo(54)) - Removing RDD 13 from persistence list
2017-05-04 08:53:37,188 INFO  [block-manager-slave-async-thread-pool-1] storage.BlockManager (Logging.scala:logInfo(54)) - Removing RDD 13
2017-05-04 08:53:37,190 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(3) (from destroy at KMeans.scala:388)
2017-05-04 08:53:37,191 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(6) (from destroy at KMeans.scala:388)
2017-05-04 08:53:37,191 INFO  [dispatcher-event-loop-26] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_3_piece0 on 192.168.0.101:42487 in memory (size: 385.0 B, free: 364.2 MB)
2017-05-04 08:53:37,192 INFO  [dispatcher-event-loop-31] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_6_piece0 on 192.168.0.101:42487 in memory (size: 1665.0 B, free: 364.2 MB)
2017-05-04 08:53:37,196 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_9 stored as values in memory (estimated size 6.1 KB, free 364.0 MB)
2017-05-04 08:53:37,201 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.3 KB, free 364.0 MB)
2017-05-04 08:53:37,201 INFO  [dispatcher-event-loop-30] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_9_piece0 in memory on 192.168.0.101:42487 (size: 3.3 KB, free: 364.2 MB)
2017-05-04 08:53:37,202 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 9 from broadcast at KMeans.scala:398
2017-05-04 08:53:37,247 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: countByValue at KMeans.scala:399
2017-05-04 08:53:37,451 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 18 (countByValue at KMeans.scala:399)
2017-05-04 08:53:37,451 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 6 (countByValue at KMeans.scala:399) with 2 output partitions
2017-05-04 08:53:37,452 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 7 (countByValue at KMeans.scala:399)
2017-05-04 08:53:37,452 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 6)
2017-05-04 08:53:37,452 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 6)
2017-05-04 08:53:37,454 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399), which has no missing parents
2017-05-04 08:53:37,464 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_10 stored as values in memory (estimated size 6.2 KB, free 364.0 MB)
2017-05-04 08:53:37,467 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.5 KB, free 364.0 MB)
2017-05-04 08:53:37,468 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_10_piece0 in memory on 192.168.0.101:42487 (size: 3.5 KB, free: 364.2 MB)
2017-05-04 08:53:37,469 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 10 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,477 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[18] at countByValue at KMeans.scala:399)
2017-05-04 08:53:37,478 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 6.0 with 2 tasks
2017-05-04 08:53:37,481 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 6.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,482 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 6.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,483 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 6.0 (TID 13)
2017-05-04 08:53:37,483 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 6.0 (TID 12)
2017-05-04 08:53:37,491 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:37,491 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:37,491 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:37,492 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:37,612 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 6.0 (TID 13). 1746 bytes result sent to driver
2017-05-04 08:53:37,612 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 6.0 (TID 12). 1746 bytes result sent to driver
2017-05-04 08:53:37,615 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 6.0 (TID 13) in 133 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,616 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 6.0 (TID 12) in 137 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,616 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,618 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 6 (countByValue at KMeans.scala:399) finished in 0.139 s
2017-05-04 08:53:37,618 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:37,619 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:37,620 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 7)
2017-05-04 08:53:37,620 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:37,626 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399), which has no missing parents
2017-05-04 08:53:37,632 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_11 stored as values in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:37,634 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_11_piece0 stored as bytes in memory (estimated size 1979.0 B, free 364.0 MB)
2017-05-04 08:53:37,635 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_11_piece0 in memory on 192.168.0.101:42487 (size: 1979.0 B, free: 364.2 MB)
2017-05-04 08:53:37,635 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 11 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,636 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 7 (ShuffledRDD[19] at countByValue at KMeans.scala:399)
2017-05-04 08:53:37,636 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 7.0 with 2 tasks
2017-05-04 08:53:37,640 INFO  [dispatcher-event-loop-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 7.0 (TID 14, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:37,642 INFO  [dispatcher-event-loop-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 7.0 (TID 15, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:37,642 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 7.0 (TID 15)
2017-05-04 08:53:37,642 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 7.0 (TID 14)
2017-05-04 08:53:37,660 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,660 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,662 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 7 ms
2017-05-04 08:53:37,662 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 7 ms
2017-05-04 08:53:37,707 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 7.0 (TID 14). 2685 bytes result sent to driver
2017-05-04 08:53:37,707 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 7.0 (TID 15). 2646 bytes result sent to driver
2017-05-04 08:53:37,709 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 7.0 (TID 15) in 68 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,709 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 7.0 (TID 14) in 71 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,709 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,711 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 7 (countByValue at KMeans.scala:399) finished in 0.073 s
2017-05-04 08:53:37,711 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 6 finished: countByValue at KMeans.scala:399, took 0.464604 s
2017-05-04 08:53:37,713 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(9) (from destroy at KMeans.scala:401)
2017-05-04 08:53:37,715 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_9_piece0 on 192.168.0.101:42487 in memory (size: 3.3 KB, free: 364.2 MB)
2017-05-04 08:53:37,739 INFO  [main] clustering.LocalKMeans (Logging.scala:logInfo(54)) - Local KMeans++ converged in 3 iterations.
2017-05-04 08:53:37,740 INFO  [main] clustering.KMeans (Logging.scala:logInfo(54)) - Initialization with k-means|| took 2.103 seconds.
2017-05-04 08:53:37,742 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_12 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:37,745 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_12_piece0 stored as bytes in memory (estimated size 1219.0 B, free 364.0 MB)
2017-05-04 08:53:37,746 INFO  [dispatcher-event-loop-8] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_12_piece0 in memory on 192.168.0.101:42487 (size: 1219.0 B, free: 364.2 MB)
2017-05-04 08:53:37,747 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 12 from broadcast at KMeans.scala:273
2017-05-04 08:53:37,777 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:37,778 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 20 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:37,779 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 7 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:37,779 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 9 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:37,779 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 8)
2017-05-04 08:53:37,779 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 8)
2017-05-04 08:53:37,781 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:37,785 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_13 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:37,787 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:37,788 INFO  [dispatcher-event-loop-13] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_13_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:37,789 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 13 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,789 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:37,789 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 8.0 with 2 tasks
2017-05-04 08:53:37,792 INFO  [dispatcher-event-loop-12] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 8.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,793 INFO  [dispatcher-event-loop-12] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 8.0 (TID 17, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,794 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 8.0 (TID 17)
2017-05-04 08:53:37,794 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 8.0 (TID 16)
2017-05-04 08:53:37,799 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:37,799 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:37,799 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:37,799 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:37,830 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 8.0 (TID 16). 1799 bytes result sent to driver
2017-05-04 08:53:37,830 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 8.0 (TID 17). 1799 bytes result sent to driver
2017-05-04 08:53:37,831 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 8.0 (TID 17) in 39 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,832 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 8.0 (TID 16) in 42 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,832 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,833 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 8 (mapPartitions at KMeans.scala:276) finished in 0.042 s
2017-05-04 08:53:37,833 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:37,833 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:37,833 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 9)
2017-05-04 08:53:37,833 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:37,834 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:37,836 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_14 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:37,838 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_14_piece0 stored as bytes in memory (estimated size 1658.0 B, free 364.0 MB)
2017-05-04 08:53:37,838 INFO  [dispatcher-event-loop-18] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_14_piece0 in memory on 192.168.0.101:42487 (size: 1658.0 B, free: 364.2 MB)
2017-05-04 08:53:37,839 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 14 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,839 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 9 (ShuffledRDD[21] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:37,840 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 9.0 with 2 tasks
2017-05-04 08:53:37,841 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 9.0 (TID 18, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:37,842 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 9.0 (TID 19, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:37,843 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 9.0 (TID 18)
2017-05-04 08:53:37,843 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 9.0 (TID 19)
2017-05-04 08:53:37,846 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,846 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,846 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:37,846 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:37,852 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 9.0 (TID 19). 2721 bytes result sent to driver
2017-05-04 08:53:37,852 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 9.0 (TID 18). 2907 bytes result sent to driver
2017-05-04 08:53:37,853 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 9.0 (TID 19) in 11 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,854 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 9.0 (TID 18) in 13 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,854 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,854 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 9 (collectAsMap at KMeans.scala:295) finished in 0.014 s
2017-05-04 08:53:37,854 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 7 finished: collectAsMap at KMeans.scala:295, took 0.077201 s
2017-05-04 08:53:37,856 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(12) (from destroy at KMeans.scala:297)
2017-05-04 08:53:37,857 INFO  [dispatcher-event-loop-27] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_12_piece0 on 192.168.0.101:42487 in memory (size: 1219.0 B, free: 364.2 MB)
2017-05-04 08:53:37,859 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_15 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:37,860 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_15_piece0 stored as bytes in memory (estimated size 1276.0 B, free 364.0 MB)
2017-05-04 08:53:37,861 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_15_piece0 in memory on 192.168.0.101:42487 (size: 1276.0 B, free: 364.2 MB)
2017-05-04 08:53:37,862 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 15 from broadcast at KMeans.scala:273
2017-05-04 08:53:37,879 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:37,880 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 22 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:37,881 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 8 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:37,881 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 11 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:37,881 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 10)
2017-05-04 08:53:37,881 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 10)
2017-05-04 08:53:37,882 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:37,884 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_16 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:37,886 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:37,887 INFO  [dispatcher-event-loop-31] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_16_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:37,887 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 16 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,888 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:37,888 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 10.0 with 2 tasks
2017-05-04 08:53:37,890 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 10.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,891 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 10.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,891 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 10.0 (TID 20)
2017-05-04 08:53:37,891 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 10.0 (TID 21)
2017-05-04 08:53:37,895 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:37,895 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:37,895 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:37,895 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:37,904 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 10.0 (TID 20). 1799 bytes result sent to driver
2017-05-04 08:53:37,904 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 10.0 (TID 21). 1799 bytes result sent to driver
2017-05-04 08:53:37,905 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 10.0 (TID 20) in 16 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,906 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 10.0 (TID 21) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,906 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 10 (mapPartitions at KMeans.scala:276) finished in 0.018 s
2017-05-04 08:53:37,906 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,906 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:37,906 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:37,906 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 11)
2017-05-04 08:53:37,906 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:37,907 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:37,908 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_17 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:37,910 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_17_piece0 stored as bytes in memory (estimated size 1654.0 B, free 363.9 MB)
2017-05-04 08:53:37,911 INFO  [dispatcher-event-loop-36] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_17_piece0 in memory on 192.168.0.101:42487 (size: 1654.0 B, free: 364.2 MB)
2017-05-04 08:53:37,911 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 17 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,912 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[23] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:37,912 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 11.0 with 2 tasks
2017-05-04 08:53:37,914 INFO  [dispatcher-event-loop-37] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 11.0 (TID 22, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:37,915 INFO  [dispatcher-event-loop-37] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 11.0 (TID 23, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:37,915 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 11.0 (TID 22)
2017-05-04 08:53:37,915 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 11.0 (TID 23)
2017-05-04 08:53:37,918 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,918 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,918 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:37,918 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:37,921 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 11.0 (TID 23). 2721 bytes result sent to driver
2017-05-04 08:53:37,921 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 11.0 (TID 22). 2820 bytes result sent to driver
2017-05-04 08:53:37,922 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 11.0 (TID 23) in 8 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,922 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 11.0 (TID 22) in 9 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,922 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,923 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 11 (collectAsMap at KMeans.scala:295) finished in 0.011 s
2017-05-04 08:53:37,923 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 8 finished: collectAsMap at KMeans.scala:295, took 0.043869 s
2017-05-04 08:53:37,924 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(15) (from destroy at KMeans.scala:297)
2017-05-04 08:53:37,926 INFO  [dispatcher-event-loop-6] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_15_piece0 on 192.168.0.101:42487 in memory (size: 1276.0 B, free: 364.2 MB)
2017-05-04 08:53:37,926 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_18 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:37,928 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_18_piece0 stored as bytes in memory (estimated size 1282.0 B, free 363.9 MB)
2017-05-04 08:53:37,929 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_18_piece0 in memory on 192.168.0.101:42487 (size: 1282.0 B, free: 364.2 MB)
2017-05-04 08:53:37,929 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 18 from broadcast at KMeans.scala:273
2017-05-04 08:53:37,947 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:37,948 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 24 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:37,948 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 9 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:37,948 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 13 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:37,948 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 12)
2017-05-04 08:53:37,948 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 12)
2017-05-04 08:53:37,950 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 12 (MapPartitionsRDD[24] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:37,952 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_19 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:37,954 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:37,954 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_19_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:37,955 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 19 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,955 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[24] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:37,955 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 12.0 with 2 tasks
2017-05-04 08:53:37,958 INFO  [dispatcher-event-loop-8] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 12.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,959 INFO  [dispatcher-event-loop-8] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 12.0 (TID 25, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:37,959 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 12.0 (TID 24)
2017-05-04 08:53:37,959 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 12.0 (TID 25)
2017-05-04 08:53:37,963 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:37,963 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:37,963 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:37,963 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:37,976 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 12.0 (TID 25). 1799 bytes result sent to driver
2017-05-04 08:53:37,976 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 12.0 (TID 24). 1799 bytes result sent to driver
2017-05-04 08:53:37,977 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 12.0 (TID 25) in 19 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,978 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 12.0 (TID 24) in 22 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,978 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,978 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 12 (mapPartitions at KMeans.scala:276) finished in 0.022 s
2017-05-04 08:53:37,978 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:37,978 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:37,979 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 13)
2017-05-04 08:53:37,979 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:37,979 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 13 (ShuffledRDD[25] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:37,980 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_20 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:37,983 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_20_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:37,983 INFO  [dispatcher-event-loop-14] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_20_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:37,984 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 20 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:37,984 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 13 (ShuffledRDD[25] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:37,984 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 13.0 with 2 tasks
2017-05-04 08:53:37,986 INFO  [dispatcher-event-loop-15] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 13.0 (TID 26, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:37,987 INFO  [dispatcher-event-loop-15] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 13.0 (TID 27, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:37,987 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 13.0 (TID 26)
2017-05-04 08:53:37,987 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 13.0 (TID 27)
2017-05-04 08:53:37,989 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,989 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:37,989 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:37,990 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:37,991 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 13.0 (TID 26). 2820 bytes result sent to driver
2017-05-04 08:53:37,992 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 13.0 (TID 27). 2721 bytes result sent to driver
2017-05-04 08:53:37,992 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 13.0 (TID 26) in 7 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:37,992 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 13.0 (TID 27) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:37,993 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2017-05-04 08:53:37,993 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 13 (collectAsMap at KMeans.scala:295) finished in 0.008 s
2017-05-04 08:53:37,994 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 9 finished: collectAsMap at KMeans.scala:295, took 0.046940 s
2017-05-04 08:53:37,995 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(18) (from destroy at KMeans.scala:297)
2017-05-04 08:53:37,996 INFO  [dispatcher-event-loop-24] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_18_piece0 on 192.168.0.101:42487 in memory (size: 1282.0 B, free: 364.2 MB)
2017-05-04 08:53:37,996 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_21 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:37,998 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_21_piece0 stored as bytes in memory (estimated size 1277.0 B, free 363.9 MB)
2017-05-04 08:53:37,998 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_21_piece0 in memory on 192.168.0.101:42487 (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:37,999 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 21 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,018 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 26 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 10 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 15 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 14)
2017-05-04 08:53:38,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 14)
2017-05-04 08:53:38,021 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 14 (MapPartitionsRDD[26] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,022 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_22 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,024 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,024 INFO  [dispatcher-event-loop-27] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_22_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,025 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 22 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,025 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[26] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,025 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 14.0 with 2 tasks
2017-05-04 08:53:38,028 INFO  [dispatcher-event-loop-28] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 14.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,028 INFO  [dispatcher-event-loop-28] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 14.0 (TID 29, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,029 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 14.0 (TID 29)
2017-05-04 08:53:38,029 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 14.0 (TID 28)
2017-05-04 08:53:38,032 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,032 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,032 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,032 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,042 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 14.0 (TID 29). 1799 bytes result sent to driver
2017-05-04 08:53:38,042 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 14.0 (TID 29) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,043 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 14.0 (TID 28). 1799 bytes result sent to driver
2017-05-04 08:53:38,044 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 14.0 (TID 28) in 18 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,044 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,044 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 14 (mapPartitions at KMeans.scala:276) finished in 0.018 s
2017-05-04 08:53:38,044 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,045 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,045 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 15)
2017-05-04 08:53:38,045 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,045 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 15 (ShuffledRDD[27] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,047 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_23 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,050 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_23_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,050 INFO  [dispatcher-event-loop-33] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_23_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,051 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 23 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,051 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 15 (ShuffledRDD[27] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,052 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 15.0 with 2 tasks
2017-05-04 08:53:38,054 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 15.0 (TID 30, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:38,055 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 15.0 (TID 31, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:38,055 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 15.0 (TID 31)
2017-05-04 08:53:38,055 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 15.0 (TID 30)
2017-05-04 08:53:38,058 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,058 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,058 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,058 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,061 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 15.0 (TID 31). 2721 bytes result sent to driver
2017-05-04 08:53:38,061 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 15.0 (TID 30). 2820 bytes result sent to driver
2017-05-04 08:53:38,062 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 15.0 (TID 31) in 8 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,063 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 15.0 (TID 30) in 10 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,063 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,063 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 15 (collectAsMap at KMeans.scala:295) finished in 0.011 s
2017-05-04 08:53:38,064 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 10 finished: collectAsMap at KMeans.scala:295, took 0.045954 s
2017-05-04 08:53:38,065 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(21) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,066 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_21_piece0 on 192.168.0.101:42487 in memory (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:38,066 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_24 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,068 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_24_piece0 stored as bytes in memory (estimated size 1275.0 B, free 363.9 MB)
2017-05-04 08:53:38,069 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_24_piece0 in memory on 192.168.0.101:42487 (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:38,070 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 24 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,087 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,088 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 28 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,088 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 11 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,088 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 17 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,088 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 16)
2017-05-04 08:53:38,088 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 16)
2017-05-04 08:53:38,089 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 16 (MapPartitionsRDD[28] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,091 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_25 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,093 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,094 INFO  [dispatcher-event-loop-6] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_25_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,094 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 25 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,094 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[28] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,094 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 16.0 with 2 tasks
2017-05-04 08:53:38,097 INFO  [dispatcher-event-loop-4] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 16.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,097 INFO  [dispatcher-event-loop-4] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 16.0 (TID 33, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,098 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 16.0 (TID 32)
2017-05-04 08:53:38,098 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 16.0 (TID 33)
2017-05-04 08:53:38,101 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,101 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,101 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,102 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,121 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_22_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,123 INFO  [dispatcher-event-loop-13] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_23_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,124 INFO  [dispatcher-event-loop-17] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_20_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,126 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 16.0 (TID 32). 1959 bytes result sent to driver
2017-05-04 08:53:38,127 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 16.0 (TID 32) in 32 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,127 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 1011
2017-05-04 08:53:38,127 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 16.0 (TID 33). 1872 bytes result sent to driver
2017-05-04 08:53:38,128 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 16.0 (TID 33) in 31 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,128 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,128 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 16 (mapPartitions at KMeans.scala:276) finished in 0.033 s
2017-05-04 08:53:38,128 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 17)
2017-05-04 08:53:38,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 17 (ShuffledRDD[29] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,130 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_26 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,132 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_26_piece0 stored as bytes in memory (estimated size 1657.0 B, free 363.9 MB)
2017-05-04 08:53:38,132 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_26_piece0 in memory on 192.168.0.101:42487 (size: 1657.0 B, free: 364.2 MB)
2017-05-04 08:53:38,133 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 26 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,133 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 4
2017-05-04 08:53:38,133 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 17 (ShuffledRDD[29] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,133 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 17.0 with 2 tasks
2017-05-04 08:53:38,135 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_19_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,135 INFO  [dispatcher-event-loop-27] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 17.0 (TID 34, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:38,137 INFO  [dispatcher-event-loop-27] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 17.0 (TID 35, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:38,137 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 17.0 (TID 35)
2017-05-04 08:53:38,137 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 17.0 (TID 34)
2017-05-04 08:53:38,137 INFO  [dispatcher-event-loop-31] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_17_piece0 on 192.168.0.101:42487 in memory (size: 1654.0 B, free: 364.2 MB)
2017-05-04 08:53:38,138 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 866
2017-05-04 08:53:38,139 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 3
2017-05-04 08:53:38,140 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_14_piece0 on 192.168.0.101:42487 in memory (size: 1658.0 B, free: 364.2 MB)
2017-05-04 08:53:38,140 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,140 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,140 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,140 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 721
2017-05-04 08:53:38,140 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,141 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 2
2017-05-04 08:53:38,142 INFO  [dispatcher-event-loop-9] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_16_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,142 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 17.0 (TID 35). 2721 bytes result sent to driver
2017-05-04 08:53:38,143 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_13_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,143 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 17.0 (TID 35) in 8 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,143 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 17.0 (TID 34). 2820 bytes result sent to driver
2017-05-04 08:53:38,144 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_11_piece0 on 192.168.0.101:42487 in memory (size: 1979.0 B, free: 364.2 MB)
2017-05-04 08:53:38,144 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 17.0 (TID 34) in 10 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,145 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 17.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,145 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 17 (collectAsMap at KMeans.scala:295) finished in 0.010 s
2017-05-04 08:53:38,145 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 576
2017-05-04 08:53:38,145 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 11 finished: collectAsMap at KMeans.scala:295, took 0.058040 s
2017-05-04 08:53:38,146 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 1
2017-05-04 08:53:38,146 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(24) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,147 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_8_piece0 on 192.168.0.101:42487 in memory (size: 3.1 KB, free: 364.2 MB)
2017-05-04 08:53:38,147 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_24_piece0 on 192.168.0.101:42487 in memory (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:38,147 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_27 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,148 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 0
2017-05-04 08:53:38,149 INFO  [dispatcher-event-loop-33] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_10_piece0 on 192.168.0.101:42487 in memory (size: 3.5 KB, free: 364.2 MB)
2017-05-04 08:53:38,149 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_27_piece0 stored as bytes in memory (estimated size 1278.0 B, free 364.0 MB)
2017-05-04 08:53:38,150 INFO  [dispatcher-event-loop-31] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_27_piece0 in memory on 192.168.0.101:42487 (size: 1278.0 B, free: 364.2 MB)
2017-05-04 08:53:38,150 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 27 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,150 INFO  [dispatcher-event-loop-36] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_7_piece0 on 192.168.0.101:42487 in memory (size: 2.9 KB, free: 364.2 MB)
2017-05-04 08:53:38,151 INFO  [dispatcher-event-loop-37] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_4_piece0 on 192.168.0.101:42487 in memory (size: 2.8 KB, free: 364.2 MB)
2017-05-04 08:53:38,152 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_5_piece0 on 192.168.0.101:42487 in memory (size: 3.0 KB, free: 364.2 MB)
2017-05-04 08:53:38,155 INFO  [block-manager-slave-async-thread-pool-3] storage.BlockManager (Logging.scala:logInfo(54)) - Removing RDD 13
2017-05-04 08:53:38,157 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned RDD 13
2017-05-04 08:53:38,158 INFO  [block-manager-slave-async-thread-pool-3] storage.BlockManager (Logging.scala:logInfo(54)) - Removing RDD 9
2017-05-04 08:53:38,158 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned RDD 9
2017-05-04 08:53:38,165 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,167 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 30 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,168 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 12 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,168 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 19 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,168 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 18)
2017-05-04 08:53:38,168 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 18)
2017-05-04 08:53:38,169 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 18 (MapPartitionsRDD[30] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,171 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_28 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:38,172 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:38,173 INFO  [dispatcher-event-loop-11] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_28_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,173 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 28 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,173 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[30] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,174 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 18.0 with 2 tasks
2017-05-04 08:53:38,176 INFO  [dispatcher-event-loop-12] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 18.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,177 INFO  [dispatcher-event-loop-12] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 18.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,177 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 18.0 (TID 37)
2017-05-04 08:53:38,177 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 18.0 (TID 36)
2017-05-04 08:53:38,180 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,180 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,180 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,180 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,189 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 18.0 (TID 37). 1799 bytes result sent to driver
2017-05-04 08:53:38,189 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 18.0 (TID 37) in 13 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,192 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 18.0 (TID 36). 1799 bytes result sent to driver
2017-05-04 08:53:38,193 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 18.0 (TID 36) in 19 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,193 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,193 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 18 (mapPartitions at KMeans.scala:276) finished in 0.019 s
2017-05-04 08:53:38,193 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,193 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,194 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 19)
2017-05-04 08:53:38,194 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,194 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 19 (ShuffledRDD[31] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,195 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_29 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:38,197 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_29_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:38,197 INFO  [dispatcher-event-loop-16] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_29_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,198 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 29 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,198 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 19 (ShuffledRDD[31] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,198 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 19.0 with 2 tasks
2017-05-04 08:53:38,200 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 19.0 (TID 38, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:38,201 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 19.0 (TID 39, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:38,201 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 19.0 (TID 38)
2017-05-04 08:53:38,201 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 19.0 (TID 39)
2017-05-04 08:53:38,203 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,203 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,204 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,204 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,206 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 19.0 (TID 38). 2820 bytes result sent to driver
2017-05-04 08:53:38,206 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 19.0 (TID 39). 2721 bytes result sent to driver
2017-05-04 08:53:38,206 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 19.0 (TID 38) in 7 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,207 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 19.0 (TID 39) in 7 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,207 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 19.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,207 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 19 (collectAsMap at KMeans.scala:295) finished in 0.008 s
2017-05-04 08:53:38,207 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 12 finished: collectAsMap at KMeans.scala:295, took 0.041952 s
2017-05-04 08:53:38,208 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(27) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,209 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_27_piece0 on 192.168.0.101:42487 in memory (size: 1278.0 B, free: 364.2 MB)
2017-05-04 08:53:38,209 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_30 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,211 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_30_piece0 stored as bytes in memory (estimated size 1276.0 B, free 364.0 MB)
2017-05-04 08:53:38,211 INFO  [dispatcher-event-loop-29] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_30_piece0 in memory on 192.168.0.101:42487 (size: 1276.0 B, free: 364.2 MB)
2017-05-04 08:53:38,212 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 30 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,226 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,227 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 32 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,227 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 13 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,227 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 21 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,228 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 20)
2017-05-04 08:53:38,228 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 20)
2017-05-04 08:53:38,229 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 20 (MapPartitionsRDD[32] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,232 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_31 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:38,233 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:38,234 INFO  [dispatcher-event-loop-27] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_31_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,235 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 31 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,235 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[32] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,236 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 20.0 with 2 tasks
2017-05-04 08:53:38,238 INFO  [dispatcher-event-loop-33] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 20.0 (TID 40, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,239 INFO  [dispatcher-event-loop-33] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 20.0 (TID 41, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,239 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 20.0 (TID 41)
2017-05-04 08:53:38,239 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 20.0 (TID 40)
2017-05-04 08:53:38,242 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,242 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,242 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,242 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,250 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 20.0 (TID 40). 1799 bytes result sent to driver
2017-05-04 08:53:38,250 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 20.0 (TID 40) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,254 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 20.0 (TID 41). 1799 bytes result sent to driver
2017-05-04 08:53:38,254 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 20.0 (TID 41) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,254 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,255 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 20 (mapPartitions at KMeans.scala:276) finished in 0.019 s
2017-05-04 08:53:38,255 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,255 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,255 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 21)
2017-05-04 08:53:38,255 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,256 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 21 (ShuffledRDD[33] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,257 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_32 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:38,258 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_32_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:38,259 INFO  [dispatcher-event-loop-38] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_32_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,259 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 32 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,260 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 21 (ShuffledRDD[33] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,260 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 21.0 with 2 tasks
2017-05-04 08:53:38,262 INFO  [dispatcher-event-loop-35] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 21.0 (TID 42, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:38,263 INFO  [dispatcher-event-loop-35] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 21.0 (TID 43, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:38,263 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 21.0 (TID 42)
2017-05-04 08:53:38,263 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 21.0 (TID 43)
2017-05-04 08:53:38,265 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,265 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,265 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,265 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,267 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 21.0 (TID 42). 2820 bytes result sent to driver
2017-05-04 08:53:38,268 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 21.0 (TID 43). 2721 bytes result sent to driver
2017-05-04 08:53:38,268 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 21.0 (TID 42) in 7 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,268 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 21.0 (TID 43) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,268 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,269 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 21 (collectAsMap at KMeans.scala:295) finished in 0.008 s
2017-05-04 08:53:38,269 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 13 finished: collectAsMap at KMeans.scala:295, took 0.042911 s
2017-05-04 08:53:38,270 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(30) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,270 INFO  [dispatcher-event-loop-6] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_30_piece0 on 192.168.0.101:42487 in memory (size: 1276.0 B, free: 364.2 MB)
2017-05-04 08:53:38,271 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_33 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,272 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_33_piece0 stored as bytes in memory (estimated size 1275.0 B, free 364.0 MB)
2017-05-04 08:53:38,272 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_33_piece0 in memory on 192.168.0.101:42487 (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:38,273 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 33 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,289 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,290 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 34 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,290 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 14 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,290 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 23 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,290 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 22)
2017-05-04 08:53:38,290 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 22)
2017-05-04 08:53:38,291 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 22 (MapPartitionsRDD[34] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,293 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_34 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:38,295 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:38,303 INFO  [dispatcher-event-loop-8] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_34_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,304 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 34 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,304 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[34] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,304 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 22.0 with 2 tasks
2017-05-04 08:53:38,306 INFO  [dispatcher-event-loop-7] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 22.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,307 INFO  [dispatcher-event-loop-7] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 22.0 (TID 45, localhost, executor driver, partition 1, PROCESS_LOCAL, 6364 bytes)
2017-05-04 08:53:38,307 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 22.0 (TID 44)
2017-05-04 08:53:38,307 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 22.0 (TID 45)
2017-05-04 08:53:38,309 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,310 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,310 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,310 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,320 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 22.0 (TID 44). 1799 bytes result sent to driver
2017-05-04 08:53:38,321 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 22.0 (TID 44) in 16 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,322 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 22.0 (TID 45). 1799 bytes result sent to driver
2017-05-04 08:53:38,322 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 22.0 (TID 45) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,322 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 22.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,323 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 22 (mapPartitions at KMeans.scala:276) finished in 0.019 s
2017-05-04 08:53:38,323 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,323 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,323 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 23)
2017-05-04 08:53:38,323 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,324 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 23 (ShuffledRDD[35] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,325 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_35 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:38,327 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_35_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:38,327 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_35_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,328 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 35 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,328 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 23 (ShuffledRDD[35] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,328 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 23.0 with 2 tasks
2017-05-04 08:53:38,330 INFO  [dispatcher-event-loop-17] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 23.0 (TID 46, localhost, executor driver, partition 0, ANY, 5826 bytes)
2017-05-04 08:53:38,330 INFO  [dispatcher-event-loop-17] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 23.0 (TID 47, localhost, executor driver, partition 1, ANY, 5826 bytes)
2017-05-04 08:53:38,330 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 23.0 (TID 46)
2017-05-04 08:53:38,331 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 23.0 (TID 47)
2017-05-04 08:53:38,332 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,333 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,333 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,333 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,334 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 23.0 (TID 46). 2820 bytes result sent to driver
2017-05-04 08:53:38,335 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 23.0 (TID 47). 2721 bytes result sent to driver
2017-05-04 08:53:38,335 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 23.0 (TID 46) in 6 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,335 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 23.0 (TID 47) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,336 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 23.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,336 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 23 (collectAsMap at KMeans.scala:295) finished in 0.007 s
2017-05-04 08:53:38,336 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 14 finished: collectAsMap at KMeans.scala:295, took 0.047296 s
2017-05-04 08:53:38,337 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(33) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,338 INFO  [dispatcher-event-loop-23] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_33_piece0 on 192.168.0.101:42487 in memory (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:38,338 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_36 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,340 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_36_piece0 stored as bytes in memory (estimated size 1274.0 B, free 364.0 MB)
2017-05-04 08:53:38,340 INFO  [dispatcher-event-loop-20] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_36_piece0 in memory on 192.168.0.101:42487 (size: 1274.0 B, free: 364.2 MB)
2017-05-04 08:53:38,341 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 36 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,357 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,358 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 36 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,359 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 15 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,359 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 25 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,359 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 24)
2017-05-04 08:53:38,359 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 24)
2017-05-04 08:53:38,360 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 24 (MapPartitionsRDD[36] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,361 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_37 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:38,363 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_37_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:38,363 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_37_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,364 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 37 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,364 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[36] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,365 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 24.0 with 2 tasks
2017-05-04 08:53:38,366 INFO  [dispatcher-event-loop-29] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 24.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,367 INFO  [dispatcher-event-loop-29] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 24.0 (TID 49, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,367 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 24.0 (TID 48)
2017-05-04 08:53:38,367 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 24.0 (TID 49)
2017-05-04 08:53:38,370 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,370 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,371 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,371 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,379 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 24.0 (TID 48). 1799 bytes result sent to driver
2017-05-04 08:53:38,380 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 24.0 (TID 49). 1799 bytes result sent to driver
2017-05-04 08:53:38,380 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 24.0 (TID 48) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,381 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 24.0 (TID 49) in 14 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,381 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 24.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,381 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 24 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:38,381 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,381 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,381 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 25)
2017-05-04 08:53:38,382 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,382 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 25 (ShuffledRDD[37] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,383 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_38 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:38,385 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_38_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:38,385 INFO  [dispatcher-event-loop-34] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_38_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,386 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 38 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,386 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 25 (ShuffledRDD[37] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,386 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 25.0 with 2 tasks
2017-05-04 08:53:38,388 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 25.0 (TID 50, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,388 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 25.0 (TID 51, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,389 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 25.0 (TID 51)
2017-05-04 08:53:38,389 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 25.0 (TID 50)
2017-05-04 08:53:38,391 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,391 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,391 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,391 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,393 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 25.0 (TID 51). 2721 bytes result sent to driver
2017-05-04 08:53:38,393 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 25.0 (TID 50). 2820 bytes result sent to driver
2017-05-04 08:53:38,393 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 25.0 (TID 51) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,393 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 25.0 (TID 50) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,394 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 25.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,394 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 25 (collectAsMap at KMeans.scala:295) finished in 0.008 s
2017-05-04 08:53:38,394 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 15 finished: collectAsMap at KMeans.scala:295, took 0.036898 s
2017-05-04 08:53:38,395 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(36) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,396 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_36_piece0 on 192.168.0.101:42487 in memory (size: 1274.0 B, free: 364.2 MB)
2017-05-04 08:53:38,396 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_39 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,397 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_39_piece0 stored as bytes in memory (estimated size 1273.0 B, free 364.0 MB)
2017-05-04 08:53:38,398 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_39_piece0 in memory on 192.168.0.101:42487 (size: 1273.0 B, free: 364.2 MB)
2017-05-04 08:53:38,398 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 39 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,415 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,416 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 38 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,416 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 16 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,416 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 27 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,417 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 26)
2017-05-04 08:53:38,417 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 26)
2017-05-04 08:53:38,418 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 26 (MapPartitionsRDD[38] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,419 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_40 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:38,421 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,422 INFO  [dispatcher-event-loop-6] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_40_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,422 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 40 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,422 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[38] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,422 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 26.0 with 2 tasks
2017-05-04 08:53:38,425 INFO  [dispatcher-event-loop-4] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 26.0 (TID 52, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,425 INFO  [dispatcher-event-loop-4] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 26.0 (TID 53, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,426 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 26.0 (TID 52)
2017-05-04 08:53:38,426 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 26.0 (TID 53)
2017-05-04 08:53:38,428 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,428 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,428 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,428 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,436 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 26.0 (TID 52). 1799 bytes result sent to driver
2017-05-04 08:53:38,437 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 26.0 (TID 52) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,439 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 26.0 (TID 53). 1799 bytes result sent to driver
2017-05-04 08:53:38,440 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 26.0 (TID 53) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,440 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,440 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 26 (mapPartitions at KMeans.scala:276) finished in 0.017 s
2017-05-04 08:53:38,441 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,441 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,441 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 27)
2017-05-04 08:53:38,441 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,441 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 27 (ShuffledRDD[39] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,442 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_41 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,444 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_41_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,444 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_41_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,445 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 41 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,445 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 27 (ShuffledRDD[39] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,445 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 27.0 with 2 tasks
2017-05-04 08:53:38,447 INFO  [dispatcher-event-loop-13] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 27.0 (TID 54, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,447 INFO  [dispatcher-event-loop-13] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 27.0 (TID 55, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,448 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 27.0 (TID 54)
2017-05-04 08:53:38,448 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 27.0 (TID 55)
2017-05-04 08:53:38,450 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,450 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,450 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,450 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,451 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 27.0 (TID 55). 2721 bytes result sent to driver
2017-05-04 08:53:38,452 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 27.0 (TID 54). 2820 bytes result sent to driver
2017-05-04 08:53:38,452 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 27.0 (TID 55) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,453 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 27.0 (TID 54) in 7 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,453 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 27.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,453 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 27 (collectAsMap at KMeans.scala:295) finished in 0.007 s
2017-05-04 08:53:38,453 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 16 finished: collectAsMap at KMeans.scala:295, took 0.037813 s
2017-05-04 08:53:38,454 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(39) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,455 INFO  [dispatcher-event-loop-16] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_39_piece0 on 192.168.0.101:42487 in memory (size: 1273.0 B, free: 364.2 MB)
2017-05-04 08:53:38,455 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_42 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,457 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_42_piece0 stored as bytes in memory (estimated size 1280.0 B, free 363.9 MB)
2017-05-04 08:53:38,457 INFO  [dispatcher-event-loop-19] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_42_piece0 in memory on 192.168.0.101:42487 (size: 1280.0 B, free: 364.2 MB)
2017-05-04 08:53:38,458 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 42 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,473 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,474 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 40 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,474 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 17 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,475 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 29 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,475 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 28)
2017-05-04 08:53:38,475 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 28)
2017-05-04 08:53:38,476 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 28 (MapPartitionsRDD[40] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,477 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_43 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,479 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,479 INFO  [dispatcher-event-loop-23] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_43_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,480 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 43 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,480 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[40] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,480 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 28.0 with 2 tasks
2017-05-04 08:53:38,482 INFO  [dispatcher-event-loop-20] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 28.0 (TID 56, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,483 INFO  [dispatcher-event-loop-20] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 28.0 (TID 57, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,483 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 28.0 (TID 56)
2017-05-04 08:53:38,483 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 28.0 (TID 57)
2017-05-04 08:53:38,486 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,486 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,486 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,486 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,495 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 28.0 (TID 57). 1799 bytes result sent to driver
2017-05-04 08:53:38,496 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 28.0 (TID 57) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,496 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 28.0 (TID 56). 1799 bytes result sent to driver
2017-05-04 08:53:38,497 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 28.0 (TID 56) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,497 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 28.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,497 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 28 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:38,497 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,497 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,497 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 29)
2017-05-04 08:53:38,497 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,498 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 29 (ShuffledRDD[41] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,499 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_44 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,500 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_44_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,501 INFO  [dispatcher-event-loop-26] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_44_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,501 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 44 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,502 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 29 (ShuffledRDD[41] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,502 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 29.0 with 2 tasks
2017-05-04 08:53:38,503 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 29.0 (TID 58, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,504 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 29.0 (TID 59, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,504 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 29.0 (TID 58)
2017-05-04 08:53:38,504 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 29.0 (TID 59)
2017-05-04 08:53:38,506 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,506 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,506 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,506 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,508 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 29.0 (TID 58). 2820 bytes result sent to driver
2017-05-04 08:53:38,508 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 29.0 (TID 59). 2721 bytes result sent to driver
2017-05-04 08:53:38,509 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 29.0 (TID 59) in 6 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,509 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 29.0 (TID 58) in 7 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,509 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 29.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,509 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 29 (collectAsMap at KMeans.scala:295) finished in 0.007 s
2017-05-04 08:53:38,510 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 17 finished: collectAsMap at KMeans.scala:295, took 0.036152 s
2017-05-04 08:53:38,511 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(42) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,511 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_45 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,511 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_42_piece0 on 192.168.0.101:42487 in memory (size: 1280.0 B, free: 364.2 MB)
2017-05-04 08:53:38,513 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_45_piece0 stored as bytes in memory (estimated size 1319.0 B, free 363.9 MB)
2017-05-04 08:53:38,513 INFO  [dispatcher-event-loop-38] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_45_piece0 in memory on 192.168.0.101:42487 (size: 1319.0 B, free: 364.2 MB)
2017-05-04 08:53:38,514 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 45 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,530 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,531 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 42 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,532 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 18 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,532 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 31 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,532 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 30)
2017-05-04 08:53:38,532 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 30)
2017-05-04 08:53:38,533 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 30 (MapPartitionsRDD[42] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,534 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_46 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,536 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,536 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_46_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,537 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 46 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,537 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[42] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,537 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 30.0 with 2 tasks
2017-05-04 08:53:38,539 INFO  [dispatcher-event-loop-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 30.0 (TID 60, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,540 INFO  [dispatcher-event-loop-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 30.0 (TID 61, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,540 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 30.0 (TID 60)
2017-05-04 08:53:38,540 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 30.0 (TID 61)
2017-05-04 08:53:38,543 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,543 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,543 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,543 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,551 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 30.0 (TID 60). 1799 bytes result sent to driver
2017-05-04 08:53:38,552 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 30.0 (TID 60) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,553 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 30.0 (TID 61). 1799 bytes result sent to driver
2017-05-04 08:53:38,554 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 30.0 (TID 61) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,554 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 30.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 30 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:38,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 31)
2017-05-04 08:53:38,555 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,555 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 31 (ShuffledRDD[43] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,556 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_47 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,557 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_47_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,558 INFO  [dispatcher-event-loop-5] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_47_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,558 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 47 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,559 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 31 (ShuffledRDD[43] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,559 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 31.0 with 2 tasks
2017-05-04 08:53:38,561 INFO  [dispatcher-event-loop-9] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 31.0 (TID 62, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,561 INFO  [dispatcher-event-loop-9] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 31.0 (TID 63, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,562 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 31.0 (TID 62)
2017-05-04 08:53:38,562 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 31.0 (TID 63)
2017-05-04 08:53:38,563 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,564 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,564 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,564 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,565 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 31.0 (TID 63). 2721 bytes result sent to driver
2017-05-04 08:53:38,566 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 31.0 (TID 63) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,567 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 31.0 (TID 62). 2820 bytes result sent to driver
2017-05-04 08:53:38,568 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 31.0 (TID 62) in 8 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,568 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,568 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 31 (collectAsMap at KMeans.scala:295) finished in 0.009 s
2017-05-04 08:53:38,569 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 18 finished: collectAsMap at KMeans.scala:295, took 0.038226 s
2017-05-04 08:53:38,570 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(45) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,570 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_45_piece0 on 192.168.0.101:42487 in memory (size: 1319.0 B, free: 364.2 MB)
2017-05-04 08:53:38,571 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_48 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,572 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_48_piece0 stored as bytes in memory (estimated size 1277.0 B, free 363.9 MB)
2017-05-04 08:53:38,572 INFO  [dispatcher-event-loop-17] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_48_piece0 in memory on 192.168.0.101:42487 (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:38,573 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 48 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,588 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,589 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 44 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,589 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 19 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,589 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 33 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,589 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 32)
2017-05-04 08:53:38,589 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 32)
2017-05-04 08:53:38,590 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 32 (MapPartitionsRDD[44] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,592 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_49 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,593 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_49_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,594 INFO  [dispatcher-event-loop-16] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_49_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,594 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 49 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,595 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[44] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,595 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 32.0 with 2 tasks
2017-05-04 08:53:38,596 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 32.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,597 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 32.0 (TID 65, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,597 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 32.0 (TID 64)
2017-05-04 08:53:38,597 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 32.0 (TID 65)
2017-05-04 08:53:38,600 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,600 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,600 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,600 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,610 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 32.0 (TID 65). 1799 bytes result sent to driver
2017-05-04 08:53:38,610 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 32.0 (TID 65) in 13 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,611 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 32.0 (TID 64). 1799 bytes result sent to driver
2017-05-04 08:53:38,611 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 32.0 (TID 64) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,611 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,612 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 32 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:38,612 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,612 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,612 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 33)
2017-05-04 08:53:38,612 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,612 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 33 (ShuffledRDD[45] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,613 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_50 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,615 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_50_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,615 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_50_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,616 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 50 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,616 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 33 (ShuffledRDD[45] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,616 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 33.0 with 2 tasks
2017-05-04 08:53:38,618 INFO  [dispatcher-event-loop-21] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 33.0 (TID 66, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,618 INFO  [dispatcher-event-loop-21] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 33.0 (TID 67, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,619 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 33.0 (TID 66)
2017-05-04 08:53:38,619 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 33.0 (TID 67)
2017-05-04 08:53:38,620 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,620 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,620 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,620 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,622 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 33.0 (TID 66). 2820 bytes result sent to driver
2017-05-04 08:53:38,622 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 33.0 (TID 67). 2721 bytes result sent to driver
2017-05-04 08:53:38,622 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 33.0 (TID 66) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,622 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 33.0 (TID 67) in 4 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,622 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,623 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 33 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:38,623 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 19 finished: collectAsMap at KMeans.scala:295, took 0.034769 s
2017-05-04 08:53:38,624 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(48) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,625 INFO  [dispatcher-event-loop-32] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_48_piece0 on 192.168.0.101:42487 in memory (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:38,625 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_51 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,627 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_51_piece0 stored as bytes in memory (estimated size 1277.0 B, free 363.9 MB)
2017-05-04 08:53:38,627 INFO  [dispatcher-event-loop-34] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_51_piece0 in memory on 192.168.0.101:42487 (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:38,628 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 51 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,646 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,647 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 46 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,647 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 20 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,647 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 35 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,647 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 34)
2017-05-04 08:53:38,648 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 34)
2017-05-04 08:53:38,649 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 34 (MapPartitionsRDD[46] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,651 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_52 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,653 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,653 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_52_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,654 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 52 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,654 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[46] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,654 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 34.0 with 2 tasks
2017-05-04 08:53:38,656 INFO  [dispatcher-event-loop-38] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 34.0 (TID 68, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,657 INFO  [dispatcher-event-loop-38] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 34.0 (TID 69, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,657 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 34.0 (TID 69)
2017-05-04 08:53:38,657 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 34.0 (TID 68)
2017-05-04 08:53:38,659 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,659 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,660 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,660 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,667 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 34.0 (TID 69). 1799 bytes result sent to driver
2017-05-04 08:53:38,667 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 34.0 (TID 69) in 10 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,669 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 34.0 (TID 68). 1799 bytes result sent to driver
2017-05-04 08:53:38,670 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 34.0 (TID 68) in 14 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,670 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,670 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 34 (mapPartitions at KMeans.scala:276) finished in 0.015 s
2017-05-04 08:53:38,670 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,670 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,670 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 35)
2017-05-04 08:53:38,670 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,670 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 35 (ShuffledRDD[47] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,672 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_53 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,673 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_53_piece0 stored as bytes in memory (estimated size 1660.0 B, free 363.9 MB)
2017-05-04 08:53:38,673 INFO  [dispatcher-event-loop-3] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_53_piece0 in memory on 192.168.0.101:42487 (size: 1660.0 B, free: 364.2 MB)
2017-05-04 08:53:38,674 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 53 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,674 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 35 (ShuffledRDD[47] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,674 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 35.0 with 2 tasks
2017-05-04 08:53:38,676 INFO  [dispatcher-event-loop-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 35.0 (TID 70, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,676 INFO  [dispatcher-event-loop-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 35.0 (TID 71, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,677 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 35.0 (TID 70)
2017-05-04 08:53:38,677 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 35.0 (TID 71)
2017-05-04 08:53:38,678 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,678 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,678 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,678 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,680 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 35.0 (TID 71). 2721 bytes result sent to driver
2017-05-04 08:53:38,680 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 35.0 (TID 71) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,680 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 35.0 (TID 70). 2820 bytes result sent to driver
2017-05-04 08:53:38,681 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 35.0 (TID 70) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,681 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,681 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 35 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:38,682 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 20 finished: collectAsMap at KMeans.scala:295, took 0.035974 s
2017-05-04 08:53:38,683 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(51) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,684 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_51_piece0 on 192.168.0.101:42487 in memory (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:38,684 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_54 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,685 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_54_piece0 stored as bytes in memory (estimated size 1277.0 B, free 363.9 MB)
2017-05-04 08:53:38,686 INFO  [dispatcher-event-loop-13] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_54_piece0 in memory on 192.168.0.101:42487 (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:38,686 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 54 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,702 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,702 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 48 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,703 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 21 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,703 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 37 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,703 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 36)
2017-05-04 08:53:38,703 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 36)
2017-05-04 08:53:38,704 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 36 (MapPartitionsRDD[48] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,706 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_55 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,707 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_55_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,708 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_55_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,708 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 55 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,708 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[48] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,709 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 36.0 with 2 tasks
2017-05-04 08:53:38,711 INFO  [dispatcher-event-loop-17] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 36.0 (TID 72, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,711 INFO  [dispatcher-event-loop-17] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 36.0 (TID 73, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,712 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 36.0 (TID 73)
2017-05-04 08:53:38,712 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 36.0 (TID 72)
2017-05-04 08:53:38,715 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,715 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,715 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,715 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,726 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 36.0 (TID 72). 1799 bytes result sent to driver
2017-05-04 08:53:38,726 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 36.0 (TID 73). 1799 bytes result sent to driver
2017-05-04 08:53:38,727 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 36.0 (TID 72) in 17 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,727 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 36.0 (TID 73) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,727 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,727 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 36 (mapPartitions at KMeans.scala:276) finished in 0.018 s
2017-05-04 08:53:38,727 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,727 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,728 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 37)
2017-05-04 08:53:38,728 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,728 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 37 (ShuffledRDD[49] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,729 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_56 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,731 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_56_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,731 INFO  [dispatcher-event-loop-24] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_56_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,731 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 56 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,732 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 37 (ShuffledRDD[49] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,732 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 37.0 with 2 tasks
2017-05-04 08:53:38,733 INFO  [dispatcher-event-loop-25] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 37.0 (TID 74, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,734 INFO  [dispatcher-event-loop-25] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 37.0 (TID 75, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,734 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 37.0 (TID 74)
2017-05-04 08:53:38,734 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 37.0 (TID 75)
2017-05-04 08:53:38,736 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,736 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,736 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,736 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,737 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 37.0 (TID 74). 2820 bytes result sent to driver
2017-05-04 08:53:38,738 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 37.0 (TID 74) in 6 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,738 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 37.0 (TID 75). 2721 bytes result sent to driver
2017-05-04 08:53:38,738 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 37.0 (TID 75) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,739 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 37.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,739 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 37 (collectAsMap at KMeans.scala:295) finished in 0.007 s
2017-05-04 08:53:38,739 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 21 finished: collectAsMap at KMeans.scala:295, took 0.037248 s
2017-05-04 08:53:38,740 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(54) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,741 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_57 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,741 INFO  [dispatcher-event-loop-30] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_54_piece0 on 192.168.0.101:42487 in memory (size: 1277.0 B, free: 364.2 MB)
2017-05-04 08:53:38,742 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_57_piece0 stored as bytes in memory (estimated size 1267.0 B, free 363.9 MB)
2017-05-04 08:53:38,742 INFO  [dispatcher-event-loop-26] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_57_piece0 in memory on 192.168.0.101:42487 (size: 1267.0 B, free: 364.2 MB)
2017-05-04 08:53:38,743 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 57 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,759 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,760 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 50 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,761 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 22 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,761 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 39 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,761 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 38)
2017-05-04 08:53:38,761 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 38)
2017-05-04 08:53:38,762 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 38 (MapPartitionsRDD[50] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,763 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_58 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,765 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_58_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,766 INFO  [dispatcher-event-loop-32] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_58_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,766 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 58 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,766 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[50] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,767 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 38.0 with 2 tasks
2017-05-04 08:53:38,769 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 38.0 (TID 76, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,769 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 38.0 (TID 77, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,770 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 38.0 (TID 76)
2017-05-04 08:53:38,770 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 38.0 (TID 77)
2017-05-04 08:53:38,772 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,772 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,772 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,772 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,779 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 38.0 (TID 77). 1799 bytes result sent to driver
2017-05-04 08:53:38,780 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 38.0 (TID 77) in 10 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,785 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 38.0 (TID 76). 1799 bytes result sent to driver
2017-05-04 08:53:38,786 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 38.0 (TID 76) in 19 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,787 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 38.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,787 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 38 (mapPartitions at KMeans.scala:276) finished in 0.020 s
2017-05-04 08:53:38,787 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,787 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,787 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 39)
2017-05-04 08:53:38,787 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,787 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 39 (ShuffledRDD[51] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,789 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_59 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,790 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_59_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,791 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_59_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,791 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 59 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,791 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 39 (ShuffledRDD[51] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,792 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 39.0 with 2 tasks
2017-05-04 08:53:38,793 INFO  [dispatcher-event-loop-37] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 39.0 (TID 78, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,794 INFO  [dispatcher-event-loop-37] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 39.0 (TID 79, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,794 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 39.0 (TID 79)
2017-05-04 08:53:38,794 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 39.0 (TID 78)
2017-05-04 08:53:38,795 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,795 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,796 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,796 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,796 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 39.0 (TID 79). 2721 bytes result sent to driver
2017-05-04 08:53:38,797 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 39.0 (TID 79) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,797 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 39.0 (TID 78). 2820 bytes result sent to driver
2017-05-04 08:53:38,798 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 39.0 (TID 78) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,798 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 39.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,798 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 39 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:38,798 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 22 finished: collectAsMap at KMeans.scala:295, took 0.038782 s
2017-05-04 08:53:38,799 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(57) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,800 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_60 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:38,800 INFO  [dispatcher-event-loop-5] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_57_piece0 on 192.168.0.101:42487 in memory (size: 1267.0 B, free: 364.2 MB)
2017-05-04 08:53:38,802 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_60_piece0 stored as bytes in memory (estimated size 1272.0 B, free 363.9 MB)
2017-05-04 08:53:38,802 INFO  [dispatcher-event-loop-9] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_60_piece0 in memory on 192.168.0.101:42487 (size: 1272.0 B, free: 364.2 MB)
2017-05-04 08:53:38,803 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 60 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,819 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,820 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 52 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,820 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 23 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,820 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 41 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,821 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 40)
2017-05-04 08:53:38,821 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 40)
2017-05-04 08:53:38,822 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 40 (MapPartitionsRDD[52] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,823 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_61 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:38,838 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_61_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:38,838 INFO  [dispatcher-event-loop-12] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_61_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,839 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 16
2017-05-04 08:53:38,839 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 61 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,839 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[52] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,839 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 40.0 with 2 tasks
2017-05-04 08:53:38,839 INFO  [dispatcher-event-loop-14] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_58_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,842 INFO  [dispatcher-event-loop-14] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 40.0 (TID 80, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,842 INFO  [dispatcher-event-loop-25] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_59_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,843 INFO  [dispatcher-event-loop-14] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 40.0 (TID 81, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,843 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 40.0 (TID 81)
2017-05-04 08:53:38,843 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 40.0 (TID 80)
2017-05-04 08:53:38,843 INFO  [dispatcher-event-loop-21] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_53_piece0 on 192.168.0.101:42487 in memory (size: 1660.0 B, free: 364.2 MB)
2017-05-04 08:53:38,844 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 2606
2017-05-04 08:53:38,845 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,845 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 15
2017-05-04 08:53:38,845 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,846 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,846 INFO  [dispatcher-event-loop-34] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_55_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,846 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,848 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_56_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,848 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 2751
2017-05-04 08:53:38,850 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_49_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,851 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_50_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,852 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 2461
2017-05-04 08:53:38,853 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 14
2017-05-04 08:53:38,853 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_52_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,855 INFO  [dispatcher-event-loop-12] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_44_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,855 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 2171
2017-05-04 08:53:38,856 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 12
2017-05-04 08:53:38,857 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 40.0 (TID 81). 1799 bytes result sent to driver
2017-05-04 08:53:38,857 INFO  [dispatcher-event-loop-20] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_46_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,857 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 40.0 (TID 81) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,858 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 40.0 (TID 80). 1799 bytes result sent to driver
2017-05-04 08:53:38,858 INFO  [dispatcher-event-loop-29] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_47_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,858 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 40.0 (TID 80) in 18 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,858 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 40.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,858 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 2316
2017-05-04 08:53:38,858 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 40 (mapPartitions at KMeans.scala:276) finished in 0.018 s
2017-05-04 08:53:38,859 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,859 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,859 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 41)
2017-05-04 08:53:38,859 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,859 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 41 (ShuffledRDD[53] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,859 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 13
2017-05-04 08:53:38,860 INFO  [dispatcher-event-loop-34] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_40_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,861 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_62 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:38,861 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_41_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,862 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 2026
2017-05-04 08:53:38,862 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_62_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:38,862 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_62_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,863 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 11
2017-05-04 08:53:38,863 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 62 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,863 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 41 (ShuffledRDD[53] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,863 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 41.0 with 2 tasks
2017-05-04 08:53:38,863 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_43_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,864 INFO  [dispatcher-event-loop-9] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_37_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,865 INFO  [dispatcher-event-loop-5] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 41.0 (TID 82, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,865 INFO  [dispatcher-event-loop-5] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 41.0 (TID 83, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,865 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 41.0 (TID 83)
2017-05-04 08:53:38,865 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_38_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,865 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 41.0 (TID 82)
2017-05-04 08:53:38,866 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 1881
2017-05-04 08:53:38,867 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 10
2017-05-04 08:53:38,867 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,868 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,868 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,868 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,868 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 8
2017-05-04 08:53:38,869 INFO  [dispatcher-event-loop-14] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_34_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,870 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 41.0 (TID 82). 2820 bytes result sent to driver
2017-05-04 08:53:38,870 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_35_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,870 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 41.0 (TID 83). 2721 bytes result sent to driver
2017-05-04 08:53:38,870 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 1736
2017-05-04 08:53:38,870 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 41.0 (TID 83) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,871 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 41.0 (TID 82) in 7 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,871 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 41.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,871 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 9
2017-05-04 08:53:38,872 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 41 (collectAsMap at KMeans.scala:295) finished in 0.007 s
2017-05-04 08:53:38,872 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 23 finished: collectAsMap at KMeans.scala:295, took 0.052502 s
2017-05-04 08:53:38,872 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_29_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,873 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 1446
2017-05-04 08:53:38,873 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(60) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,873 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 7
2017-05-04 08:53:38,874 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_63 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,874 INFO  [dispatcher-event-loop-2] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_60_piece0 on 192.168.0.101:42487 in memory (size: 1272.0 B, free: 364.2 MB)
2017-05-04 08:53:38,874 INFO  [dispatcher-event-loop-2] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_31_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,875 INFO  [dispatcher-event-loop-9] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_32_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,875 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_63_piece0 stored as bytes in memory (estimated size 1273.0 B, free 364.0 MB)
2017-05-04 08:53:38,875 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 1591
2017-05-04 08:53:38,876 INFO  [dispatcher-event-loop-8] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_63_piece0 in memory on 192.168.0.101:42487 (size: 1273.0 B, free: 364.2 MB)
2017-05-04 08:53:38,877 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 63 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,877 INFO  [dispatcher-event-loop-11] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_25_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,878 INFO  [dispatcher-event-loop-17] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_26_piece0 on 192.168.0.101:42487 in memory (size: 1657.0 B, free: 364.2 MB)
2017-05-04 08:53:38,878 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 1301
2017-05-04 08:53:38,879 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 6
2017-05-04 08:53:38,880 INFO  [dispatcher-event-loop-25] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_28_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,880 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 1156
2017-05-04 08:53:38,881 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 5
2017-05-04 08:53:38,892 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,893 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 54 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,894 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 24 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,894 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 43 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,894 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 42)
2017-05-04 08:53:38,894 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 42)
2017-05-04 08:53:38,895 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 42 (MapPartitionsRDD[54] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,896 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_64 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:38,898 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_64_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:38,898 INFO  [dispatcher-event-loop-26] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_64_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,898 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 64 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,899 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[54] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,899 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 42.0 with 2 tasks
2017-05-04 08:53:38,901 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 42.0 (TID 84, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,901 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 42.0 (TID 85, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,902 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 42.0 (TID 84)
2017-05-04 08:53:38,902 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 42.0 (TID 85)
2017-05-04 08:53:38,904 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,904 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,904 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,904 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,915 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 42.0 (TID 84). 1799 bytes result sent to driver
2017-05-04 08:53:38,915 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 42.0 (TID 85). 1799 bytes result sent to driver
2017-05-04 08:53:38,916 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 42.0 (TID 84) in 17 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,916 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 42.0 (TID 85) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,916 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 42.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,916 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 42 (mapPartitions at KMeans.scala:276) finished in 0.017 s
2017-05-04 08:53:38,916 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,916 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 43)
2017-05-04 08:53:38,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 43 (ShuffledRDD[55] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,918 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_65 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:38,920 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_65_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:38,920 INFO  [dispatcher-event-loop-36] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_65_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,921 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 65 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,921 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 43 (ShuffledRDD[55] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,921 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 43.0 with 2 tasks
2017-05-04 08:53:38,922 INFO  [dispatcher-event-loop-31] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 43.0 (TID 86, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,922 INFO  [dispatcher-event-loop-31] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 43.0 (TID 87, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,922 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 43.0 (TID 86)
2017-05-04 08:53:38,922 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 43.0 (TID 87)
2017-05-04 08:53:38,924 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,924 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,925 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,925 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:38,926 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 43.0 (TID 87). 2721 bytes result sent to driver
2017-05-04 08:53:38,926 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 43.0 (TID 86). 2820 bytes result sent to driver
2017-05-04 08:53:38,927 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 43.0 (TID 87) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,927 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 43.0 (TID 86) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,927 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 43.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,927 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 43 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:38,928 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 24 finished: collectAsMap at KMeans.scala:295, took 0.034968 s
2017-05-04 08:53:38,928 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(63) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,929 INFO  [dispatcher-event-loop-3] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_63_piece0 on 192.168.0.101:42487 in memory (size: 1273.0 B, free: 364.2 MB)
2017-05-04 08:53:38,929 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_66 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,931 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_66_piece0 stored as bytes in memory (estimated size 1267.0 B, free 364.0 MB)
2017-05-04 08:53:38,931 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_66_piece0 in memory on 192.168.0.101:42487 (size: 1267.0 B, free: 364.2 MB)
2017-05-04 08:53:38,932 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 66 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,946 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,947 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 56 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,947 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 25 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,947 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 45 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,947 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 44)
2017-05-04 08:53:38,947 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 44)
2017-05-04 08:53:38,948 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 44 (MapPartitionsRDD[56] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,950 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_67 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:38,951 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_67_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:38,951 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_67_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:38,952 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 67 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,952 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[56] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,952 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 44.0 with 2 tasks
2017-05-04 08:53:38,954 INFO  [dispatcher-event-loop-9] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 44.0 (TID 88, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,954 INFO  [dispatcher-event-loop-9] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 44.0 (TID 89, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:38,955 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 44.0 (TID 88)
2017-05-04 08:53:38,955 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 44.0 (TID 89)
2017-05-04 08:53:38,957 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:38,957 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:38,957 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:38,957 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:38,967 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 44.0 (TID 89). 1799 bytes result sent to driver
2017-05-04 08:53:38,967 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 44.0 (TID 88). 1799 bytes result sent to driver
2017-05-04 08:53:38,967 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 44.0 (TID 89) in 13 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,968 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 44.0 (TID 88) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,968 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 44.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,968 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 44 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:38,968 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:38,968 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:38,968 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 45)
2017-05-04 08:53:38,968 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:38,969 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 45 (ShuffledRDD[57] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:38,970 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_68 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:38,971 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_68_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:38,971 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_68_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:38,972 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 68 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:38,972 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 45 (ShuffledRDD[57] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:38,972 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 45.0 with 2 tasks
2017-05-04 08:53:38,973 INFO  [dispatcher-event-loop-10] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 45.0 (TID 90, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:38,974 INFO  [dispatcher-event-loop-10] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 45.0 (TID 91, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:38,974 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 45.0 (TID 90)
2017-05-04 08:53:38,974 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 45.0 (TID 91)
2017-05-04 08:53:38,976 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,976 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:38,976 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,976 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:38,977 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 45.0 (TID 91). 2721 bytes result sent to driver
2017-05-04 08:53:38,977 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 45.0 (TID 90). 2820 bytes result sent to driver
2017-05-04 08:53:38,978 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 45.0 (TID 91) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:38,978 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 45.0 (TID 90) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:38,978 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 45.0, whose tasks have all completed, from pool 
2017-05-04 08:53:38,978 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 45 (collectAsMap at KMeans.scala:295) finished in 0.005 s
2017-05-04 08:53:38,979 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 25 finished: collectAsMap at KMeans.scala:295, took 0.032692 s
2017-05-04 08:53:38,980 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(66) (from destroy at KMeans.scala:297)
2017-05-04 08:53:38,980 INFO  [dispatcher-event-loop-20] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_66_piece0 on 192.168.0.101:42487 in memory (size: 1267.0 B, free: 364.2 MB)
2017-05-04 08:53:38,981 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_69 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:38,982 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_69_piece0 stored as bytes in memory (estimated size 1270.0 B, free 364.0 MB)
2017-05-04 08:53:38,982 INFO  [dispatcher-event-loop-25] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_69_piece0 in memory on 192.168.0.101:42487 (size: 1270.0 B, free: 364.2 MB)
2017-05-04 08:53:38,983 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 69 from broadcast at KMeans.scala:273
2017-05-04 08:53:38,996 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:38,997 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 58 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:38,997 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 26 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:38,997 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 47 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:38,997 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 46)
2017-05-04 08:53:38,997 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 46)
2017-05-04 08:53:38,998 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 46 (MapPartitionsRDD[58] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:38,999 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_70 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:39,001 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_70_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:39,001 INFO  [dispatcher-event-loop-29] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_70_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,002 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 70 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,002 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[58] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,002 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 46.0 with 2 tasks
2017-05-04 08:53:39,004 INFO  [dispatcher-event-loop-14] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 46.0 (TID 92, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,005 INFO  [dispatcher-event-loop-14] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 46.0 (TID 93, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,005 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 46.0 (TID 92)
2017-05-04 08:53:39,005 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 46.0 (TID 93)
2017-05-04 08:53:39,007 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,007 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,007 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,007 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,018 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 46.0 (TID 93). 1799 bytes result sent to driver
2017-05-04 08:53:39,018 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 46.0 (TID 92). 1799 bytes result sent to driver
2017-05-04 08:53:39,019 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 46.0 (TID 93) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,019 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 46.0 (TID 92) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,019 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 46.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 46 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:39,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 47)
2017-05-04 08:53:39,019 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,020 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 47 (ShuffledRDD[59] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,021 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_71 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:39,022 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_71_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:39,023 INFO  [dispatcher-event-loop-27] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_71_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,023 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 71 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,023 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 47 (ShuffledRDD[59] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,024 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 47.0 with 2 tasks
2017-05-04 08:53:39,025 INFO  [dispatcher-event-loop-33] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 47.0 (TID 94, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,025 INFO  [dispatcher-event-loop-33] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 47.0 (TID 95, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,025 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 47.0 (TID 94)
2017-05-04 08:53:39,026 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 47.0 (TID 95)
2017-05-04 08:53:39,027 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,027 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,027 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,027 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,029 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 47.0 (TID 94). 2820 bytes result sent to driver
2017-05-04 08:53:39,029 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 47.0 (TID 95). 2721 bytes result sent to driver
2017-05-04 08:53:39,029 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 47.0 (TID 94) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,030 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 47.0 (TID 95) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,030 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 47.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,030 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 47 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,030 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 26 finished: collectAsMap at KMeans.scala:295, took 0.034346 s
2017-05-04 08:53:39,031 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(69) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,032 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_69_piece0 on 192.168.0.101:42487 in memory (size: 1270.0 B, free: 364.2 MB)
2017-05-04 08:53:39,032 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_72 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:39,033 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_72_piece0 stored as bytes in memory (estimated size 1270.0 B, free 364.0 MB)
2017-05-04 08:53:39,034 INFO  [dispatcher-event-loop-37] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_72_piece0 in memory on 192.168.0.101:42487 (size: 1270.0 B, free: 364.2 MB)
2017-05-04 08:53:39,034 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 72 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,049 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,050 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 60 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,051 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 27 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,051 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 49 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,051 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 48)
2017-05-04 08:53:39,051 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 48)
2017-05-04 08:53:39,052 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 48 (MapPartitionsRDD[60] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,053 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_73 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:39,054 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_73_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:39,055 INFO  [dispatcher-event-loop-3] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_73_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,055 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 73 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,056 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[60] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,056 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 48.0 with 2 tasks
2017-05-04 08:53:39,057 INFO  [dispatcher-event-loop-4] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 48.0 (TID 96, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,058 INFO  [dispatcher-event-loop-4] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 48.0 (TID 97, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,058 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 48.0 (TID 97)
2017-05-04 08:53:39,058 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 48.0 (TID 96)
2017-05-04 08:53:39,060 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,060 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,060 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,060 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,071 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 48.0 (TID 96). 1799 bytes result sent to driver
2017-05-04 08:53:39,071 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 48.0 (TID 97). 1799 bytes result sent to driver
2017-05-04 08:53:39,071 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 48.0 (TID 96) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,072 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 48.0 (TID 97) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,072 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 48.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,072 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 48 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:39,072 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,072 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,072 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 49)
2017-05-04 08:53:39,072 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,073 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 49 (ShuffledRDD[61] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,074 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_74 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:39,075 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_74_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:39,076 INFO  [dispatcher-event-loop-13] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_74_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,076 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 74 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,077 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 49 (ShuffledRDD[61] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,077 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 49.0 with 2 tasks
2017-05-04 08:53:39,078 INFO  [dispatcher-event-loop-5] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 49.0 (TID 98, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,078 INFO  [dispatcher-event-loop-5] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 49.0 (TID 99, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,078 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 49.0 (TID 98)
2017-05-04 08:53:39,078 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 49.0 (TID 99)
2017-05-04 08:53:39,080 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,080 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,080 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,081 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,082 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 49.0 (TID 99). 2721 bytes result sent to driver
2017-05-04 08:53:39,082 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 49.0 (TID 98). 2820 bytes result sent to driver
2017-05-04 08:53:39,083 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 49.0 (TID 99) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,083 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 49.0 (TID 98) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,083 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 49.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,083 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 49 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,084 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 27 finished: collectAsMap at KMeans.scala:295, took 0.034225 s
2017-05-04 08:53:39,085 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(72) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,086 INFO  [dispatcher-event-loop-18] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_72_piece0 on 192.168.0.101:42487 in memory (size: 1270.0 B, free: 364.2 MB)
2017-05-04 08:53:39,086 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_75 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:39,087 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_75_piece0 stored as bytes in memory (estimated size 1264.0 B, free 364.0 MB)
2017-05-04 08:53:39,088 INFO  [dispatcher-event-loop-12] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_75_piece0 in memory on 192.168.0.101:42487 (size: 1264.0 B, free: 364.2 MB)
2017-05-04 08:53:39,088 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 75 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,104 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,105 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 62 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,105 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 28 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,105 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 51 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,105 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 50)
2017-05-04 08:53:39,105 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 50)
2017-05-04 08:53:39,106 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 50 (MapPartitionsRDD[62] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,108 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_76 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:39,109 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_76_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,110 INFO  [dispatcher-event-loop-20] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_76_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,110 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 76 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,111 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[62] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,111 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 50.0 with 2 tasks
2017-05-04 08:53:39,113 INFO  [dispatcher-event-loop-25] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 50.0 (TID 100, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,114 INFO  [dispatcher-event-loop-25] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 50.0 (TID 101, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,114 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 50.0 (TID 100)
2017-05-04 08:53:39,114 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 50.0 (TID 101)
2017-05-04 08:53:39,116 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,116 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,116 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,117 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,128 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 50.0 (TID 100). 1799 bytes result sent to driver
2017-05-04 08:53:39,128 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 50.0 (TID 101). 1799 bytes result sent to driver
2017-05-04 08:53:39,128 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 50.0 (TID 100) in 17 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,129 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 50.0 (TID 101) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,129 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 50.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 50 (mapPartitions at KMeans.scala:276) finished in 0.018 s
2017-05-04 08:53:39,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 51)
2017-05-04 08:53:39,129 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,130 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 51 (ShuffledRDD[63] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,131 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_77 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,133 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_77_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,133 INFO  [dispatcher-event-loop-21] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_77_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,133 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 77 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,134 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 51 (ShuffledRDD[63] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,134 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 51.0 with 2 tasks
2017-05-04 08:53:39,135 INFO  [dispatcher-event-loop-22] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 51.0 (TID 102, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,136 INFO  [dispatcher-event-loop-22] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 51.0 (TID 103, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,136 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 51.0 (TID 102)
2017-05-04 08:53:39,136 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 51.0 (TID 103)
2017-05-04 08:53:39,138 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,138 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,138 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,138 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,140 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 51.0 (TID 102). 2820 bytes result sent to driver
2017-05-04 08:53:39,140 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 51.0 (TID 103). 2721 bytes result sent to driver
2017-05-04 08:53:39,140 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 51.0 (TID 102) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,141 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 51.0 (TID 103) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,141 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 51.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,141 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 51 (collectAsMap at KMeans.scala:295) finished in 0.007 s
2017-05-04 08:53:39,141 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 28 finished: collectAsMap at KMeans.scala:295, took 0.037144 s
2017-05-04 08:53:39,142 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(75) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,143 INFO  [dispatcher-event-loop-31] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_75_piece0 on 192.168.0.101:42487 in memory (size: 1264.0 B, free: 364.2 MB)
2017-05-04 08:53:39,143 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_78 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,145 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_78_piece0 stored as bytes in memory (estimated size 1270.0 B, free 363.9 MB)
2017-05-04 08:53:39,145 INFO  [dispatcher-event-loop-36] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_78_piece0 in memory on 192.168.0.101:42487 (size: 1270.0 B, free: 364.2 MB)
2017-05-04 08:53:39,146 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 78 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,161 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,161 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 64 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,162 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 29 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,162 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 53 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,162 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 52)
2017-05-04 08:53:39,162 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 52)
2017-05-04 08:53:39,163 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 52 (MapPartitionsRDD[64] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,164 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_79 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,165 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_79_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,166 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_79_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,166 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 79 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,167 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[64] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,167 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 52.0 with 2 tasks
2017-05-04 08:53:39,168 INFO  [dispatcher-event-loop-37] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 52.0 (TID 104, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,169 INFO  [dispatcher-event-loop-37] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 52.0 (TID 105, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,169 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 52.0 (TID 104)
2017-05-04 08:53:39,169 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 52.0 (TID 105)
2017-05-04 08:53:39,172 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,172 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,172 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,172 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,182 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 52.0 (TID 104). 1799 bytes result sent to driver
2017-05-04 08:53:39,183 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 52.0 (TID 104) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,184 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 52.0 (TID 105). 1799 bytes result sent to driver
2017-05-04 08:53:39,184 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 52.0 (TID 105) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,184 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 52.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,185 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 52 (mapPartitions at KMeans.scala:276) finished in 0.018 s
2017-05-04 08:53:39,185 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,185 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,185 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 53)
2017-05-04 08:53:39,185 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,185 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 53 (ShuffledRDD[65] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,186 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_80 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,188 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_80_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,188 INFO  [dispatcher-event-loop-2] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_80_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,189 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 80 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,189 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 53 (ShuffledRDD[65] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,189 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 53.0 with 2 tasks
2017-05-04 08:53:39,190 INFO  [dispatcher-event-loop-6] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 53.0 (TID 106, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,191 INFO  [dispatcher-event-loop-6] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 53.0 (TID 107, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,191 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 53.0 (TID 106)
2017-05-04 08:53:39,191 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 53.0 (TID 107)
2017-05-04 08:53:39,192 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,193 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,193 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,193 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,194 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 53.0 (TID 107). 2721 bytes result sent to driver
2017-05-04 08:53:39,194 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 53.0 (TID 106). 2820 bytes result sent to driver
2017-05-04 08:53:39,195 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 53.0 (TID 107) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,195 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 53.0 (TID 106) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,195 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 53.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,195 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 53 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,196 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 29 finished: collectAsMap at KMeans.scala:295, took 0.035127 s
2017-05-04 08:53:39,197 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(78) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,198 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_81 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,198 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_78_piece0 on 192.168.0.101:42487 in memory (size: 1270.0 B, free: 364.2 MB)
2017-05-04 08:53:39,199 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_81_piece0 stored as bytes in memory (estimated size 1275.0 B, free 363.9 MB)
2017-05-04 08:53:39,200 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_81_piece0 in memory on 192.168.0.101:42487 (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,200 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 81 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,216 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,217 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 66 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,217 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 30 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,218 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 55 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,218 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 54)
2017-05-04 08:53:39,218 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 54)
2017-05-04 08:53:39,218 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 54 (MapPartitionsRDD[66] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,220 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_82 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,221 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_82_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,221 INFO  [dispatcher-event-loop-18] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_82_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,222 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 82 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,222 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[66] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,222 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 54.0 with 2 tasks
2017-05-04 08:53:39,224 INFO  [dispatcher-event-loop-12] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 54.0 (TID 108, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,225 INFO  [dispatcher-event-loop-12] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 54.0 (TID 109, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,225 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 54.0 (TID 108)
2017-05-04 08:53:39,225 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 54.0 (TID 109)
2017-05-04 08:53:39,227 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,227 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,227 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,227 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,237 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 54.0 (TID 108). 1799 bytes result sent to driver
2017-05-04 08:53:39,237 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 54.0 (TID 109). 1799 bytes result sent to driver
2017-05-04 08:53:39,238 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 54.0 (TID 108) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,238 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 54.0 (TID 109) in 14 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,238 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 54.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,238 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 54 (mapPartitions at KMeans.scala:276) finished in 0.015 s
2017-05-04 08:53:39,239 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,239 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,239 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 55)
2017-05-04 08:53:39,239 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,239 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 55 (ShuffledRDD[67] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,240 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_83 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,242 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_83_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,242 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_83_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,243 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 83 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,243 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 55 (ShuffledRDD[67] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,243 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 55.0 with 2 tasks
2017-05-04 08:53:39,244 INFO  [dispatcher-event-loop-23] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 55.0 (TID 110, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,245 INFO  [dispatcher-event-loop-23] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 55.0 (TID 111, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,245 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 55.0 (TID 110)
2017-05-04 08:53:39,245 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 55.0 (TID 111)
2017-05-04 08:53:39,246 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,246 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,246 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,247 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,248 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 55.0 (TID 111). 2721 bytes result sent to driver
2017-05-04 08:53:39,248 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 55.0 (TID 110). 2820 bytes result sent to driver
2017-05-04 08:53:39,249 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 55.0 (TID 111) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,249 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 55.0 (TID 110) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,249 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 55.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,249 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 55 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,249 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 30 finished: collectAsMap at KMeans.scala:295, took 0.032746 s
2017-05-04 08:53:39,250 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(81) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,251 INFO  [dispatcher-event-loop-33] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_81_piece0 on 192.168.0.101:42487 in memory (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,251 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_84 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,253 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_84_piece0 stored as bytes in memory (estimated size 1272.0 B, free 363.9 MB)
2017-05-04 08:53:39,253 INFO  [dispatcher-event-loop-27] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_84_piece0 in memory on 192.168.0.101:42487 (size: 1272.0 B, free: 364.2 MB)
2017-05-04 08:53:39,253 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 84 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,266 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,267 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 68 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,267 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 31 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,268 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 57 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,268 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 56)
2017-05-04 08:53:39,268 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 56)
2017-05-04 08:53:39,269 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 56 (MapPartitionsRDD[68] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,270 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_85 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,271 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_85_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,272 INFO  [dispatcher-event-loop-31] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_85_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,272 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 85 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,272 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[68] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,272 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 56.0 with 2 tasks
2017-05-04 08:53:39,274 INFO  [dispatcher-event-loop-36] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 56.0 (TID 112, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,275 INFO  [dispatcher-event-loop-36] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 56.0 (TID 113, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,275 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 56.0 (TID 113)
2017-05-04 08:53:39,275 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 56.0 (TID 112)
2017-05-04 08:53:39,278 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,278 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,278 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,278 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,289 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 56.0 (TID 112). 1799 bytes result sent to driver
2017-05-04 08:53:39,289 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 56.0 (TID 113). 1799 bytes result sent to driver
2017-05-04 08:53:39,290 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 56.0 (TID 112) in 17 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,290 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 56.0 (TID 113) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,290 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 56.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,290 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 56 (mapPartitions at KMeans.scala:276) finished in 0.017 s
2017-05-04 08:53:39,291 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,291 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,291 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 57)
2017-05-04 08:53:39,291 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,291 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 57 (ShuffledRDD[69] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,292 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_86 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,294 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_86_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,294 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_86_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,295 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 86 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,295 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 57 (ShuffledRDD[69] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,295 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 57.0 with 2 tasks
2017-05-04 08:53:39,296 INFO  [dispatcher-event-loop-38] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 57.0 (TID 114, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,297 INFO  [dispatcher-event-loop-38] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 57.0 (TID 115, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,297 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 57.0 (TID 114)
2017-05-04 08:53:39,297 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 57.0 (TID 115)
2017-05-04 08:53:39,299 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,299 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,299 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,299 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,301 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 57.0 (TID 115). 2721 bytes result sent to driver
2017-05-04 08:53:39,301 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 57.0 (TID 114). 2820 bytes result sent to driver
2017-05-04 08:53:39,302 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 57.0 (TID 115) in 6 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,302 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 57.0 (TID 114) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,302 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 57.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,302 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 57 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,302 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 31 finished: collectAsMap at KMeans.scala:295, took 0.035872 s
2017-05-04 08:53:39,303 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(84) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,304 INFO  [dispatcher-event-loop-5] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_84_piece0 on 192.168.0.101:42487 in memory (size: 1272.0 B, free: 364.2 MB)
2017-05-04 08:53:39,304 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_87 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,306 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_87_piece0 stored as bytes in memory (estimated size 1275.0 B, free 363.9 MB)
2017-05-04 08:53:39,306 INFO  [dispatcher-event-loop-13] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_87_piece0 in memory on 192.168.0.101:42487 (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,306 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 87 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,320 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,321 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 70 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,321 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 32 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,321 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 59 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,322 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 58)
2017-05-04 08:53:39,322 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 58)
2017-05-04 08:53:39,322 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 58 (MapPartitionsRDD[70] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,324 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_88 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,325 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_88_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,326 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_88_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,326 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 88 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,327 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[70] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,327 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 58.0 with 2 tasks
2017-05-04 08:53:39,329 INFO  [dispatcher-event-loop-15] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 58.0 (TID 116, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,329 INFO  [dispatcher-event-loop-15] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 58.0 (TID 117, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,330 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 58.0 (TID 116)
2017-05-04 08:53:39,330 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 58.0 (TID 117)
2017-05-04 08:53:39,331 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,331 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,332 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,332 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,340 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 58.0 (TID 117). 1799 bytes result sent to driver
2017-05-04 08:53:39,341 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 58.0 (TID 117) in 12 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,341 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 58.0 (TID 116). 1799 bytes result sent to driver
2017-05-04 08:53:39,342 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 58.0 (TID 116) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,342 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 58.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,342 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 58 (mapPartitions at KMeans.scala:276) finished in 0.015 s
2017-05-04 08:53:39,342 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,342 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,342 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 59)
2017-05-04 08:53:39,342 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,343 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 59 (ShuffledRDD[71] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,344 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_89 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,345 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_89_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,346 INFO  [dispatcher-event-loop-24] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_89_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,346 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 89 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,346 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 59 (ShuffledRDD[71] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,347 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 59.0 with 2 tasks
2017-05-04 08:53:39,347 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 59.0 (TID 118, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,348 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 59.0 (TID 119, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,348 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 59.0 (TID 118)
2017-05-04 08:53:39,348 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 59.0 (TID 119)
2017-05-04 08:53:39,349 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,350 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,350 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,350 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,351 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 59.0 (TID 118). 2820 bytes result sent to driver
2017-05-04 08:53:39,351 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 59.0 (TID 119). 2721 bytes result sent to driver
2017-05-04 08:53:39,351 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 59.0 (TID 118) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,352 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 59.0 (TID 119) in 4 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,352 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 59.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,352 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 59 (collectAsMap at KMeans.scala:295) finished in 0.005 s
2017-05-04 08:53:39,352 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 32 finished: collectAsMap at KMeans.scala:295, took 0.032065 s
2017-05-04 08:53:39,353 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(87) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,354 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_90 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,354 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_87_piece0 on 192.168.0.101:42487 in memory (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,355 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_90_piece0 stored as bytes in memory (estimated size 1264.0 B, free 363.9 MB)
2017-05-04 08:53:39,356 INFO  [dispatcher-event-loop-21] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_90_piece0 in memory on 192.168.0.101:42487 (size: 1264.0 B, free: 364.2 MB)
2017-05-04 08:53:39,356 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 90 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,372 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,373 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 72 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,373 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 33 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,373 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 61 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,373 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 60)
2017-05-04 08:53:39,373 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 60)
2017-05-04 08:53:39,374 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 60 (MapPartitionsRDD[72] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,375 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_91 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,378 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_91_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,379 INFO  [dispatcher-event-loop-33] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_91_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,379 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 91 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,379 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[72] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,380 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 60.0 with 2 tasks
2017-05-04 08:53:39,381 INFO  [dispatcher-event-loop-27] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 60.0 (TID 120, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,382 INFO  [dispatcher-event-loop-27] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 60.0 (TID 121, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,384 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 60.0 (TID 121)
2017-05-04 08:53:39,384 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 60.0 (TID 120)
2017-05-04 08:53:39,386 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,386 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,386 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,386 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,396 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 60.0 (TID 121). 1799 bytes result sent to driver
2017-05-04 08:53:39,396 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 60.0 (TID 121) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,397 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 60.0 (TID 120). 1799 bytes result sent to driver
2017-05-04 08:53:39,397 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 60.0 (TID 120) in 17 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,397 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 60.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,398 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 60 (mapPartitions at KMeans.scala:276) finished in 0.017 s
2017-05-04 08:53:39,398 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,398 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,398 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 61)
2017-05-04 08:53:39,398 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,398 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 61 (ShuffledRDD[73] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,399 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_92 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,401 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_92_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,401 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_92_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,402 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 92 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,402 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 61 (ShuffledRDD[73] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,402 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 61.0 with 2 tasks
2017-05-04 08:53:39,403 INFO  [dispatcher-event-loop-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 61.0 (TID 122, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,404 INFO  [dispatcher-event-loop-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 61.0 (TID 123, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,404 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 61.0 (TID 123)
2017-05-04 08:53:39,404 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 61.0 (TID 122)
2017-05-04 08:53:39,406 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,406 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,407 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,407 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,407 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 61.0 (TID 123). 2721 bytes result sent to driver
2017-05-04 08:53:39,408 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 61.0 (TID 123) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,408 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 61.0 (TID 122). 2820 bytes result sent to driver
2017-05-04 08:53:39,409 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 61.0 (TID 122) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,409 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 61.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,409 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 61 (collectAsMap at KMeans.scala:295) finished in 0.007 s
2017-05-04 08:53:39,409 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 33 finished: collectAsMap at KMeans.scala:295, took 0.037149 s
2017-05-04 08:53:39,410 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(90) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,411 INFO  [dispatcher-event-loop-6] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_90_piece0 on 192.168.0.101:42487 in memory (size: 1264.0 B, free: 364.2 MB)
2017-05-04 08:53:39,411 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_93 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,412 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_93_piece0 stored as bytes in memory (estimated size 1275.0 B, free 363.9 MB)
2017-05-04 08:53:39,413 INFO  [dispatcher-event-loop-2] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_93_piece0 in memory on 192.168.0.101:42487 (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,414 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 93 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,429 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,430 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 74 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,430 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 34 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,431 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 63 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,431 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 62)
2017-05-04 08:53:39,431 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 62)
2017-05-04 08:53:39,431 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 62 (MapPartitionsRDD[74] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,433 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_94 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,435 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_94_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,436 INFO  [dispatcher-event-loop-5] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_94_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,436 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 94 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,436 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[74] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,437 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 62.0 with 2 tasks
2017-05-04 08:53:39,438 INFO  [dispatcher-event-loop-13] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 62.0 (TID 124, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,439 INFO  [dispatcher-event-loop-13] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 62.0 (TID 125, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,439 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 62.0 (TID 124)
2017-05-04 08:53:39,439 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 62.0 (TID 125)
2017-05-04 08:53:39,442 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,442 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,442 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,442 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,452 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 62.0 (TID 125). 1799 bytes result sent to driver
2017-05-04 08:53:39,452 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 62.0 (TID 124). 1799 bytes result sent to driver
2017-05-04 08:53:39,453 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 62.0 (TID 125) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,453 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 62.0 (TID 124) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,453 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 62.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,453 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 62 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:39,453 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,454 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,454 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 63)
2017-05-04 08:53:39,454 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,454 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 63 (ShuffledRDD[75] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,455 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_95 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,457 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_95_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,457 INFO  [dispatcher-event-loop-16] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_95_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,458 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 95 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,458 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 63 (ShuffledRDD[75] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,458 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 63.0 with 2 tasks
2017-05-04 08:53:39,460 INFO  [dispatcher-event-loop-17] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 63.0 (TID 126, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,460 INFO  [dispatcher-event-loop-17] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 63.0 (TID 127, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,461 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 63.0 (TID 126)
2017-05-04 08:53:39,462 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 63.0 (TID 127)
2017-05-04 08:53:39,463 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,463 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,463 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,463 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,464 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 63.0 (TID 126). 2733 bytes result sent to driver
2017-05-04 08:53:39,465 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 63.0 (TID 126) in 6 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,465 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 63.0 (TID 127). 2721 bytes result sent to driver
2017-05-04 08:53:39,467 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 63.0 (TID 127) in 6 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,467 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 63.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,467 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 63 (collectAsMap at KMeans.scala:295) finished in 0.008 s
2017-05-04 08:53:39,468 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 34 finished: collectAsMap at KMeans.scala:295, took 0.038587 s
2017-05-04 08:53:39,469 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(93) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,470 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_93_piece0 on 192.168.0.101:42487 in memory (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,470 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_96 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,471 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_96_piece0 stored as bytes in memory (estimated size 1264.0 B, free 363.9 MB)
2017-05-04 08:53:39,471 INFO  [dispatcher-event-loop-23] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_96_piece0 in memory on 192.168.0.101:42487 (size: 1264.0 B, free: 364.2 MB)
2017-05-04 08:53:39,472 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 96 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,485 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,485 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 76 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,486 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 35 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,486 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 65 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,486 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 64)
2017-05-04 08:53:39,486 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 64)
2017-05-04 08:53:39,487 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 64 (MapPartitionsRDD[76] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,488 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_97 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,489 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_97_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,489 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_97_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,490 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 97 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,490 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 64 (MapPartitionsRDD[76] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,490 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 64.0 with 2 tasks
2017-05-04 08:53:39,492 INFO  [dispatcher-event-loop-21] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 64.0 (TID 128, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,493 INFO  [dispatcher-event-loop-21] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 64.0 (TID 129, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,493 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 64.0 (TID 128)
2017-05-04 08:53:39,493 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 64.0 (TID 129)
2017-05-04 08:53:39,495 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,495 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,495 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,495 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,505 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 64.0 (TID 129). 1886 bytes result sent to driver
2017-05-04 08:53:39,506 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 64.0 (TID 129) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,506 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 64.0 (TID 128). 1799 bytes result sent to driver
2017-05-04 08:53:39,507 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 64.0 (TID 128) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,507 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 64.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,507 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 64 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:39,507 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,507 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,507 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 65)
2017-05-04 08:53:39,507 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,508 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 65 (ShuffledRDD[77] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,508 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_98 stored as values in memory (estimated size 2.8 KB, free 363.9 MB)
2017-05-04 08:53:39,510 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_98_piece0 stored as bytes in memory (estimated size 1659.0 B, free 363.9 MB)
2017-05-04 08:53:39,510 INFO  [dispatcher-event-loop-34] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_98_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,511 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 98 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,511 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 65 (ShuffledRDD[77] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,511 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 65.0 with 2 tasks
2017-05-04 08:53:39,512 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 65.0 (TID 130, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,512 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 65.0 (TID 131, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,513 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 65.0 (TID 130)
2017-05-04 08:53:39,513 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 65.0 (TID 131)
2017-05-04 08:53:39,514 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,514 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,514 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,514 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,516 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 65.0 (TID 131). 2808 bytes result sent to driver
2017-05-04 08:53:39,516 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 65.0 (TID 130). 2733 bytes result sent to driver
2017-05-04 08:53:39,516 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 65.0 (TID 131) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,516 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 65.0 (TID 130) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,517 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 65.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,517 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 65 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,517 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 35 finished: collectAsMap at KMeans.scala:295, took 0.032256 s
2017-05-04 08:53:39,518 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(96) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,518 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_96_piece0 on 192.168.0.101:42487 in memory (size: 1264.0 B, free: 364.2 MB)
2017-05-04 08:53:39,518 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_99 stored as values in memory (estimated size 1784.0 B, free 363.9 MB)
2017-05-04 08:53:39,537 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_99_piece0 stored as bytes in memory (estimated size 1271.0 B, free 363.9 MB)
2017-05-04 08:53:39,537 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_70_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,537 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_99_piece0 in memory on 192.168.0.101:42487 (size: 1271.0 B, free: 364.2 MB)
2017-05-04 08:53:39,538 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 99 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,538 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_92_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,539 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 4491
2017-05-04 08:53:39,540 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 28
2017-05-04 08:53:39,540 INFO  [dispatcher-event-loop-16] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_94_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,541 INFO  [dispatcher-event-loop-12] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_95_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,542 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 4636
2017-05-04 08:53:39,543 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 29
2017-05-04 08:53:39,543 INFO  [dispatcher-event-loop-14] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_97_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,544 INFO  [dispatcher-event-loop-21] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_98_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,545 INFO  [dispatcher-event-loop-33] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_86_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,546 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 4201
2017-05-04 08:53:39,547 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 26
2017-05-04 08:53:39,547 INFO  [dispatcher-event-loop-35] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_88_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,548 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_89_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,549 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 4346
2017-05-04 08:53:39,550 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 27
2017-05-04 08:53:39,550 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_91_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,551 INFO  [dispatcher-event-loop-11] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_79_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,552 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_80_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,552 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,552 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 3911
2017-05-04 08:53:39,553 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 78 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,553 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 24
2017-05-04 08:53:39,553 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 36 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 67 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 66)
2017-05-04 08:53:39,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 66)
2017-05-04 08:53:39,554 INFO  [dispatcher-event-loop-20] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_82_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,554 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 66 (MapPartitionsRDD[78] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,555 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_83_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,555 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 4056
2017-05-04 08:53:39,556 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_100 stored as values in memory (estimated size 5.8 KB, free 363.9 MB)
2017-05-04 08:53:39,556 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 25
2017-05-04 08:53:39,557 INFO  [dispatcher-event-loop-27] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_85_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,557 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_100_piece0 stored as bytes in memory (estimated size 3.2 KB, free 363.9 MB)
2017-05-04 08:53:39,558 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_100_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,558 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_73_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,558 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 100 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,558 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 66 (MapPartitionsRDD[78] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,559 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 66.0 with 2 tasks
2017-05-04 08:53:39,559 INFO  [dispatcher-event-loop-37] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_74_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,560 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 3621
2017-05-04 08:53:39,560 INFO  [dispatcher-event-loop-39] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 66.0 (TID 132, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,561 INFO  [dispatcher-event-loop-39] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 66.0 (TID 133, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,561 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 22
2017-05-04 08:53:39,561 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 66.0 (TID 133)
2017-05-04 08:53:39,561 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 66.0 (TID 132)
2017-05-04 08:53:39,561 INFO  [dispatcher-event-loop-13] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_76_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,563 INFO  [dispatcher-event-loop-8] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_77_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,563 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 3766
2017-05-04 08:53:39,563 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,563 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,564 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,564 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,564 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 23
2017-05-04 08:53:39,565 INFO  [dispatcher-event-loop-19] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_65_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,565 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 3186
2017-05-04 08:53:39,566 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 19
2017-05-04 08:53:39,566 INFO  [dispatcher-event-loop-29] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_67_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,567 INFO  [dispatcher-event-loop-30] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_68_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,567 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 3331
2017-05-04 08:53:39,568 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 20
2017-05-04 08:53:39,569 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_71_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,569 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 3476
2017-05-04 08:53:39,570 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 21
2017-05-04 08:53:39,570 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 2896
2017-05-04 08:53:39,570 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 17
2017-05-04 08:53:39,571 INFO  [dispatcher-event-loop-2] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_61_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,572 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_62_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,572 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 3041
2017-05-04 08:53:39,573 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 18
2017-05-04 08:53:39,573 INFO  [dispatcher-event-loop-24] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_64_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.3 MB)
2017-05-04 08:53:39,574 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 66.0 (TID 133). 1799 bytes result sent to driver
2017-05-04 08:53:39,575 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 66.0 (TID 133) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,575 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 66.0 (TID 132). 1799 bytes result sent to driver
2017-05-04 08:53:39,575 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 66.0 (TID 132) in 16 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,576 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 66.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,576 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 66 (mapPartitions at KMeans.scala:276) finished in 0.017 s
2017-05-04 08:53:39,576 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,576 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,576 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 67)
2017-05-04 08:53:39,576 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,576 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 67 (ShuffledRDD[79] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,577 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_101 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:39,578 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_101_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:39,579 INFO  [dispatcher-event-loop-20] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_101_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,579 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 101 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,579 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 67 (ShuffledRDD[79] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,580 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 67.0 with 2 tasks
2017-05-04 08:53:39,580 INFO  [dispatcher-event-loop-25] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 67.0 (TID 134, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,581 INFO  [dispatcher-event-loop-25] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 67.0 (TID 135, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,581 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 67.0 (TID 134)
2017-05-04 08:53:39,581 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 67.0 (TID 135)
2017-05-04 08:53:39,582 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,583 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,583 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,583 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,584 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 67.0 (TID 134). 2820 bytes result sent to driver
2017-05-04 08:53:39,585 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 67.0 (TID 135). 2721 bytes result sent to driver
2017-05-04 08:53:39,585 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 67.0 (TID 134) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,585 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 67.0 (TID 135) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,585 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 67.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,585 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 67 (collectAsMap at KMeans.scala:295) finished in 0.005 s
2017-05-04 08:53:39,586 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 36 finished: collectAsMap at KMeans.scala:295, took 0.033401 s
2017-05-04 08:53:39,587 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(99) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,587 INFO  [dispatcher-event-loop-21] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_99_piece0 on 192.168.0.101:42487 in memory (size: 1271.0 B, free: 364.3 MB)
2017-05-04 08:53:39,587 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_102 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:39,589 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_102_piece0 stored as bytes in memory (estimated size 1275.0 B, free 364.0 MB)
2017-05-04 08:53:39,589 INFO  [dispatcher-event-loop-30] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_102_piece0 in memory on 192.168.0.101:42487 (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,589 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 102 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,602 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,603 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 80 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,603 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 37 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,603 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 69 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,603 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 68)
2017-05-04 08:53:39,603 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 68)
2017-05-04 08:53:39,604 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 68 (MapPartitionsRDD[80] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,605 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_103 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:39,607 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_103_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:39,607 INFO  [dispatcher-event-loop-27] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_103_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,607 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 103 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,608 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 68 (MapPartitionsRDD[80] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,608 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 68.0 with 2 tasks
2017-05-04 08:53:39,609 INFO  [dispatcher-event-loop-31] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 68.0 (TID 136, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,610 INFO  [dispatcher-event-loop-31] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 68.0 (TID 137, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,610 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 68.0 (TID 137)
2017-05-04 08:53:39,610 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 68.0 (TID 136)
2017-05-04 08:53:39,612 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,612 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,613 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,613 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,623 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 68.0 (TID 137). 1799 bytes result sent to driver
2017-05-04 08:53:39,624 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 68.0 (TID 137) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,625 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 68.0 (TID 136). 1799 bytes result sent to driver
2017-05-04 08:53:39,625 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 68.0 (TID 136) in 17 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,625 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 68.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,625 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 68 (mapPartitions at KMeans.scala:276) finished in 0.017 s
2017-05-04 08:53:39,626 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,626 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,626 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 69)
2017-05-04 08:53:39,626 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,626 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 69 (ShuffledRDD[81] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,627 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_104 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:39,628 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_104_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:39,629 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_104_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,629 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 104 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,629 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 69 (ShuffledRDD[81] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,630 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 69.0 with 2 tasks
2017-05-04 08:53:39,630 INFO  [dispatcher-event-loop-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 69.0 (TID 138, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,631 INFO  [dispatcher-event-loop-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 69.0 (TID 139, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,631 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 69.0 (TID 138)
2017-05-04 08:53:39,631 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 69.0 (TID 139)
2017-05-04 08:53:39,633 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,633 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,633 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,633 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,634 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 69.0 (TID 139). 2721 bytes result sent to driver
2017-05-04 08:53:39,635 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 69.0 (TID 139) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,635 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 69.0 (TID 138). 2820 bytes result sent to driver
2017-05-04 08:53:39,635 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 69.0 (TID 138) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,636 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 69 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,636 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 69.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,636 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 37 finished: collectAsMap at KMeans.scala:295, took 0.033634 s
2017-05-04 08:53:39,637 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(102) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,637 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_105 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:39,637 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_102_piece0 on 192.168.0.101:42487 in memory (size: 1275.0 B, free: 364.2 MB)
2017-05-04 08:53:39,639 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_105_piece0 stored as bytes in memory (estimated size 1267.0 B, free 364.0 MB)
2017-05-04 08:53:39,639 INFO  [dispatcher-event-loop-2] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_105_piece0 in memory on 192.168.0.101:42487 (size: 1267.0 B, free: 364.2 MB)
2017-05-04 08:53:39,639 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 105 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,655 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,656 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 82 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,656 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 38 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,656 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 71 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,656 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 70)
2017-05-04 08:53:39,656 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 70)
2017-05-04 08:53:39,657 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 70 (MapPartitionsRDD[82] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,658 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_106 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:39,659 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_106_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:39,660 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_106_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,660 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 106 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,661 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[82] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,661 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 70.0 with 2 tasks
2017-05-04 08:53:39,662 INFO  [dispatcher-event-loop-8] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 70.0 (TID 140, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,663 INFO  [dispatcher-event-loop-8] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 70.0 (TID 141, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,663 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 70.0 (TID 141)
2017-05-04 08:53:39,663 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 70.0 (TID 140)
2017-05-04 08:53:39,665 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,665 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,666 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,666 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,676 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 70.0 (TID 140). 1799 bytes result sent to driver
2017-05-04 08:53:39,676 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 70.0 (TID 141). 1799 bytes result sent to driver
2017-05-04 08:53:39,677 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 70.0 (TID 140) in 15 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,677 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 70.0 (TID 141) in 14 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,677 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 70.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,677 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 70 (mapPartitions at KMeans.scala:276) finished in 0.016 s
2017-05-04 08:53:39,678 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,678 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,678 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 71)
2017-05-04 08:53:39,678 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,678 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 71 (ShuffledRDD[83] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,679 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_107 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:39,680 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_107_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:39,681 INFO  [dispatcher-event-loop-12] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_107_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,681 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 107 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,681 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 71 (ShuffledRDD[83] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,681 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 71.0 with 2 tasks
2017-05-04 08:53:39,682 INFO  [dispatcher-event-loop-24] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 71.0 (TID 142, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,683 INFO  [dispatcher-event-loop-24] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 71.0 (TID 143, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,683 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 71.0 (TID 143)
2017-05-04 08:53:39,683 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 71.0 (TID 142)
2017-05-04 08:53:39,685 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,685 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,685 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,685 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,686 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 71.0 (TID 143). 2721 bytes result sent to driver
2017-05-04 08:53:39,686 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 71.0 (TID 142). 2820 bytes result sent to driver
2017-05-04 08:53:39,687 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 71.0 (TID 143) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,687 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 71.0 (TID 142) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,688 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 71.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,688 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 71 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,688 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 38 finished: collectAsMap at KMeans.scala:295, took 0.032903 s
2017-05-04 08:53:39,689 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(105) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,689 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_108 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:39,689 INFO  [dispatcher-event-loop-14] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_105_piece0 on 192.168.0.101:42487 in memory (size: 1267.0 B, free: 364.2 MB)
2017-05-04 08:53:39,691 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_108_piece0 stored as bytes in memory (estimated size 1269.0 B, free 364.0 MB)
2017-05-04 08:53:39,691 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_108_piece0 in memory on 192.168.0.101:42487 (size: 1269.0 B, free: 364.2 MB)
2017-05-04 08:53:39,692 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 108 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,704 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,704 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 84 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,705 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 39 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,705 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 73 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,705 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 72)
2017-05-04 08:53:39,705 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 72)
2017-05-04 08:53:39,706 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 72 (MapPartitionsRDD[84] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,707 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_109 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:39,709 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_109_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:39,709 INFO  [dispatcher-event-loop-21] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_109_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,709 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 109 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,710 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[84] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,710 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 72.0 with 2 tasks
2017-05-04 08:53:39,711 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 72.0 (TID 144, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,712 INFO  [dispatcher-event-loop-30] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 72.0 (TID 145, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,712 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 72.0 (TID 145)
2017-05-04 08:53:39,712 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 72.0 (TID 144)
2017-05-04 08:53:39,714 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,714 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,714 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,714 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,724 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 72.0 (TID 145). 1799 bytes result sent to driver
2017-05-04 08:53:39,724 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 72.0 (TID 145) in 13 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,725 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 72.0 (TID 144). 1799 bytes result sent to driver
2017-05-04 08:53:39,725 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 72.0 (TID 144) in 15 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,725 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 72.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,725 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 72 (mapPartitions at KMeans.scala:276) finished in 0.015 s
2017-05-04 08:53:39,725 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,726 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,726 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 73)
2017-05-04 08:53:39,726 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,726 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 73 (ShuffledRDD[85] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,727 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_110 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:39,728 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_110_piece0 stored as bytes in memory (estimated size 1659.0 B, free 364.0 MB)
2017-05-04 08:53:39,729 INFO  [dispatcher-event-loop-36] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_110_piece0 in memory on 192.168.0.101:42487 (size: 1659.0 B, free: 364.2 MB)
2017-05-04 08:53:39,729 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 110 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,729 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 73 (ShuffledRDD[85] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,729 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 73.0 with 2 tasks
2017-05-04 08:53:39,730 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 73.0 (TID 146, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,731 INFO  [dispatcher-event-loop-32] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 73.0 (TID 147, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,731 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 73.0 (TID 147)
2017-05-04 08:53:39,731 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 73.0 (TID 146)
2017-05-04 08:53:39,732 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,732 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,733 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,733 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 1 ms
2017-05-04 08:53:39,734 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 73.0 (TID 146). 2820 bytes result sent to driver
2017-05-04 08:53:39,734 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 73.0 (TID 147). 2721 bytes result sent to driver
2017-05-04 08:53:39,735 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 73.0 (TID 146) in 5 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,735 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 73.0 (TID 147) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,735 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 73.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,735 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 73 (collectAsMap at KMeans.scala:295) finished in 0.005 s
2017-05-04 08:53:39,736 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 39 finished: collectAsMap at KMeans.scala:295, took 0.031566 s
2017-05-04 08:53:39,736 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(108) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,737 INFO  [dispatcher-event-loop-6] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_108_piece0 on 192.168.0.101:42487 in memory (size: 1269.0 B, free: 364.2 MB)
2017-05-04 08:53:39,737 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_111 stored as values in memory (estimated size 1784.0 B, free 364.0 MB)
2017-05-04 08:53:39,739 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_111_piece0 stored as bytes in memory (estimated size 1265.0 B, free 364.0 MB)
2017-05-04 08:53:39,739 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_111_piece0 in memory on 192.168.0.101:42487 (size: 1265.0 B, free: 364.2 MB)
2017-05-04 08:53:39,739 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 111 from broadcast at KMeans.scala:273
2017-05-04 08:53:39,755 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collectAsMap at KMeans.scala:295
2017-05-04 08:53:39,756 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Registering RDD 86 (mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,756 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 40 (collectAsMap at KMeans.scala:295) with 2 output partitions
2017-05-04 08:53:39,756 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 75 (collectAsMap at KMeans.scala:295)
2017-05-04 08:53:39,756 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List(ShuffleMapStage 74)
2017-05-04 08:53:39,756 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List(ShuffleMapStage 74)
2017-05-04 08:53:39,757 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ShuffleMapStage 74 (MapPartitionsRDD[86] at mapPartitions at KMeans.scala:276), which has no missing parents
2017-05-04 08:53:39,758 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_112 stored as values in memory (estimated size 5.8 KB, free 364.0 MB)
2017-05-04 08:53:39,760 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_112_piece0 stored as bytes in memory (estimated size 3.2 KB, free 364.0 MB)
2017-05-04 08:53:39,760 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_112_piece0 in memory on 192.168.0.101:42487 (size: 3.2 KB, free: 364.2 MB)
2017-05-04 08:53:39,761 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 112 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,761 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ShuffleMapStage 74 (MapPartitionsRDD[86] at mapPartitions at KMeans.scala:276)
2017-05-04 08:53:39,761 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 74.0 with 2 tasks
2017-05-04 08:53:39,763 INFO  [dispatcher-event-loop-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 74.0 (TID 148, localhost, executor driver, partition 0, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,764 INFO  [dispatcher-event-loop-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 74.0 (TID 149, localhost, executor driver, partition 1, PROCESS_LOCAL, 6365 bytes)
2017-05-04 08:53:39,764 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 74.0 (TID 148)
2017-05-04 08:53:39,764 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 74.0 (TID 149)
2017-05-04 08:53:39,766 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,766 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,766 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_1 locally
2017-05-04 08:53:39,766 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_3_0 locally
2017-05-04 08:53:39,777 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 74.0 (TID 149). 1799 bytes result sent to driver
2017-05-04 08:53:39,777 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 74.0 (TID 149) in 14 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,777 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 74.0 (TID 148). 1799 bytes result sent to driver
2017-05-04 08:53:39,778 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 74.0 (TID 148) in 17 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,778 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 74.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,778 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ShuffleMapStage 74 (mapPartitions at KMeans.scala:276) finished in 0.017 s
2017-05-04 08:53:39,778 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - looking for newly runnable stages
2017-05-04 08:53:39,778 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - running: Set()
2017-05-04 08:53:39,778 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - waiting: Set(ResultStage 75)
2017-05-04 08:53:39,778 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - failed: Set()
2017-05-04 08:53:39,779 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 75 (ShuffledRDD[87] at reduceByKey at KMeans.scala:292), which has no missing parents
2017-05-04 08:53:39,780 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_113 stored as values in memory (estimated size 2.8 KB, free 364.0 MB)
2017-05-04 08:53:39,781 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_113_piece0 stored as bytes in memory (estimated size 1658.0 B, free 364.0 MB)
2017-05-04 08:53:39,782 INFO  [dispatcher-event-loop-11] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_113_piece0 in memory on 192.168.0.101:42487 (size: 1658.0 B, free: 364.2 MB)
2017-05-04 08:53:39,782 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 113 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,783 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 75 (ShuffledRDD[87] at reduceByKey at KMeans.scala:292)
2017-05-04 08:53:39,783 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 75.0 with 2 tasks
2017-05-04 08:53:39,784 INFO  [dispatcher-event-loop-16] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 75.0 (TID 150, localhost, executor driver, partition 0, ANY, 5827 bytes)
2017-05-04 08:53:39,784 INFO  [dispatcher-event-loop-16] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 75.0 (TID 151, localhost, executor driver, partition 1, ANY, 5827 bytes)
2017-05-04 08:53:39,785 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 75.0 (TID 151)
2017-05-04 08:53:39,785 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 75.0 (TID 150)
2017-05-04 08:53:39,786 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,786 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Getting 2 non-empty blocks out of 2 blocks
2017-05-04 08:53:39,786 INFO  [Executor task launch worker-1] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,786 INFO  [Executor task launch worker-0] storage.ShuffleBlockFetcherIterator (Logging.scala:logInfo(54)) - Started 0 remote fetches in 0 ms
2017-05-04 08:53:39,787 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 75.0 (TID 151). 2721 bytes result sent to driver
2017-05-04 08:53:39,788 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 75.0 (TID 150). 2820 bytes result sent to driver
2017-05-04 08:53:39,788 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 75.0 (TID 151) in 4 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,788 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 75.0 (TID 150) in 5 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,789 INFO  [task-result-getter-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 75.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,789 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 75 (collectAsMap at KMeans.scala:295) finished in 0.006 s
2017-05-04 08:53:39,789 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 40 finished: collectAsMap at KMeans.scala:295, took 0.033868 s
2017-05-04 08:53:39,790 INFO  [main] broadcast.TorrentBroadcast (Logging.scala:logInfo(54)) - Destroying Broadcast(111) (from destroy at KMeans.scala:297)
2017-05-04 08:53:39,791 INFO  [dispatcher-event-loop-25] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_111_piece0 on 192.168.0.101:42487 in memory (size: 1265.0 B, free: 364.2 MB)
2017-05-04 08:53:39,792 INFO  [main] clustering.KMeans (Logging.scala:logInfo(54)) - Iterations took 2.050 seconds.
2017-05-04 08:53:39,793 INFO  [main] clustering.KMeans (Logging.scala:logInfo(54)) - KMeans converged in 34 iterations.
2017-05-04 08:53:39,793 INFO  [main] clustering.KMeans (Logging.scala:logInfo(54)) - The cost is 9996.86440432316.
2017-05-04 08:53:39,795 INFO  [main] rdd.MapPartitionsRDD (Logging.scala:logInfo(54)) - Removing RDD 3 from persistence list
2017-05-04 08:53:39,796 INFO  [block-manager-slave-async-thread-pool-3] storage.BlockManager (Logging.scala:logInfo(54)) - Removing RDD 3
2017-05-04 08:53:39,798 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_114 stored as values in memory (estimated size 1808.0 B, free 364.1 MB)
2017-05-04 08:53:39,800 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_114_piece0 stored as bytes in memory (estimated size 1358.0 B, free 364.1 MB)
2017-05-04 08:53:39,800 INFO  [dispatcher-event-loop-23] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_114_piece0 in memory on 192.168.0.101:42487 (size: 1358.0 B, free: 364.4 MB)
2017-05-04 08:53:39,801 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 114 from broadcast at KMeansModel.scala:86
2017-05-04 08:53:39,808 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: sum at KMeansModel.scala:87
2017-05-04 08:53:39,808 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 41 (sum at KMeansModel.scala:87) with 2 output partitions
2017-05-04 08:53:39,808 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 76 (sum at KMeansModel.scala:87)
2017-05-04 08:53:39,808 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:39,809 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:39,809 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 76 (MapPartitionsRDD[88] at map at KMeansModel.scala:87), which has no missing parents
2017-05-04 08:53:39,810 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_115 stored as values in memory (estimated size 4.0 KB, free 364.1 MB)
2017-05-04 08:53:39,812 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_115_piece0 stored as bytes in memory (estimated size 2.4 KB, free 364.1 MB)
2017-05-04 08:53:39,812 INFO  [dispatcher-event-loop-22] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_115_piece0 in memory on 192.168.0.101:42487 (size: 2.4 KB, free: 364.4 MB)
2017-05-04 08:53:39,812 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 115 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,813 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 76 (MapPartitionsRDD[88] at map at KMeansModel.scala:87)
2017-05-04 08:53:39,813 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 76.0 with 2 tasks
2017-05-04 08:53:39,814 INFO  [dispatcher-event-loop-26] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 76.0 (TID 152, localhost, executor driver, partition 0, PROCESS_LOCAL, 6056 bytes)
2017-05-04 08:53:39,814 INFO  [dispatcher-event-loop-26] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 76.0 (TID 153, localhost, executor driver, partition 1, PROCESS_LOCAL, 6056 bytes)
2017-05-04 08:53:39,815 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 76.0 (TID 152)
2017-05-04 08:53:39,815 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 76.0 (TID 153)
2017-05-04 08:53:39,816 INFO  [Executor task launch worker-1] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_1 locally
2017-05-04 08:53:39,816 INFO  [Executor task launch worker-0] storage.BlockManager (Logging.scala:logInfo(54)) - Found block rdd_2_0 locally
2017-05-04 08:53:39,831 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 76.0 (TID 153). 1125 bytes result sent to driver
2017-05-04 08:53:39,831 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 76.0 (TID 152). 1125 bytes result sent to driver
2017-05-04 08:53:39,831 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 76.0 (TID 153) in 17 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:39,831 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 76.0 (TID 152) in 18 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:39,831 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 76.0, whose tasks have all completed, from pool 
2017-05-04 08:53:39,831 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 76 (sum at KMeansModel.scala:87) finished in 0.018 s
2017-05-04 08:53:39,832 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 41 finished: sum at KMeansModel.scala:87, took 0.024053 s
Within Set Sum of Squared Errors = 9996.86440432316
2017-05-04 08:53:39,914 INFO  [main] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:39,942 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: saveAsTextFile at KMeansModel.scala:129
2017-05-04 08:53:39,943 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 42 (saveAsTextFile at KMeansModel.scala:129) with 1 output partitions
2017-05-04 08:53:39,943 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 77 (saveAsTextFile at KMeansModel.scala:129)
2017-05-04 08:53:39,943 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:39,943 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:39,943 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 77 (MapPartitionsRDD[90] at saveAsTextFile at KMeansModel.scala:129), which has no missing parents
2017-05-04 08:53:39,959 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_116 stored as values in memory (estimated size 71.1 KB, free 364.0 MB)
2017-05-04 08:53:39,961 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_116_piece0 stored as bytes in memory (estimated size 25.3 KB, free 364.0 MB)
2017-05-04 08:53:39,962 INFO  [dispatcher-event-loop-34] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_116_piece0 in memory on 192.168.0.101:42487 (size: 25.3 KB, free: 364.4 MB)
2017-05-04 08:53:39,962 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 116 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:39,962 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[90] at saveAsTextFile at KMeansModel.scala:129)
2017-05-04 08:53:39,963 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 77.0 with 1 tasks
2017-05-04 08:53:39,967 INFO  [dispatcher-event-loop-27] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 77.0 (TID 154, localhost, executor driver, partition 0, PROCESS_LOCAL, 6118 bytes)
2017-05-04 08:53:39,967 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 77.0 (TID 154)
2017-05-04 08:53:40,003 INFO  [Executor task launch worker-0] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:40,036 INFO  [Executor task launch worker-0] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085339_0077_m_000000_154' to file:/usr/local/TrojanD/KMeansModel/metadata/_temporary/0/task_20170504085339_0077_m_000000
2017-05-04 08:53:40,037 INFO  [Executor task launch worker-0] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085339_0077_m_000000_154: Committed
2017-05-04 08:53:40,038 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 77.0 (TID 154). 1180 bytes result sent to driver
2017-05-04 08:53:40,039 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 77.0 (TID 154) in 76 ms on localhost (executor driver) (1/1)
2017-05-04 08:53:40,039 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 77.0, whose tasks have all completed, from pool 
2017-05-04 08:53:40,039 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 77 (saveAsTextFile at KMeansModel.scala:129) finished in 0.076 s
2017-05-04 08:53:40,040 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 42 finished: saveAsTextFile at KMeansModel.scala:129, took 0.097563 s
2017-05-04 08:53:41,224 INFO  [dispatcher-event-loop-3] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_107_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.4 MB)
2017-05-04 08:53:41,225 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 5216
2017-05-04 08:53:41,225 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 33
2017-05-04 08:53:41,226 INFO  [dispatcher-event-loop-9] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_109_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.4 MB)
2017-05-04 08:53:41,227 INFO  [dispatcher-event-loop-13] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_110_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.4 MB)
2017-05-04 08:53:41,228 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 5361
2017-05-04 08:53:41,229 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 34
2017-05-04 08:53:41,229 INFO  [dispatcher-event-loop-17] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_112_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.4 MB)
2017-05-04 08:53:41,230 INFO  [dispatcher-event-loop-19] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_113_piece0 on 192.168.0.101:42487 in memory (size: 1658.0 B, free: 364.4 MB)
2017-05-04 08:53:41,231 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_114_piece0 on 192.168.0.101:42487 in memory (size: 1358.0 B, free: 364.4 MB)
2017-05-04 08:53:41,232 INFO  [dispatcher-event-loop-14] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_115_piece0 on 192.168.0.101:42487 in memory (size: 2.4 KB, free: 364.4 MB)
2017-05-04 08:53:41,233 INFO  [dispatcher-event-loop-29] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_116_piece0 on 192.168.0.101:42487 in memory (size: 25.3 KB, free: 364.4 MB)
2017-05-04 08:53:41,234 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 4781
2017-05-04 08:53:41,235 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 30
2017-05-04 08:53:41,236 INFO  [dispatcher-event-loop-36] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_100_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.4 MB)
2017-05-04 08:53:41,237 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_101_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.4 MB)
2017-05-04 08:53:41,237 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 4926
2017-05-04 08:53:41,238 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 31
2017-05-04 08:53:41,239 INFO  [dispatcher-event-loop-39] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_103_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.4 MB)
2017-05-04 08:53:41,240 INFO  [dispatcher-event-loop-7] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_104_piece0 on 192.168.0.101:42487 in memory (size: 1659.0 B, free: 364.4 MB)
2017-05-04 08:53:41,240 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned accumulator 5071
2017-05-04 08:53:41,241 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned shuffle 32
2017-05-04 08:53:41,242 INFO  [dispatcher-event-loop-10] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_106_piece0 on 192.168.0.101:42487 in memory (size: 3.2 KB, free: 364.4 MB)
2017-05-04 08:53:41,657 INFO  [main] internal.SharedState (Logging.scala:logInfo(54)) - Warehouse path is 'file:/usr/local/TrojanD/ML/spark-warehouse/'.
2017-05-04 08:53:41,665 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@3bb4c2b2{/SQL,null,AVAILABLE}
2017-05-04 08:53:41,666 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@1acc768{/SQL/json,null,AVAILABLE}
2017-05-04 08:53:41,666 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@65930e02{/SQL/execution,null,AVAILABLE}
2017-05-04 08:53:41,667 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@18be3205{/SQL/execution/json,null,AVAILABLE}
2017-05-04 08:53:41,669 INFO  [main] handler.ContextHandler (ContextHandler.java:doStart(744)) - Started o.s.j.s.ServletContextHandler@180f33b2{/static/sql,null,AVAILABLE}
2017-05-04 08:53:42,036 INFO  [block-manager-slave-async-thread-pool-0] storage.BlockManager (Logging.scala:logInfo(54)) - Removing RDD 3
2017-05-04 08:53:42,036 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(54)) - Cleaned RDD 3
2017-05-04 08:53:42,567 INFO  [main] parquet.ParquetFileFormat (Logging.scala:logInfo(54)) - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:42,633 INFO  [main] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:42,634 INFO  [main] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:42,634 INFO  [main] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:42,635 INFO  [main] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:42,996 INFO  [main] codegen.CodeGenerator (Logging.scala:logInfo(54)) - Code generated in 285.304211 ms
2017-05-04 08:53:43,046 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: parquet at KMeansModel.scala:133
2017-05-04 08:53:43,049 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 43 (parquet at KMeansModel.scala:133) with 40 output partitions
2017-05-04 08:53:43,049 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 78 (parquet at KMeansModel.scala:133)
2017-05-04 08:53:43,049 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:43,050 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:43,051 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 78 (MapPartitionsRDD[94] at parquet at KMeansModel.scala:133), which has no missing parents
2017-05-04 08:53:43,075 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_117 stored as values in memory (estimated size 87.3 KB, free 364.1 MB)
2017-05-04 08:53:43,078 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_117_piece0 stored as bytes in memory (estimated size 31.7 KB, free 364.1 MB)
2017-05-04 08:53:43,079 INFO  [dispatcher-event-loop-24] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_117_piece0 in memory on 192.168.0.101:42487 (size: 31.7 KB, free: 364.4 MB)
2017-05-04 08:53:43,080 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 117 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:43,081 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 40 missing tasks from ResultStage 78 (MapPartitionsRDD[94] at parquet at KMeansModel.scala:133)
2017-05-04 08:53:43,081 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 78.0 with 40 tasks
2017-05-04 08:53:43,083 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 78.0 (TID 155, localhost, executor driver, partition 0, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,084 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 78.0 (TID 156, localhost, executor driver, partition 1, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,084 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 2.0 in stage 78.0 (TID 157, localhost, executor driver, partition 2, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,085 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 3.0 in stage 78.0 (TID 158, localhost, executor driver, partition 3, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,085 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 4.0 in stage 78.0 (TID 159, localhost, executor driver, partition 4, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,086 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 5.0 in stage 78.0 (TID 160, localhost, executor driver, partition 5, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,086 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 6.0 in stage 78.0 (TID 161, localhost, executor driver, partition 6, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,087 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 7.0 in stage 78.0 (TID 162, localhost, executor driver, partition 7, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,087 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 8.0 in stage 78.0 (TID 163, localhost, executor driver, partition 8, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,088 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 9.0 in stage 78.0 (TID 164, localhost, executor driver, partition 9, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,088 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 10.0 in stage 78.0 (TID 165, localhost, executor driver, partition 10, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,089 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 11.0 in stage 78.0 (TID 166, localhost, executor driver, partition 11, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,089 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 12.0 in stage 78.0 (TID 167, localhost, executor driver, partition 12, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,090 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 13.0 in stage 78.0 (TID 168, localhost, executor driver, partition 13, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,091 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 14.0 in stage 78.0 (TID 169, localhost, executor driver, partition 14, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,091 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 15.0 in stage 78.0 (TID 170, localhost, executor driver, partition 15, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,092 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 16.0 in stage 78.0 (TID 171, localhost, executor driver, partition 16, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,092 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 17.0 in stage 78.0 (TID 172, localhost, executor driver, partition 17, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,093 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 18.0 in stage 78.0 (TID 173, localhost, executor driver, partition 18, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,093 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 19.0 in stage 78.0 (TID 174, localhost, executor driver, partition 19, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,094 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 20.0 in stage 78.0 (TID 175, localhost, executor driver, partition 20, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,094 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 21.0 in stage 78.0 (TID 176, localhost, executor driver, partition 21, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,095 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 22.0 in stage 78.0 (TID 177, localhost, executor driver, partition 22, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,095 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 23.0 in stage 78.0 (TID 178, localhost, executor driver, partition 23, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,096 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 24.0 in stage 78.0 (TID 179, localhost, executor driver, partition 24, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,096 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 25.0 in stage 78.0 (TID 180, localhost, executor driver, partition 25, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,097 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 26.0 in stage 78.0 (TID 181, localhost, executor driver, partition 26, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,097 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 27.0 in stage 78.0 (TID 182, localhost, executor driver, partition 27, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,097 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 28.0 in stage 78.0 (TID 183, localhost, executor driver, partition 28, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,098 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 29.0 in stage 78.0 (TID 184, localhost, executor driver, partition 29, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,098 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 30.0 in stage 78.0 (TID 185, localhost, executor driver, partition 30, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,099 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 31.0 in stage 78.0 (TID 186, localhost, executor driver, partition 31, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,099 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 32.0 in stage 78.0 (TID 187, localhost, executor driver, partition 32, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,100 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 33.0 in stage 78.0 (TID 188, localhost, executor driver, partition 33, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,100 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 34.0 in stage 78.0 (TID 189, localhost, executor driver, partition 34, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,101 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 35.0 in stage 78.0 (TID 190, localhost, executor driver, partition 35, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,101 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 36.0 in stage 78.0 (TID 191, localhost, executor driver, partition 36, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,102 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 37.0 in stage 78.0 (TID 192, localhost, executor driver, partition 37, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,102 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 38.0 in stage 78.0 (TID 193, localhost, executor driver, partition 38, PROCESS_LOCAL, 6061 bytes)
2017-05-04 08:53:43,102 INFO  [dispatcher-event-loop-19] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 39.0 in stage 78.0 (TID 194, localhost, executor driver, partition 39, PROCESS_LOCAL, 6271 bytes)
2017-05-04 08:53:43,103 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 78.0 (TID 156)
2017-05-04 08:53:43,103 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 78.0 (TID 155)
2017-05-04 08:53:43,103 INFO  [Executor task launch worker-2] executor.Executor (Logging.scala:logInfo(54)) - Running task 2.0 in stage 78.0 (TID 157)
2017-05-04 08:53:43,104 INFO  [Executor task launch worker-3] executor.Executor (Logging.scala:logInfo(54)) - Running task 3.0 in stage 78.0 (TID 158)
2017-05-04 08:53:43,105 INFO  [Executor task launch worker-4] executor.Executor (Logging.scala:logInfo(54)) - Running task 4.0 in stage 78.0 (TID 159)
2017-05-04 08:53:43,105 INFO  [Executor task launch worker-5] executor.Executor (Logging.scala:logInfo(54)) - Running task 5.0 in stage 78.0 (TID 160)
2017-05-04 08:53:43,105 INFO  [Executor task launch worker-6] executor.Executor (Logging.scala:logInfo(54)) - Running task 6.0 in stage 78.0 (TID 161)
2017-05-04 08:53:43,105 INFO  [Executor task launch worker-7] executor.Executor (Logging.scala:logInfo(54)) - Running task 7.0 in stage 78.0 (TID 162)
2017-05-04 08:53:43,106 INFO  [Executor task launch worker-8] executor.Executor (Logging.scala:logInfo(54)) - Running task 8.0 in stage 78.0 (TID 163)
2017-05-04 08:53:43,106 INFO  [Executor task launch worker-9] executor.Executor (Logging.scala:logInfo(54)) - Running task 9.0 in stage 78.0 (TID 164)
2017-05-04 08:53:43,120 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(54)) - Running task 10.0 in stage 78.0 (TID 165)
2017-05-04 08:53:43,121 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(54)) - Running task 11.0 in stage 78.0 (TID 166)
2017-05-04 08:53:43,121 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(54)) - Running task 12.0 in stage 78.0 (TID 167)
2017-05-04 08:53:43,133 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(54)) - Running task 13.0 in stage 78.0 (TID 168)
2017-05-04 08:53:43,133 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(54)) - Running task 14.0 in stage 78.0 (TID 169)
2017-05-04 08:53:43,133 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(54)) - Running task 15.0 in stage 78.0 (TID 170)
2017-05-04 08:53:43,134 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(54)) - Running task 16.0 in stage 78.0 (TID 171)
2017-05-04 08:53:43,150 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(54)) - Running task 18.0 in stage 78.0 (TID 173)
2017-05-04 08:53:43,150 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(54)) - Running task 17.0 in stage 78.0 (TID 172)
2017-05-04 08:53:43,154 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(54)) - Running task 19.0 in stage 78.0 (TID 174)
2017-05-04 08:53:43,158 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(54)) - Running task 20.0 in stage 78.0 (TID 175)
2017-05-04 08:53:43,158 INFO  [Executor task launch worker-22] executor.Executor (Logging.scala:logInfo(54)) - Running task 22.0 in stage 78.0 (TID 177)
2017-05-04 08:53:43,158 INFO  [Executor task launch worker-21] executor.Executor (Logging.scala:logInfo(54)) - Running task 21.0 in stage 78.0 (TID 176)
2017-05-04 08:53:43,158 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(54)) - Running task 23.0 in stage 78.0 (TID 178)
2017-05-04 08:53:43,159 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(54)) - Running task 24.0 in stage 78.0 (TID 179)
2017-05-04 08:53:43,159 INFO  [Executor task launch worker-25] executor.Executor (Logging.scala:logInfo(54)) - Running task 25.0 in stage 78.0 (TID 180)
2017-05-04 08:53:43,161 INFO  [Executor task launch worker-26] executor.Executor (Logging.scala:logInfo(54)) - Running task 26.0 in stage 78.0 (TID 181)
2017-05-04 08:53:43,161 INFO  [Executor task launch worker-27] executor.Executor (Logging.scala:logInfo(54)) - Running task 27.0 in stage 78.0 (TID 182)
2017-05-04 08:53:43,162 INFO  [Executor task launch worker-28] executor.Executor (Logging.scala:logInfo(54)) - Running task 28.0 in stage 78.0 (TID 183)
2017-05-04 08:53:43,167 INFO  [Executor task launch worker-29] executor.Executor (Logging.scala:logInfo(54)) - Running task 29.0 in stage 78.0 (TID 184)
2017-05-04 08:53:43,168 INFO  [Executor task launch worker-30] executor.Executor (Logging.scala:logInfo(54)) - Running task 30.0 in stage 78.0 (TID 185)
2017-05-04 08:53:43,168 INFO  [Executor task launch worker-31] executor.Executor (Logging.scala:logInfo(54)) - Running task 31.0 in stage 78.0 (TID 186)
2017-05-04 08:53:43,169 INFO  [Executor task launch worker-32] executor.Executor (Logging.scala:logInfo(54)) - Running task 32.0 in stage 78.0 (TID 187)
2017-05-04 08:53:43,169 INFO  [Executor task launch worker-33] executor.Executor (Logging.scala:logInfo(54)) - Running task 33.0 in stage 78.0 (TID 188)
2017-05-04 08:53:43,169 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Running task 34.0 in stage 78.0 (TID 189)
2017-05-04 08:53:43,170 INFO  [Executor task launch worker-35] executor.Executor (Logging.scala:logInfo(54)) - Running task 35.0 in stage 78.0 (TID 190)
2017-05-04 08:53:43,170 INFO  [Executor task launch worker-36] executor.Executor (Logging.scala:logInfo(54)) - Running task 36.0 in stage 78.0 (TID 191)
2017-05-04 08:53:43,171 INFO  [Executor task launch worker-37] executor.Executor (Logging.scala:logInfo(54)) - Running task 37.0 in stage 78.0 (TID 192)
2017-05-04 08:53:43,171 INFO  [Executor task launch worker-38] executor.Executor (Logging.scala:logInfo(54)) - Running task 38.0 in stage 78.0 (TID 193)
2017-05-04 08:53:43,174 INFO  [Executor task launch worker-39] executor.Executor (Logging.scala:logInfo(54)) - Running task 39.0 in stage 78.0 (TID 194)
2017-05-04 08:53:43,207 INFO  [Executor task launch worker-12] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,207 INFO  [Executor task launch worker-3] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,208 INFO  [Executor task launch worker-12] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,209 INFO  [Executor task launch worker-12] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,210 INFO  [Executor task launch worker-3] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,210 INFO  [Executor task launch worker-12] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,210 INFO  [Executor task launch worker-3] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,210 INFO  [Executor task launch worker-0] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,210 INFO  [Executor task launch worker-6] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,210 INFO  [Executor task launch worker-3] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,211 INFO  [Executor task launch worker-9] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,211 INFO  [Executor task launch worker-4] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,211 INFO  [Executor task launch worker-6] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,212 INFO  [Executor task launch worker-6] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,212 INFO  [Executor task launch worker-0] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,212 INFO  [Executor task launch worker-7] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,212 INFO  [Executor task launch worker-0] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,213 INFO  [Executor task launch worker-2] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,213 INFO  [Executor task launch worker-1] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,214 INFO  [Executor task launch worker-9] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,213 INFO  [Executor task launch worker-10] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,214 INFO  [Executor task launch worker-9] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,214 INFO  [Executor task launch worker-12] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,216 INFO  [Executor task launch worker-3] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,217 INFO  [Executor task launch worker-4] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,217 INFO  [Executor task launch worker-4] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,217 INFO  [Executor task launch worker-1] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,217 INFO  [Executor task launch worker-7] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,217 INFO  [Executor task launch worker-1] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,217 INFO  [Executor task launch worker-6] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,218 INFO  [Executor task launch worker-6] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,219 INFO  [Executor task launch worker-13] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,220 INFO  [Executor task launch worker-3] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,221 INFO  [Executor task launch worker-11] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,222 INFO  [Executor task launch worker-9] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,222 INFO  [Executor task launch worker-9] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,217 INFO  [Executor task launch worker-7] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,220 INFO  [Executor task launch worker-12] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,220 INFO  [Executor task launch worker-6] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,223 INFO  [Executor task launch worker-2] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,224 INFO  [Executor task launch worker-2] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,225 INFO  [Executor task launch worker-13] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,226 INFO  [Executor task launch worker-15] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,226 INFO  [Executor task launch worker-0] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,226 INFO  [Executor task launch worker-13] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,226 INFO  [Executor task launch worker-0] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,226 INFO  [Executor task launch worker-4] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,226 INFO  [Executor task launch worker-4] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,228 INFO  [Executor task launch worker-10] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,228 INFO  [Executor task launch worker-11] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,229 INFO  [Executor task launch worker-10] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,229 INFO  [Executor task launch worker-11] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,229 INFO  [Executor task launch worker-9] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,229 INFO  [Executor task launch worker-8] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,230 INFO  [Executor task launch worker-0] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,230 INFO  [Executor task launch worker-7] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,234 INFO  [Executor task launch worker-16] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-6] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-8] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-13] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-11] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-13] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-10] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-4] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-4] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-4] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,237 INFO  [Executor task launch worker-4] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,237 INFO  [Executor task launch worker-4] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,237 INFO  [Executor task launch worker-4] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-2] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-0] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,237 INFO  [Executor task launch worker-0] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-15] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,234 INFO  [Executor task launch worker-9] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,234 INFO  [Executor task launch worker-17] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,234 INFO  [Executor task launch worker-3] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,234 INFO  [Executor task launch worker-5] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,234 INFO  [Executor task launch worker-12] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,230 INFO  [Executor task launch worker-1] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-5] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-12] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-17] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,239 INFO  [Executor task launch worker-5] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-17] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-3] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-9] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-17] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-15] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-5] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-5] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-0] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,238 INFO  [Executor task launch worker-25] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,237 INFO  [Executor task launch worker-4] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,237 INFO  [Executor task launch worker-2] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-13] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-10] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,242 INFO  [Executor task launch worker-10] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,242 INFO  [Executor task launch worker-10] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,242 INFO  [Executor task launch worker-10] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,242 INFO  [Executor task launch worker-10] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,242 INFO  [Executor task launch worker-10] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,242 INFO  [Executor task launch worker-10] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,243 INFO  [Executor task launch worker-10] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,243 INFO  [Executor task launch worker-10] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,243 INFO  [Executor task launch worker-18] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,243 INFO  [Executor task launch worker-18] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,244 INFO  [Executor task launch worker-18] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,244 INFO  [Executor task launch worker-18] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,244 INFO  [Executor task launch worker-18] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,245 INFO  [Executor task launch worker-18] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,245 INFO  [Executor task launch worker-18] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,245 INFO  [Executor task launch worker-18] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,245 INFO  [Executor task launch worker-18] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,245 INFO  [Executor task launch worker-18] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,245 INFO  [Executor task launch worker-18] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-11] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,246 INFO  [Executor task launch worker-11] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-16] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-8] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,236 INFO  [Executor task launch worker-6] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,235 INFO  [Executor task launch worker-7] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,246 INFO  [Executor task launch worker-6] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,246 INFO  [Executor task launch worker-16] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,246 INFO  [Executor task launch worker-11] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,245 INFO  [Executor task launch worker-18] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,247 INFO  [Executor task launch worker-18] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,243 INFO  [Executor task launch worker-33] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,247 INFO  [Executor task launch worker-33] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,247 INFO  [Executor task launch worker-33] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,248 INFO  [Executor task launch worker-33] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,248 INFO  [Executor task launch worker-32] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,243 INFO  [Executor task launch worker-26] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,242 INFO  [Executor task launch worker-38] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-13] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-2] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-4] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,249 INFO  [Executor task launch worker-2] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,249 INFO  [Executor task launch worker-2] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-25] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-5] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-0] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,241 INFO  [Executor task launch worker-15] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-14] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-22] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-17] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-9] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,240 INFO  [Executor task launch worker-3] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,239 INFO  [Executor task launch worker-12] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,239 INFO  [Executor task launch worker-1] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,239 INFO  [Executor task launch worker-31] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-12] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-3] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-1] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-17] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-34] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-22] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-9] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-9] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-34] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,251 INFO  [Executor task launch worker-14] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,251 INFO  [Executor task launch worker-15] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,251 INFO  [Executor task launch worker-5] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-15] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,250 INFO  [Executor task launch worker-0] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,249 INFO  [Executor task launch worker-25] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,249 INFO  [Executor task launch worker-2] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,249 INFO  [Executor task launch worker-38] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,248 INFO  [Executor task launch worker-28] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,248 INFO  [Executor task launch worker-26] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,248 INFO  [Executor task launch worker-13] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,248 INFO  [Executor task launch worker-32] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,248 INFO  [Executor task launch worker-33] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-20] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,247 INFO  [Executor task launch worker-16] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,247 INFO  [Executor task launch worker-11] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,247 INFO  [Executor task launch worker-7] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,247 INFO  [Executor task launch worker-6] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,246 INFO  [Executor task launch worker-8] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-6] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-27] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-30] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-7] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-27] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-29] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-21] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-20] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-11] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-21] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-37] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-36] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-16] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-33] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-36] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-19] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-32] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-13] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-28] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-26] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-39] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-38] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-25] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-2] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,254 INFO  [Executor task launch worker-0] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-0] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-15] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-15] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-5] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-5] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-23] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-14] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-34] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-9] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-35] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-22] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-17] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-17] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,253 INFO  [Executor task launch worker-24] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-1] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-3] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-31] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,252 INFO  [Executor task launch worker-12] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-31] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-3] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-3] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-24] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-1] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-17] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-22] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-35] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-34] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-9] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-14] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-23] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-5] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,258 INFO  [Executor task launch worker-15] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-2] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-25] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-38] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-39] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-26] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-28] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-32] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-13] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-13] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-33] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-19] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-16] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,257 INFO  [Executor task launch worker-36] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-37] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-21] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-29] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-11] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-20] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-30] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-27] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-20] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-27] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,256 INFO  [Executor task launch worker-7] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-7] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-6] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-6] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,255 INFO  [Executor task launch worker-8] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-7] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-7] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-27] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-20] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-27] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,263 INFO  [Executor task launch worker-30] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-11] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-30] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-21] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-29] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-36] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-29] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-37] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-16] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-37] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-19] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-33] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-33] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,262 INFO  [Executor task launch worker-13] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-28] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-32] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-26] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-39] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-25] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-38] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-2] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-15] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,261 INFO  [Executor task launch worker-5] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-23] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-14] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-34] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-35] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-34] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-17] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-22] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-1] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-22] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,260 INFO  [Executor task launch worker-24] output.FileOutputCommitter (FileOutputCommitter.java:<init>(108)) - File Output Committer Algorithm version is 1
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-31] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,259 INFO  [Executor task launch worker-12] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-24] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-31] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-22] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-1] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-17] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-34] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-35] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-14] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,267 INFO  [Executor task launch worker-23] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-5] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-15] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-15] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-2] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-38] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-25] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-25] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-39] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-26] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-32] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,266 INFO  [Executor task launch worker-28] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-13] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-33] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-19] datasources.SQLHadoopMapReduceCommitProtocol (Logging.scala:logInfo(54)) - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-37] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-16] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-29] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,265 INFO  [Executor task launch worker-36] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-21] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-30] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-21] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-11] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-20] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-27] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-27] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-7] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,273 INFO  [Executor task launch worker-7] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,264 INFO  [Executor task launch worker-8] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-27] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-20] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-11] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-30] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-21] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-36] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,272 INFO  [Executor task launch worker-29] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,271 INFO  [Executor task launch worker-16] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,271 INFO  [Executor task launch worker-37] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,270 INFO  [Executor task launch worker-19] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,270 INFO  [Executor task launch worker-33] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,270 INFO  [Executor task launch worker-28] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,270 INFO  [Executor task launch worker-32] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-26] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-39] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-25] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,275 INFO  [Executor task launch worker-25] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-38] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,275 INFO  [Executor task launch worker-38] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-15] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-5] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,269 INFO  [Executor task launch worker-23] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-14] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,275 INFO  [Executor task launch worker-14] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-35] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-34] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-35] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-17] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-1] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-22] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-22] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-31] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-24] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,268 INFO  [Executor task launch worker-12] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-31] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-22] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-1] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-17] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-35] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-34] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,275 INFO  [Executor task launch worker-14] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,275 INFO  [Executor task launch worker-23] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,275 INFO  [Executor task launch worker-38] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,275 INFO  [Executor task launch worker-25] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-39] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-26] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-32] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-28] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-33] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-33] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-19] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-37] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-16] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,274 INFO  [Executor task launch worker-29] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,273 INFO  [Executor task launch worker-36] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,273 INFO  [Executor task launch worker-21] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,273 INFO  [Executor task launch worker-30] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,273 INFO  [Executor task launch worker-20] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,273 INFO  [Executor task launch worker-27] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,333 INFO  [Executor task launch worker-27] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,333 INFO  [Executor task launch worker-27] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,273 INFO  [Executor task launch worker-8] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,333 INFO  [Executor task launch worker-27] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,333 INFO  [Executor task launch worker-33] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,333 INFO  [Executor task launch worker-20] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,332 INFO  [Executor task launch worker-30] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,331 INFO  [Executor task launch worker-21] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,331 INFO  [Executor task launch worker-36] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,331 INFO  [Executor task launch worker-29] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,331 INFO  [Executor task launch worker-16] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,331 INFO  [Executor task launch worker-37] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,331 INFO  [Executor task launch worker-19] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,331 INFO  [Executor task launch worker-17] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,337 INFO  [Executor task launch worker-27] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-28] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-32] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,339 INFO  [Executor task launch worker-32] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-12] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-26] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,339 INFO  [Executor task launch worker-26] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-39] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,340 INFO  [Executor task launch worker-39] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-25] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-9] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-38] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-10] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-18] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-0] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-4] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-11] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-13] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-15] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,330 INFO  [Executor task launch worker-3] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,328 INFO  [Executor task launch worker-7] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,328 INFO  [Executor task launch worker-23] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,328 INFO  [Executor task launch worker-2] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,328 INFO  [Executor task launch worker-5] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,328 INFO  [Executor task launch worker-6] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,327 INFO  [Executor task launch worker-14] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,326 INFO  [Executor task launch worker-34] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,325 INFO  [Executor task launch worker-35] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,347 INFO  [Executor task launch worker-35] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,277 INFO  [Executor task launch worker-1] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,277 INFO  [Executor task launch worker-22] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,277 INFO  [Executor task launch worker-31] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,276 INFO  [Executor task launch worker-24] codec.CodecConfig (Log.java:info(151)) - Compression: SNAPPY
2017-05-04 08:53:43,347 INFO  [Executor task launch worker-31] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-24] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet block size to 134217728
2017-05-04 08:53:43,347 INFO  [Executor task launch worker-22] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-22] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,347 INFO  [Executor task launch worker-35] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-35] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,347 INFO  [Executor task launch worker-34] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,346 INFO  [Executor task launch worker-14] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-14] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,345 INFO  [Executor task launch worker-23] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,340 INFO  [Executor task launch worker-38] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,349 INFO  [Executor task launch worker-38] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,340 INFO  [Executor task launch worker-25] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,340 INFO  [Executor task launch worker-39] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,339 INFO  [Executor task launch worker-26] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,349 INFO  [Executor task launch worker-22] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,339 INFO  [Executor task launch worker-32] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,338 INFO  [Executor task launch worker-28] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,337 INFO  [Executor task launch worker-19] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,337 INFO  [Executor task launch worker-37] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,350 INFO  [Executor task launch worker-37] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,337 INFO  [Executor task launch worker-16] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,337 INFO  [Executor task launch worker-29] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,337 INFO  [Executor task launch worker-36] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,336 INFO  [Executor task launch worker-21] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,336 INFO  [Executor task launch worker-30] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,336 INFO  [Executor task launch worker-20] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-20] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,334 INFO  [Executor task launch worker-8] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-20] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-30] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-30] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-21] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-36] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,352 INFO  [Executor task launch worker-36] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-29] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,350 INFO  [Executor task launch worker-16] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,350 INFO  [Executor task launch worker-37] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,350 INFO  [Executor task launch worker-25] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,350 INFO  [Executor task launch worker-19] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,350 INFO  [Executor task launch worker-28] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,353 INFO  [Executor task launch worker-28] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,349 INFO  [Executor task launch worker-32] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,353 INFO  [Executor task launch worker-32] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,349 INFO  [Executor task launch worker-26] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,349 INFO  [Executor task launch worker-39] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,349 INFO  [Executor task launch worker-1] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,349 INFO  [Executor task launch worker-38] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,354 INFO  [Executor task launch worker-32] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-23] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,356 INFO  [Executor task launch worker-23] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,357 INFO  [Executor task launch worker-23] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-14] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-34] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-35] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-24] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet page size to 1048576
2017-05-04 08:53:43,348 INFO  [Executor task launch worker-31] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,357 INFO  [Executor task launch worker-24] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,357 INFO  [Executor task launch worker-35] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,357 INFO  [Executor task launch worker-34] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,357 INFO  [Executor task launch worker-23] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,354 INFO  [Executor task launch worker-39] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,358 INFO  [Executor task launch worker-39] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,358 INFO  [Executor task launch worker-39] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,359 INFO  [Executor task launch worker-34] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,359 INFO  [Executor task launch worker-35] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,359 INFO  [Executor task launch worker-14] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,354 INFO  [Executor task launch worker-16] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,354 INFO  [Executor task launch worker-26] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,353 INFO  [Executor task launch worker-28] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,353 INFO  [Executor task launch worker-19] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,352 INFO  [Executor task launch worker-37] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,352 INFO  [Executor task launch worker-29] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,352 INFO  [Executor task launch worker-36] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,352 INFO  [Executor task launch worker-21] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,352 INFO  [Executor task launch worker-30] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-20] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,351 INFO  [Executor task launch worker-8] hadoop.ParquetOutputFormat (Log.java:info(151)) - Parquet dictionary page size to 1048576
2017-05-04 08:53:43,367 INFO  [Executor task launch worker-26] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,367 INFO  [Executor task launch worker-30] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,367 INFO  [Executor task launch worker-21] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,366 INFO  [Executor task launch worker-36] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,366 INFO  [Executor task launch worker-29] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,366 INFO  [Executor task launch worker-37] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,366 INFO  [Executor task launch worker-19] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,366 INFO  [Executor task launch worker-28] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,365 INFO  [Executor task launch worker-39] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,360 INFO  [Executor task launch worker-38] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,358 INFO  [Executor task launch worker-23] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,358 INFO  [Executor task launch worker-24] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,358 INFO  [Executor task launch worker-31] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,389 INFO  [Executor task launch worker-24] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,388 INFO  [Executor task launch worker-23] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,371 INFO  [Executor task launch worker-28] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,370 INFO  [Executor task launch worker-20] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,370 INFO  [Executor task launch worker-30] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,370 INFO  [Executor task launch worker-36] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,370 INFO  [Executor task launch worker-21] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,370 INFO  [Executor task launch worker-37] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,368 INFO  [Executor task launch worker-19] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,368 INFO  [Executor task launch worker-29] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,367 INFO  [Executor task launch worker-8] hadoop.ParquetOutputFormat (Log.java:info(151)) - Dictionary is on
2017-05-04 08:53:43,402 INFO  [Executor task launch worker-19] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,390 INFO  [Executor task launch worker-24] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,389 INFO  [Executor task launch worker-31] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,403 INFO  [Executor task launch worker-24] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,403 INFO  [Executor task launch worker-8] hadoop.ParquetOutputFormat (Log.java:info(151)) - Validation is off
2017-05-04 08:53:43,403 INFO  [Executor task launch worker-31] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,404 INFO  [Executor task launch worker-29] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,404 INFO  [Executor task launch worker-8] hadoop.ParquetOutputFormat (Log.java:info(151)) - Writer version is: PARQUET_1_0
2017-05-04 08:53:43,404 INFO  [Executor task launch worker-19] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,404 INFO  [Executor task launch worker-8] hadoop.ParquetOutputFormat (Log.java:info(151)) - Maximum row group padding size is 0 bytes
2017-05-04 08:53:43,406 INFO  [Executor task launch worker-31] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,414 INFO  [Executor task launch worker-24] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,417 INFO  [Executor task launch worker-8] parquet.ParquetWriteSupport (Logging.scala:logInfo(54)) - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "point",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.mllib.linalg.VectorUDT",
      "pyClass" : "pyspark.mllib.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
2017-05-04 08:53:43,437 INFO  [Executor task launch worker-2] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-26] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-25] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,437 INFO  [Executor task launch worker-12] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-18] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-14] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,439 INFO  [Executor task launch worker-23] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,437 INFO  [Executor task launch worker-28] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-33] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-9] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,437 INFO  [Executor task launch worker-21] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-10] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,437 INFO  [Executor task launch worker-1] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-27] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-17] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,437 INFO  [Executor task launch worker-13] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-19] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-36] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,439 INFO  [Executor task launch worker-37] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-34] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-32] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-38] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-6] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-0] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-16] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-22] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-5] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-4] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-39] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-7] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-11] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-15] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-3] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,438 INFO  [Executor task launch worker-35] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-30] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-31] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-24] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-8] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-20] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,440 INFO  [Executor task launch worker-29] compress.CodecPool (CodecPool.java:getCompressor(153)) - Got brand-new compressor [.snappy]
2017-05-04 08:53:43,582 WARN  [Executor task launch worker-19] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 96.54% for 7 writers
2017-05-04 08:53:43,582 INFO  [Executor task launch worker-22] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,583 INFO  [Executor task launch worker-28] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,583 INFO  [Executor task launch worker-33] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,582 INFO  [Executor task launch worker-4] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,584 INFO  [Executor task launch worker-19] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,584 WARN  [Executor task launch worker-18] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 84.47% for 8 writers
2017-05-04 08:53:43,586 WARN  [Executor task launch worker-26] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 75.08% for 9 writers
2017-05-04 08:53:43,586 WARN  [Executor task launch worker-6] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 67.58% for 10 writers
2017-05-04 08:53:43,587 INFO  [Executor task launch worker-6] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,587 WARN  [Executor task launch worker-27] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 61.43% for 11 writers
2017-05-04 08:53:43,588 INFO  [Executor task launch worker-27] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,588 WARN  [Executor task launch worker-23] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 56.31% for 12 writers
2017-05-04 08:53:43,589 WARN  [Executor task launch worker-35] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 51.98% for 13 writers
2017-05-04 08:53:43,590 INFO  [Executor task launch worker-35] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,590 WARN  [Executor task launch worker-9] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 48.27% for 14 writers
2017-05-04 08:53:43,591 WARN  [Executor task launch worker-38] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 45.05% for 15 writers
2017-05-04 08:53:43,591 INFO  [Executor task launch worker-9] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,591 INFO  [Executor task launch worker-38] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,591 INFO  [Executor task launch worker-23] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,591 INFO  [Executor task launch worker-26] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,592 INFO  [Executor task launch worker-21] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,592 WARN  [Executor task launch worker-20] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 42.24% for 16 writers
2017-05-04 08:53:43,592 INFO  [Executor task launch worker-18] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,592 INFO  [Executor task launch worker-2] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,593 INFO  [Executor task launch worker-20] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,593 WARN  [Executor task launch worker-24] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 39.75% for 17 writers
2017-05-04 08:53:43,594 WARN  [Executor task launch worker-32] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 37.54% for 18 writers
2017-05-04 08:53:43,594 INFO  [Executor task launch worker-24] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,595 INFO  [Executor task launch worker-32] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,595 WARN  [Executor task launch worker-7] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 35.57% for 19 writers
2017-05-04 08:53:43,596 WARN  [Executor task launch worker-39] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 33.79% for 20 writers
2017-05-04 08:53:43,596 INFO  [Executor task launch worker-7] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,597 WARN  [Executor task launch worker-5] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 32.18% for 21 writers
2017-05-04 08:53:43,597 INFO  [Executor task launch worker-39] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,598 WARN  [Executor task launch worker-8] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 30.72% for 22 writers
2017-05-04 08:53:43,598 INFO  [Executor task launch worker-5] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,599 INFO  [Executor task launch worker-8] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,599 WARN  [Executor task launch worker-16] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 29.38% for 23 writers
2017-05-04 08:53:43,600 WARN  [Executor task launch worker-30] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 28.16% for 24 writers
2017-05-04 08:53:43,600 INFO  [Executor task launch worker-16] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,600 INFO  [Executor task launch worker-30] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,600 WARN  [Executor task launch worker-1] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 27.03% for 25 writers
2017-05-04 08:53:43,601 WARN  [Executor task launch worker-29] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 25.99% for 26 writers
2017-05-04 08:53:43,601 INFO  [Executor task launch worker-1] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,602 WARN  [Executor task launch worker-12] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 25.03% for 27 writers
2017-05-04 08:53:43,602 INFO  [Executor task launch worker-29] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,603 WARN  [Executor task launch worker-37] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 24.13% for 28 writers
2017-05-04 08:53:43,603 INFO  [Executor task launch worker-12] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,604 WARN  [Executor task launch worker-25] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 23.30% for 29 writers
2017-05-04 08:53:43,604 INFO  [Executor task launch worker-37] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,605 WARN  [Executor task launch worker-3] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 22.53% for 30 writers
2017-05-04 08:53:43,605 INFO  [Executor task launch worker-25] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,605 INFO  [Executor task launch worker-3] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,605 WARN  [Executor task launch worker-10] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 21.80% for 31 writers
2017-05-04 08:53:43,606 WARN  [Executor task launch worker-11] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 21.12% for 32 writers
2017-05-04 08:53:43,607 INFO  [Executor task launch worker-10] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,607 INFO  [Executor task launch worker-11] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,607 WARN  [Executor task launch worker-0] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 20.48% for 33 writers
2017-05-04 08:53:43,609 INFO  [Executor task launch worker-0] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,609 WARN  [Executor task launch worker-15] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 19.88% for 34 writers
2017-05-04 08:53:43,610 WARN  [Executor task launch worker-36] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 19.31% for 35 writers
2017-05-04 08:53:43,610 INFO  [Executor task launch worker-15] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,610 INFO  [Executor task launch worker-36] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,611 WARN  [Executor task launch worker-31] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 18.77% for 36 writers
2017-05-04 08:53:43,612 WARN  [Executor task launch worker-14] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 18.26% for 37 writers
2017-05-04 08:53:43,612 INFO  [Executor task launch worker-31] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,613 INFO  [Executor task launch worker-14] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,613 WARN  [Executor task launch worker-17] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 17.78% for 38 writers
2017-05-04 08:53:43,614 INFO  [Executor task launch worker-17] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 0
2017-05-04 08:53:43,614 WARN  [Executor task launch worker-34] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 17.33% for 39 writers
2017-05-04 08:53:43,614 WARN  [Executor task launch worker-13] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 16.89% for 40 writers
2017-05-04 08:53:43,615 INFO  [Executor task launch worker-34] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,616 INFO  [Executor task launch worker-13] hadoop.InternalParquetRecordWriter (Log.java:info(151)) - Flushing mem columnStore to file. allocated memory: 100
2017-05-04 08:53:43,639 WARN  [Executor task launch worker-35] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 17.33% for 39 writers
2017-05-04 08:53:43,640 WARN  [Executor task launch worker-19] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 17.78% for 38 writers
2017-05-04 08:53:43,642 WARN  [Executor task launch worker-11] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 18.26% for 37 writers
2017-05-04 08:53:43,642 INFO  [Executor task launch worker-35] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000035_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000035
2017-05-04 08:53:43,642 INFO  [Executor task launch worker-35] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000035_0: Committed
2017-05-04 08:53:43,643 WARN  [Executor task launch worker-1] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 18.77% for 36 writers
2017-05-04 08:53:43,644 INFO  [Executor task launch worker-11] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000011_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000011
2017-05-04 08:53:43,644 INFO  [Executor task launch worker-19] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000019_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000019
2017-05-04 08:53:43,644 INFO  [Executor task launch worker-11] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000011_0: Committed
2017-05-04 08:53:43,644 INFO  [Executor task launch worker-19] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000019_0: Committed
2017-05-04 08:53:43,644 WARN  [Executor task launch worker-4] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 19.31% for 35 writers
2017-05-04 08:53:43,645 INFO  [Executor task launch worker-1] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000001_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000001
2017-05-04 08:53:43,645 INFO  [Executor task launch worker-1] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000001_0: Committed
2017-05-04 08:53:43,646 WARN  [Executor task launch worker-24] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 19.88% for 34 writers
2017-05-04 08:53:43,647 WARN  [Executor task launch worker-3] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 20.48% for 33 writers
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-4] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000004_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000004
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-35] executor.Executor (Logging.scala:logInfo(54)) - Finished task 35.0 in stage 78.0 (TID 190). 1626 bytes result sent to driver
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(54)) - Finished task 19.0 in stage 78.0 (TID 174). 1626 bytes result sent to driver
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 78.0 (TID 156). 1626 bytes result sent to driver
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(54)) - Finished task 11.0 in stage 78.0 (TID 166). 1626 bytes result sent to driver
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-24] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000024_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000024
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-4] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000004_0: Committed
2017-05-04 08:53:43,647 INFO  [Executor task launch worker-24] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000024_0: Committed
2017-05-04 08:53:43,648 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(54)) - Finished task 24.0 in stage 78.0 (TID 179). 1539 bytes result sent to driver
2017-05-04 08:53:43,648 INFO  [Executor task launch worker-4] executor.Executor (Logging.scala:logInfo(54)) - Finished task 4.0 in stage 78.0 (TID 159). 1539 bytes result sent to driver
2017-05-04 08:53:43,648 WARN  [Executor task launch worker-9] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 21.12% for 32 writers
2017-05-04 08:53:43,649 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 78.0 (TID 156) in 566 ms on localhost (executor driver) (1/40)
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-13] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-34] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-13] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-10] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-13] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-39] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-13] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-39] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,651 WARN  [Executor task launch worker-33] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 21.80% for 31 writers
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-3] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000003_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000003
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-10] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-34] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-9] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000009_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000009
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-29] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-15] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-21] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-5] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-26] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 11.0 in stage 78.0 (TID 166) in 562 ms on localhost (executor driver) (2/40)
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-37] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,653 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 35.0 in stage 78.0 (TID 190) in 553 ms on localhost (executor driver) (3/40)
2017-05-04 08:53:43,653 INFO  [Executor task launch worker-26] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,653 INFO  [Executor task launch worker-5] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,653 INFO  [Executor task launch worker-21] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,650 INFO  [Executor task launch worker-2] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-18] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,654 INFO  [Executor task launch worker-2] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-23] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-7] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-31] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [id] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-15] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-29] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-33] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000033_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000033
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-9] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000009_0: Committed
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-34] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,652 WARN  [Executor task launch worker-0] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 22.53% for 30 writers
2017-05-04 08:53:43,652 INFO  [Executor task launch worker-10] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-3] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000003_0: Committed
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-39] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,651 INFO  [Executor task launch worker-13] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,655 INFO  [Executor task launch worker-39] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,656 INFO  [Executor task launch worker-3] executor.Executor (Logging.scala:logInfo(54)) - Finished task 3.0 in stage 78.0 (TID 158). 1539 bytes result sent to driver
2017-05-04 08:53:43,655 INFO  [Executor task launch worker-10] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,655 INFO  [Executor task launch worker-9] executor.Executor (Logging.scala:logInfo(54)) - Finished task 9.0 in stage 78.0 (TID 164). 1539 bytes result sent to driver
2017-05-04 08:53:43,655 INFO  [Executor task launch worker-34] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,655 INFO  [Executor task launch worker-33] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000033_0: Committed
2017-05-04 08:53:43,655 INFO  [Executor task launch worker-29] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,654 INFO  [Executor task launch worker-15] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,654 INFO  [Executor task launch worker-31] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,654 INFO  [Executor task launch worker-7] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,654 INFO  [Executor task launch worker-23] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-0] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000000_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000000
2017-05-04 08:53:43,654 INFO  [Executor task launch worker-2] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,654 INFO  [Executor task launch worker-18] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,653 INFO  [Executor task launch worker-21] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,653 INFO  [Executor task launch worker-5] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,653 INFO  [Executor task launch worker-26] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,653 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 19.0 in stage 78.0 (TID 174) in 560 ms on localhost (executor driver) (4/40)
2017-05-04 08:53:43,653 INFO  [Executor task launch worker-37] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 45B for [point, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [Executor task launch worker-26] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 4.0 in stage 78.0 (TID 159) in 573 ms on localhost (executor driver) (5/40)
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-5] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-21] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-18] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [Executor task launch worker-21] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-2] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-0] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000000_0: Committed
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-23] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-7] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,657 INFO  [Executor task launch worker-31] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,656 INFO  [Executor task launch worker-33] executor.Executor (Logging.scala:logInfo(54)) - Finished task 33.0 in stage 78.0 (TID 188). 1539 bytes result sent to driver
2017-05-04 08:53:43,656 INFO  [Executor task launch worker-15] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,656 INFO  [Executor task launch worker-29] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,656 INFO  [Executor task launch worker-34] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,656 INFO  [Executor task launch worker-10] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,656 WARN  [Executor task launch worker-12] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 23.30% for 29 writers
2017-05-04 08:53:43,656 INFO  [Executor task launch worker-39] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,660 INFO  [Executor task launch worker-29] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,659 INFO  [Executor task launch worker-15] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,659 INFO  [Executor task launch worker-31] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,659 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 78.0 (TID 155). 1539 bytes result sent to driver
2017-05-04 08:53:43,661 INFO  [Executor task launch worker-31] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,659 INFO  [Executor task launch worker-7] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,659 INFO  [Executor task launch worker-23] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,659 INFO  [Executor task launch worker-2] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [Executor task launch worker-18] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 24.0 in stage 78.0 (TID 179) in 563 ms on localhost (executor driver) (6/40)
2017-05-04 08:53:43,661 INFO  [Executor task launch worker-18] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [Executor task launch worker-5] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [Executor task launch worker-26] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,658 INFO  [Executor task launch worker-37] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 30B for [point, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
2017-05-04 08:53:43,662 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 3.0 in stage 78.0 (TID 158) in 577 ms on localhost (executor driver) (7/40)
2017-05-04 08:53:43,661 INFO  [Executor task launch worker-12] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000012_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000012
2017-05-04 08:53:43,661 INFO  [Executor task launch worker-23] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,661 INFO  [Executor task launch worker-7] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,661 WARN  [Executor task launch worker-6] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 24.13% for 28 writers
2017-05-04 08:53:43,663 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 9.0 in stage 78.0 (TID 164) in 575 ms on localhost (executor driver) (8/40)
2017-05-04 08:53:43,662 INFO  [Executor task launch worker-12] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000012_0: Committed
2017-05-04 08:53:43,662 INFO  [Executor task launch worker-37] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 36B for [point, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,663 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 33.0 in stage 78.0 (TID 188) in 564 ms on localhost (executor driver) (9/40)
2017-05-04 08:53:43,663 INFO  [Executor task launch worker-37] hadoop.ColumnChunkPageWriteStore (Log.java:info(151)) - written 113B for [point, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [RLE, PLAIN]
2017-05-04 08:53:43,663 WARN  [Executor task launch worker-16] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 25.03% for 27 writers
2017-05-04 08:53:43,664 INFO  [Executor task launch worker-6] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000006_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000006
2017-05-04 08:53:43,664 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(54)) - Finished task 12.0 in stage 78.0 (TID 167). 1539 bytes result sent to driver
2017-05-04 08:53:43,664 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 78.0 (TID 155) in 581 ms on localhost (executor driver) (10/40)
2017-05-04 08:53:43,664 INFO  [Executor task launch worker-6] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000006_0: Committed
2017-05-04 08:53:43,664 WARN  [Executor task launch worker-14] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 25.99% for 26 writers
2017-05-04 08:53:43,664 INFO  [Executor task launch worker-6] executor.Executor (Logging.scala:logInfo(54)) - Finished task 6.0 in stage 78.0 (TID 161). 1539 bytes result sent to driver
2017-05-04 08:53:43,665 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 12.0 in stage 78.0 (TID 167) in 576 ms on localhost (executor driver) (11/40)
2017-05-04 08:53:43,665 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 6.0 in stage 78.0 (TID 161) in 579 ms on localhost (executor driver) (12/40)
2017-05-04 08:53:43,665 WARN  [Executor task launch worker-8] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 27.03% for 25 writers
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-16] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000016_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000016
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-16] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000016_0: Committed
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-14] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000014_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000014
2017-05-04 08:53:43,666 WARN  [Executor task launch worker-25] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 28.16% for 24 writers
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-14] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000014_0: Committed
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(54)) - Finished task 16.0 in stage 78.0 (TID 171). 1539 bytes result sent to driver
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-8] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000008_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000008
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(54)) - Finished task 14.0 in stage 78.0 (TID 169). 1539 bytes result sent to driver
2017-05-04 08:53:43,666 INFO  [Executor task launch worker-8] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000008_0: Committed
2017-05-04 08:53:43,667 WARN  [Executor task launch worker-27] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 29.38% for 23 writers
2017-05-04 08:53:43,667 INFO  [Executor task launch worker-8] executor.Executor (Logging.scala:logInfo(54)) - Finished task 8.0 in stage 78.0 (TID 163). 1539 bytes result sent to driver
2017-05-04 08:53:43,667 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 16.0 in stage 78.0 (TID 171) in 575 ms on localhost (executor driver) (13/40)
2017-05-04 08:53:43,667 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 14.0 in stage 78.0 (TID 169) in 577 ms on localhost (executor driver) (14/40)
2017-05-04 08:53:43,667 INFO  [Executor task launch worker-25] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000025_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000025
2017-05-04 08:53:43,668 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 8.0 in stage 78.0 (TID 163) in 580 ms on localhost (executor driver) (15/40)
2017-05-04 08:53:43,668 INFO  [Executor task launch worker-25] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000025_0: Committed
2017-05-04 08:53:43,668 WARN  [Executor task launch worker-36] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 30.72% for 22 writers
2017-05-04 08:53:43,668 INFO  [Executor task launch worker-25] executor.Executor (Logging.scala:logInfo(54)) - Finished task 25.0 in stage 78.0 (TID 180). 1539 bytes result sent to driver
2017-05-04 08:53:43,669 WARN  [Executor task launch worker-23] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 32.18% for 21 writers
2017-05-04 08:53:43,669 INFO  [Executor task launch worker-27] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000027_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000027
2017-05-04 08:53:43,669 INFO  [Executor task launch worker-27] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000027_0: Committed
2017-05-04 08:53:43,669 WARN  [Executor task launch worker-17] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 33.79% for 20 writers
2017-05-04 08:53:43,669 INFO  [Executor task launch worker-27] executor.Executor (Logging.scala:logInfo(54)) - Finished task 27.0 in stage 78.0 (TID 182). 1539 bytes result sent to driver
2017-05-04 08:53:43,669 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 25.0 in stage 78.0 (TID 180) in 573 ms on localhost (executor driver) (16/40)
2017-05-04 08:53:43,669 INFO  [Executor task launch worker-36] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000036_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000036
2017-05-04 08:53:43,670 INFO  [Executor task launch worker-36] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000036_0: Committed
2017-05-04 08:53:43,670 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 27.0 in stage 78.0 (TID 182) in 573 ms on localhost (executor driver) (17/40)
2017-05-04 08:53:43,670 INFO  [Executor task launch worker-23] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000023_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000023
2017-05-04 08:53:43,670 INFO  [Executor task launch worker-23] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000023_0: Committed
2017-05-04 08:53:43,670 WARN  [Executor task launch worker-28] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 35.57% for 19 writers
2017-05-04 08:53:43,670 INFO  [Executor task launch worker-36] executor.Executor (Logging.scala:logInfo(54)) - Finished task 36.0 in stage 78.0 (TID 191). 1539 bytes result sent to driver
2017-05-04 08:53:43,670 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(54)) - Finished task 23.0 in stage 78.0 (TID 178). 1539 bytes result sent to driver
2017-05-04 08:53:43,671 WARN  [Executor task launch worker-22] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 37.54% for 18 writers
2017-05-04 08:53:43,671 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 36.0 in stage 78.0 (TID 191) in 570 ms on localhost (executor driver) (18/40)
2017-05-04 08:53:43,671 INFO  [Executor task launch worker-17] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000017_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000017
2017-05-04 08:53:43,671 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 23.0 in stage 78.0 (TID 178) in 576 ms on localhost (executor driver) (19/40)
2017-05-04 08:53:43,671 INFO  [Executor task launch worker-17] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000017_0: Committed
2017-05-04 08:53:43,671 WARN  [Executor task launch worker-32] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 39.75% for 17 writers
2017-05-04 08:53:43,671 INFO  [Executor task launch worker-28] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000028_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000028
2017-05-04 08:53:43,672 INFO  [Executor task launch worker-28] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000028_0: Committed
2017-05-04 08:53:43,672 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(54)) - Finished task 17.0 in stage 78.0 (TID 172). 1539 bytes result sent to driver
2017-05-04 08:53:43,672 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 17.0 in stage 78.0 (TID 172) in 580 ms on localhost (executor driver) (20/40)
2017-05-04 08:53:43,672 WARN  [Executor task launch worker-38] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 42.24% for 16 writers
2017-05-04 08:53:43,672 INFO  [Executor task launch worker-28] executor.Executor (Logging.scala:logInfo(54)) - Finished task 28.0 in stage 78.0 (TID 183). 1539 bytes result sent to driver
2017-05-04 08:53:43,672 INFO  [Executor task launch worker-22] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000022_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000022
2017-05-04 08:53:43,673 INFO  [Executor task launch worker-22] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000022_0: Committed
2017-05-04 08:53:43,673 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 28.0 in stage 78.0 (TID 183) in 576 ms on localhost (executor driver) (21/40)
2017-05-04 08:53:43,673 WARN  [Executor task launch worker-30] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 45.05% for 15 writers
2017-05-04 08:53:43,673 INFO  [Executor task launch worker-32] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000032_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000032
2017-05-04 08:53:43,673 INFO  [Executor task launch worker-32] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000032_0: Committed
2017-05-04 08:53:43,673 INFO  [Executor task launch worker-22] executor.Executor (Logging.scala:logInfo(54)) - Finished task 22.0 in stage 78.0 (TID 177). 1539 bytes result sent to driver
2017-05-04 08:53:43,673 WARN  [Executor task launch worker-20] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 48.27% for 14 writers
2017-05-04 08:53:43,674 INFO  [Executor task launch worker-30] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000030_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000030
2017-05-04 08:53:43,674 INFO  [Executor task launch worker-38] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000038_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000038
2017-05-04 08:53:43,674 INFO  [Executor task launch worker-32] executor.Executor (Logging.scala:logInfo(54)) - Finished task 32.0 in stage 78.0 (TID 187). 1539 bytes result sent to driver
2017-05-04 08:53:43,674 WARN  [Executor task launch worker-26] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 51.98% for 13 writers
2017-05-04 08:53:43,674 INFO  [Executor task launch worker-30] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000030_0: Committed
2017-05-04 08:53:43,674 INFO  [Executor task launch worker-38] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000038_0: Committed
2017-05-04 08:53:43,674 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 22.0 in stage 78.0 (TID 177) in 580 ms on localhost (executor driver) (22/40)
2017-05-04 08:53:43,674 WARN  [Executor task launch worker-21] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 56.31% for 12 writers
2017-05-04 08:53:43,675 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 32.0 in stage 78.0 (TID 187) in 576 ms on localhost (executor driver) (23/40)
2017-05-04 08:53:43,675 INFO  [Executor task launch worker-30] executor.Executor (Logging.scala:logInfo(54)) - Finished task 30.0 in stage 78.0 (TID 185). 1539 bytes result sent to driver
2017-05-04 08:53:43,675 INFO  [Executor task launch worker-38] executor.Executor (Logging.scala:logInfo(54)) - Finished task 38.0 in stage 78.0 (TID 193). 1539 bytes result sent to driver
2017-05-04 08:53:43,675 INFO  [Executor task launch worker-20] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000020_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000020
2017-05-04 08:53:43,675 INFO  [Executor task launch worker-26] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000026_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000026
2017-05-04 08:53:43,675 WARN  [Executor task launch worker-5] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 61.43% for 11 writers
2017-05-04 08:53:43,675 INFO  [Executor task launch worker-26] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000026_0: Committed
2017-05-04 08:53:43,675 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 30.0 in stage 78.0 (TID 185) in 577 ms on localhost (executor driver) (24/40)
2017-05-04 08:53:43,675 INFO  [Executor task launch worker-20] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000020_0: Committed
2017-05-04 08:53:43,676 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 38.0 in stage 78.0 (TID 193) in 573 ms on localhost (executor driver) (25/40)
2017-05-04 08:53:43,676 INFO  [Executor task launch worker-26] executor.Executor (Logging.scala:logInfo(54)) - Finished task 26.0 in stage 78.0 (TID 181). 1539 bytes result sent to driver
2017-05-04 08:53:43,676 INFO  [Executor task launch worker-21] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000021_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000021
2017-05-04 08:53:43,676 WARN  [Executor task launch worker-37] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 67.58% for 10 writers
2017-05-04 08:53:43,676 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 26.0 in stage 78.0 (TID 181) in 580 ms on localhost (executor driver) (26/40)
2017-05-04 08:53:43,676 WARN  [Executor task launch worker-18] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 75.08% for 9 writers
2017-05-04 08:53:43,677 INFO  [Executor task launch worker-5] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000005_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000005
2017-05-04 08:53:43,676 INFO  [Executor task launch worker-21] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000021_0: Committed
2017-05-04 08:53:43,676 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(54)) - Finished task 20.0 in stage 78.0 (TID 175). 1539 bytes result sent to driver
2017-05-04 08:53:43,677 WARN  [Executor task launch worker-10] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 84.47% for 8 writers
2017-05-04 08:53:43,677 INFO  [Executor task launch worker-5] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000005_0: Committed
2017-05-04 08:53:43,677 INFO  [Executor task launch worker-21] executor.Executor (Logging.scala:logInfo(54)) - Finished task 21.0 in stage 78.0 (TID 176). 1539 bytes result sent to driver
2017-05-04 08:53:43,677 WARN  [Executor task launch worker-29] hadoop.MemoryManager (Log.java:warn(172)) - Total allocation exceeds 95.00% (906,992,000 bytes) of heap memory
Scaling row group sizes to 96.54% for 7 writers
2017-05-04 08:53:43,678 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 20.0 in stage 78.0 (TID 175) in 585 ms on localhost (executor driver) (27/40)
2017-05-04 08:53:43,678 INFO  [Executor task launch worker-37] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000037_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000037
2017-05-04 08:53:43,678 INFO  [Executor task launch worker-5] executor.Executor (Logging.scala:logInfo(54)) - Finished task 5.0 in stage 78.0 (TID 160). 1539 bytes result sent to driver
2017-05-04 08:53:43,678 INFO  [Executor task launch worker-18] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000018_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000018
2017-05-04 08:53:43,678 INFO  [Executor task launch worker-37] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000037_0: Committed
2017-05-04 08:53:43,678 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 21.0 in stage 78.0 (TID 176) in 584 ms on localhost (executor driver) (28/40)
2017-05-04 08:53:43,678 INFO  [Executor task launch worker-18] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000018_0: Committed
2017-05-04 08:53:43,678 INFO  [Executor task launch worker-10] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000010_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000010
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-37] executor.Executor (Logging.scala:logInfo(54)) - Finished task 37.0 in stage 78.0 (TID 192). 1539 bytes result sent to driver
2017-05-04 08:53:43,679 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 5.0 in stage 78.0 (TID 160) in 593 ms on localhost (executor driver) (29/40)
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(54)) - Finished task 18.0 in stage 78.0 (TID 173). 1539 bytes result sent to driver
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-29] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000029_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000029
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-10] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000010_0: Committed
2017-05-04 08:53:43,679 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 37.0 in stage 78.0 (TID 192) in 578 ms on localhost (executor driver) (30/40)
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-29] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000029_0: Committed
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-31] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000031_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000031
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-2] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000002_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000002
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-31] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000031_0: Committed
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-29] executor.Executor (Logging.scala:logInfo(54)) - Finished task 29.0 in stage 78.0 (TID 184). 1539 bytes result sent to driver
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-7] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000007_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000007
2017-05-04 08:53:43,680 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 18.0 in stage 78.0 (TID 173) in 586 ms on localhost (executor driver) (31/40)
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-15] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000015_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000015
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-13] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000013_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000013
2017-05-04 08:53:43,681 INFO  [Executor task launch worker-13] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000013_0: Committed
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-39] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000039_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000039
2017-05-04 08:53:43,679 INFO  [Executor task launch worker-34] output.FileOutputCommitter (FileOutputCommitter.java:commitTask(535)) - Saved output of task 'attempt_20170504085343_0078_m_000034_0' to file:/usr/local/TrojanD/KMeansModel/data/_temporary/0/task_20170504085343_0078_m_000034
2017-05-04 08:53:43,681 INFO  [Executor task launch worker-39] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000039_0: Committed
2017-05-04 08:53:43,680 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 29.0 in stage 78.0 (TID 184) in 582 ms on localhost (executor driver) (32/40)
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-15] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000015_0: Committed
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-31] executor.Executor (Logging.scala:logInfo(54)) - Finished task 31.0 in stage 78.0 (TID 186). 1539 bytes result sent to driver
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-7] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000007_0: Committed
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(54)) - Finished task 10.0 in stage 78.0 (TID 165). 1539 bytes result sent to driver
2017-05-04 08:53:43,682 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 31.0 in stage 78.0 (TID 186) in 583 ms on localhost (executor driver) (33/40)
2017-05-04 08:53:43,680 INFO  [Executor task launch worker-2] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000002_0: Committed
2017-05-04 08:53:43,682 INFO  [Executor task launch worker-7] executor.Executor (Logging.scala:logInfo(54)) - Finished task 7.0 in stage 78.0 (TID 162). 1539 bytes result sent to driver
2017-05-04 08:53:43,682 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(54)) - Finished task 15.0 in stage 78.0 (TID 170). 1539 bytes result sent to driver
2017-05-04 08:53:43,681 INFO  [Executor task launch worker-39] executor.Executor (Logging.scala:logInfo(54)) - Finished task 39.0 in stage 78.0 (TID 194). 1539 bytes result sent to driver
2017-05-04 08:53:43,681 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(54)) - Finished task 13.0 in stage 78.0 (TID 168). 1539 bytes result sent to driver
2017-05-04 08:53:43,681 INFO  [Executor task launch worker-34] mapred.SparkHadoopMapRedUtil (Logging.scala:logInfo(54)) - attempt_20170504085343_0078_m_000034_0: Committed
2017-05-04 08:53:43,682 INFO  [Executor task launch worker-2] executor.Executor (Logging.scala:logInfo(54)) - Finished task 2.0 in stage 78.0 (TID 157). 1539 bytes result sent to driver
2017-05-04 08:53:43,682 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 10.0 in stage 78.0 (TID 165) in 594 ms on localhost (executor driver) (34/40)
2017-05-04 08:53:43,683 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Finished task 34.0 in stage 78.0 (TID 189). 1539 bytes result sent to driver
2017-05-04 08:53:43,683 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 7.0 in stage 78.0 (TID 162) in 596 ms on localhost (executor driver) (35/40)
2017-05-04 08:53:43,683 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 39.0 in stage 78.0 (TID 194) in 581 ms on localhost (executor driver) (36/40)
2017-05-04 08:53:43,683 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 15.0 in stage 78.0 (TID 170) in 592 ms on localhost (executor driver) (37/40)
2017-05-04 08:53:43,684 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 13.0 in stage 78.0 (TID 168) in 594 ms on localhost (executor driver) (38/40)
2017-05-04 08:53:43,684 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 2.0 in stage 78.0 (TID 157) in 600 ms on localhost (executor driver) (39/40)
2017-05-04 08:53:43,684 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 34.0 in stage 78.0 (TID 189) in 584 ms on localhost (executor driver) (40/40)
2017-05-04 08:53:43,684 INFO  [task-result-getter-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 78.0, whose tasks have all completed, from pool 
2017-05-04 08:53:43,684 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 78 (parquet at KMeansModel.scala:133) finished in 0.602 s
2017-05-04 08:53:43,684 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 43 finished: parquet at KMeansModel.scala:133, took 0.638003 s
2017-05-04 08:53:43,715 INFO  [main] datasources.FileFormatWriter (Logging.scala:logInfo(54)) - Job null committed.
2017-05-04 08:53:43,767 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_118 stored as values in memory (estimated size 238.4 KB, free 363.8 MB)
2017-05-04 08:53:43,778 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_118_piece0 stored as bytes in memory (estimated size 23.1 KB, free 363.8 MB)
2017-05-04 08:53:43,778 INFO  [dispatcher-event-loop-18] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_118_piece0 in memory on 192.168.0.101:42487 (size: 23.1 KB, free: 364.4 MB)
2017-05-04 08:53:43,779 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 118 from textFile at KmeansMLlibTrain.scala:28
2017-05-04 08:53:43,797 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_119 stored as values in memory (estimated size 238.4 KB, free 363.6 MB)
2017-05-04 08:53:43,807 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_119_piece0 stored as bytes in memory (estimated size 23.1 KB, free 363.6 MB)
2017-05-04 08:53:43,808 INFO  [dispatcher-event-loop-20] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_119_piece0 in memory on 192.168.0.101:42487 (size: 23.1 KB, free: 364.3 MB)
2017-05-04 08:53:43,808 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 119 from textFile at modelSaveLoad.scala:129
2017-05-04 08:53:43,824 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(249)) - Total input paths to process : 1
2017-05-04 08:53:43,833 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: first at modelSaveLoad.scala:129
2017-05-04 08:53:43,834 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 44 (first at modelSaveLoad.scala:129) with 1 output partitions
2017-05-04 08:53:43,834 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 79 (first at modelSaveLoad.scala:129)
2017-05-04 08:53:43,834 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:43,834 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:43,835 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 79 (file:/usr/local/TrojanD/KMeansModel/metadata MapPartitionsRDD[100] at textFile at modelSaveLoad.scala:129), which has no missing parents
2017-05-04 08:53:43,836 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_120 stored as values in memory (estimated size 3.3 KB, free 363.6 MB)
2017-05-04 08:53:43,837 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_120_piece0 stored as bytes in memory (estimated size 1978.0 B, free 363.6 MB)
2017-05-04 08:53:43,838 INFO  [dispatcher-event-loop-28] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_120_piece0 in memory on 192.168.0.101:42487 (size: 1978.0 B, free: 364.3 MB)
2017-05-04 08:53:43,838 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 120 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:43,838 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 1 missing tasks from ResultStage 79 (file:/usr/local/TrojanD/KMeansModel/metadata MapPartitionsRDD[100] at textFile at modelSaveLoad.scala:129)
2017-05-04 08:53:43,838 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 79.0 with 1 tasks
2017-05-04 08:53:43,839 INFO  [dispatcher-event-loop-23] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 79.0 (TID 195, localhost, executor driver, partition 0, PROCESS_LOCAL, 6073 bytes)
2017-05-04 08:53:43,840 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 79.0 (TID 195)
2017-05-04 08:53:43,841 INFO  [Executor task launch worker-34] rdd.HadoopRDD (Logging.scala:logInfo(54)) - Input split: file:/usr/local/TrojanD/KMeansModel/metadata/part-00000:0+40
2017-05-04 08:53:43,845 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 79.0 (TID 195). 1168 bytes result sent to driver
2017-05-04 08:53:43,846 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 79.0 (TID 195) in 7 ms on localhost (executor driver) (1/1)
2017-05-04 08:53:43,846 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 79.0, whose tasks have all completed, from pool 
2017-05-04 08:53:43,846 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 79 (first at modelSaveLoad.scala:129) finished in 0.007 s
2017-05-04 08:53:43,846 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 44 finished: first at modelSaveLoad.scala:129, took 0.012900 s
2017-05-04 08:53:43,916 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: parquet at KMeansModel.scala:143
2017-05-04 08:53:43,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 45 (parquet at KMeansModel.scala:143) with 1 output partitions
2017-05-04 08:53:43,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 80 (parquet at KMeansModel.scala:143)
2017-05-04 08:53:43,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:43,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:43,917 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 80 (MapPartitionsRDD[102] at parquet at KMeansModel.scala:143), which has no missing parents
2017-05-04 08:53:43,929 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_121 stored as values in memory (estimated size 71.1 KB, free 363.5 MB)
2017-05-04 08:53:43,931 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_121_piece0 stored as bytes in memory (estimated size 25.3 KB, free 363.5 MB)
2017-05-04 08:53:43,931 INFO  [dispatcher-event-loop-33] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_121_piece0 in memory on 192.168.0.101:42487 (size: 25.3 KB, free: 364.3 MB)
2017-05-04 08:53:43,932 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 121 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:43,932 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[102] at parquet at KMeansModel.scala:143)
2017-05-04 08:53:43,932 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 80.0 with 1 tasks
2017-05-04 08:53:43,933 INFO  [dispatcher-event-loop-29] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 80.0 (TID 196, localhost, executor driver, partition 0, PROCESS_LOCAL, 6230 bytes)
2017-05-04 08:53:43,933 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 80.0 (TID 196)
2017-05-04 08:53:43,944 INFO  [Executor task launch worker-34] hadoop.ParquetFileReader (Log.java:info(151)) - Initiating action with parallelism: 5
2017-05-04 08:53:43,961 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 80.0 (TID 196). 1905 bytes result sent to driver
2017-05-04 08:53:43,962 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 80.0 (TID 196) in 30 ms on localhost (executor driver) (1/1)
2017-05-04 08:53:43,962 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 80.0, whose tasks have all completed, from pool 
2017-05-04 08:53:43,962 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 80 (parquet at KMeansModel.scala:143) finished in 0.030 s
2017-05-04 08:53:43,962 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 45 finished: parquet at KMeansModel.scala:143, took 0.046045 s
2017-05-04 08:53:44,074 INFO  [main] datasources.FileSourceStrategy (Logging.scala:logInfo(54)) - Pruning directories with: 
2017-05-04 08:53:44,079 INFO  [main] datasources.FileSourceStrategy (Logging.scala:logInfo(54)) - Post-Scan Filters: 
2017-05-04 08:53:44,081 INFO  [main] datasources.FileSourceStrategy (Logging.scala:logInfo(54)) - Output Data Schema: struct<id: int, point: vector>
2017-05-04 08:53:44,082 INFO  [main] datasources.FileSourceStrategy (Logging.scala:logInfo(54)) - Pushed Filters: 
2017-05-04 08:53:44,149 INFO  [dispatcher-event-loop-30] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_121_piece0 on 192.168.0.101:42487 in memory (size: 25.3 KB, free: 364.3 MB)
2017-05-04 08:53:44,150 INFO  [dispatcher-event-loop-0] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_120_piece0 on 192.168.0.101:42487 in memory (size: 1978.0 B, free: 364.3 MB)
2017-05-04 08:53:44,151 INFO  [dispatcher-event-loop-6] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_119_piece0 on 192.168.0.101:42487 in memory (size: 23.1 KB, free: 364.4 MB)
2017-05-04 08:53:44,164 INFO  [main] codegen.CodeGenerator (Logging.scala:logInfo(54)) - Code generated in 58.993203 ms
2017-05-04 08:53:44,171 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_122 stored as values in memory (estimated size 282.3 KB, free 363.5 MB)
2017-05-04 08:53:44,184 INFO  [main] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_122_piece0 stored as bytes in memory (estimated size 24.7 KB, free 363.5 MB)
2017-05-04 08:53:44,185 INFO  [dispatcher-event-loop-3] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_122_piece0 in memory on 192.168.0.101:42487 (size: 24.7 KB, free: 364.3 MB)
2017-05-04 08:53:44,185 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 122 from rdd at KMeansModel.scala:145
2017-05-04 08:53:44,195 INFO  [main] execution.FileSourceScanExec (Logging.scala:logInfo(54)) - Planning scan with bin packing, max size: 4195444 bytes, open cost is considered as scanning 4194304 bytes.
2017-05-04 08:53:44,255 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collect at KMeansModel.scala:145
2017-05-04 08:53:44,256 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 46 (collect at KMeansModel.scala:145) with 40 output partitions
2017-05-04 08:53:44,256 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 81 (collect at KMeansModel.scala:145)
2017-05-04 08:53:44,256 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:44,256 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:44,256 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 81 (MapPartitionsRDD[107] at map at KMeansModel.scala:145), which has no missing parents
2017-05-04 08:53:44,262 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_123 stored as values in memory (estimated size 15.3 KB, free 363.5 MB)
2017-05-04 08:53:44,264 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_123_piece0 stored as bytes in memory (estimated size 7.0 KB, free 363.5 MB)
2017-05-04 08:53:44,264 INFO  [dispatcher-event-loop-1] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_123_piece0 in memory on 192.168.0.101:42487 (size: 7.0 KB, free: 364.3 MB)
2017-05-04 08:53:44,264 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 123 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:44,265 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 40 missing tasks from ResultStage 81 (MapPartitionsRDD[107] at map at KMeansModel.scala:145)
2017-05-04 08:53:44,265 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 81.0 with 40 tasks
2017-05-04 08:53:44,269 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 81.0 (TID 197, localhost, executor driver, partition 0, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,270 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 81.0 (TID 198, localhost, executor driver, partition 1, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,270 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 2.0 in stage 81.0 (TID 199, localhost, executor driver, partition 2, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,271 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 3.0 in stage 81.0 (TID 200, localhost, executor driver, partition 3, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,271 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 4.0 in stage 81.0 (TID 201, localhost, executor driver, partition 4, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,272 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 5.0 in stage 81.0 (TID 202, localhost, executor driver, partition 5, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,272 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 6.0 in stage 81.0 (TID 203, localhost, executor driver, partition 6, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,272 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 7.0 in stage 81.0 (TID 204, localhost, executor driver, partition 7, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,273 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 8.0 in stage 81.0 (TID 205, localhost, executor driver, partition 8, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,273 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 9.0 in stage 81.0 (TID 206, localhost, executor driver, partition 9, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,274 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 10.0 in stage 81.0 (TID 207, localhost, executor driver, partition 10, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,274 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 11.0 in stage 81.0 (TID 208, localhost, executor driver, partition 11, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,275 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 12.0 in stage 81.0 (TID 209, localhost, executor driver, partition 12, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,275 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 13.0 in stage 81.0 (TID 210, localhost, executor driver, partition 13, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,276 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 14.0 in stage 81.0 (TID 211, localhost, executor driver, partition 14, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,276 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 15.0 in stage 81.0 (TID 212, localhost, executor driver, partition 15, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,277 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 16.0 in stage 81.0 (TID 213, localhost, executor driver, partition 16, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,277 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 17.0 in stage 81.0 (TID 214, localhost, executor driver, partition 17, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,278 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 18.0 in stage 81.0 (TID 215, localhost, executor driver, partition 18, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,278 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 19.0 in stage 81.0 (TID 216, localhost, executor driver, partition 19, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,278 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 20.0 in stage 81.0 (TID 217, localhost, executor driver, partition 20, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,279 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 21.0 in stage 81.0 (TID 218, localhost, executor driver, partition 21, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,279 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 22.0 in stage 81.0 (TID 219, localhost, executor driver, partition 22, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,280 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 23.0 in stage 81.0 (TID 220, localhost, executor driver, partition 23, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,280 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 24.0 in stage 81.0 (TID 221, localhost, executor driver, partition 24, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,280 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 25.0 in stage 81.0 (TID 222, localhost, executor driver, partition 25, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,281 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 26.0 in stage 81.0 (TID 223, localhost, executor driver, partition 26, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,281 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 27.0 in stage 81.0 (TID 224, localhost, executor driver, partition 27, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,282 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 28.0 in stage 81.0 (TID 225, localhost, executor driver, partition 28, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,282 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 29.0 in stage 81.0 (TID 226, localhost, executor driver, partition 29, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,282 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 30.0 in stage 81.0 (TID 227, localhost, executor driver, partition 30, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,283 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 31.0 in stage 81.0 (TID 228, localhost, executor driver, partition 31, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,283 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 32.0 in stage 81.0 (TID 229, localhost, executor driver, partition 32, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,284 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 33.0 in stage 81.0 (TID 230, localhost, executor driver, partition 33, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,284 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 34.0 in stage 81.0 (TID 231, localhost, executor driver, partition 34, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,285 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 35.0 in stage 81.0 (TID 232, localhost, executor driver, partition 35, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,285 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 36.0 in stage 81.0 (TID 233, localhost, executor driver, partition 36, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,286 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 37.0 in stage 81.0 (TID 234, localhost, executor driver, partition 37, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,286 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 38.0 in stage 81.0 (TID 235, localhost, executor driver, partition 38, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,286 INFO  [dispatcher-event-loop-34] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 39.0 in stage 81.0 (TID 236, localhost, executor driver, partition 39, PROCESS_LOCAL, 6629 bytes)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 81.0 (TID 197)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-22] executor.Executor (Logging.scala:logInfo(54)) - Running task 18.0 in stage 81.0 (TID 215)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-3] executor.Executor (Logging.scala:logInfo(54)) - Running task 33.0 in stage 81.0 (TID 230)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-35] executor.Executor (Logging.scala:logInfo(54)) - Running task 39.0 in stage 81.0 (TID 236)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(54)) - Running task 38.0 in stage 81.0 (TID 235)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(54)) - Running task 36.0 in stage 81.0 (TID 233)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Running task 37.0 in stage 81.0 (TID 234)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-4] executor.Executor (Logging.scala:logInfo(54)) - Running task 34.0 in stage 81.0 (TID 231)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Running task 30.0 in stage 81.0 (TID 227)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(54)) - Running task 35.0 in stage 81.0 (TID 232)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(54)) - Running task 26.0 in stage 81.0 (TID 223)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-36] executor.Executor (Logging.scala:logInfo(54)) - Running task 22.0 in stage 81.0 (TID 219)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-9] executor.Executor (Logging.scala:logInfo(54)) - Running task 32.0 in stage 81.0 (TID 229)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-33] executor.Executor (Logging.scala:logInfo(54)) - Running task 31.0 in stage 81.0 (TID 228)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-27] executor.Executor (Logging.scala:logInfo(54)) - Running task 23.0 in stage 81.0 (TID 220)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(54)) - Running task 21.0 in stage 81.0 (TID 218)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(54)) - Running task 20.0 in stage 81.0 (TID 217)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-6] executor.Executor (Logging.scala:logInfo(54)) - Running task 28.0 in stage 81.0 (TID 225)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(54)) - Running task 27.0 in stage 81.0 (TID 224)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(54)) - Running task 29.0 in stage 81.0 (TID 226)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-8] executor.Executor (Logging.scala:logInfo(54)) - Running task 25.0 in stage 81.0 (TID 222)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-25] executor.Executor (Logging.scala:logInfo(54)) - Running task 24.0 in stage 81.0 (TID 221)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-32] executor.Executor (Logging.scala:logInfo(54)) - Running task 17.0 in stage 81.0 (TID 214)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-30] executor.Executor (Logging.scala:logInfo(54)) - Running task 16.0 in stage 81.0 (TID 213)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-28] executor.Executor (Logging.scala:logInfo(54)) - Running task 19.0 in stage 81.0 (TID 216)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-26] executor.Executor (Logging.scala:logInfo(54)) - Running task 14.0 in stage 81.0 (TID 211)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(54)) - Running task 2.0 in stage 81.0 (TID 199)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-37] executor.Executor (Logging.scala:logInfo(54)) - Running task 10.0 in stage 81.0 (TID 207)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-7] executor.Executor (Logging.scala:logInfo(54)) - Running task 5.0 in stage 81.0 (TID 202)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-38] executor.Executor (Logging.scala:logInfo(54)) - Running task 15.0 in stage 81.0 (TID 212)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(54)) - Running task 13.0 in stage 81.0 (TID 210)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-5] executor.Executor (Logging.scala:logInfo(54)) - Running task 11.0 in stage 81.0 (TID 208)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-31] executor.Executor (Logging.scala:logInfo(54)) - Running task 7.0 in stage 81.0 (TID 204)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-29] executor.Executor (Logging.scala:logInfo(54)) - Running task 8.0 in stage 81.0 (TID 205)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(54)) - Running task 4.0 in stage 81.0 (TID 201)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(54)) - Running task 9.0 in stage 81.0 (TID 206)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(54)) - Running task 6.0 in stage 81.0 (TID 203)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-2] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 81.0 (TID 198)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-21] executor.Executor (Logging.scala:logInfo(54)) - Running task 12.0 in stage 81.0 (TID 209)
2017-05-04 08:53:44,287 INFO  [Executor task launch worker-39] executor.Executor (Logging.scala:logInfo(54)) - Running task 3.0 in stage 81.0 (TID 200)
2017-05-04 08:53:44,333 INFO  [Executor task launch worker-25] codegen.CodeGenerator (Logging.scala:logInfo(54)) - Code generated in 19.004384 ms
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-36] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00008-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-7] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00021-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-18] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00031-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-5] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00002-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-26] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00029-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-1] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00030-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-39] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00037-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-34] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00010-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-30] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00014-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-29] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00007-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-3] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00020-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-4] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00025-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-33] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00027-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-38] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00004-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-17] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00024-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-24] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00003-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-22] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00012-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-10] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00005-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-32] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00000-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-31] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00034-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-28] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00032-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-20] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00013-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-13] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00018-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-0] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00006-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-35] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00022-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-6] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00011-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-23] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00033-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,341 INFO  [Executor task launch worker-25] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00019-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-12] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00016-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-15] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00039-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-27] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00035-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,339 INFO  [Executor task launch worker-21] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00026-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-16] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00036-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-14] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00028-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-8] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00017-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-11] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00038-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-19] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00009-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-9] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00001-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-936, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-37] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00015-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,342 INFO  [Executor task launch worker-2] datasources.FileScanRDD (Logging.scala:logInfo(54)) - Reading File path: file:///usr/local/TrojanD/KMeansModel/data/part-00023-5d008263-2406-43e2-bb6a-cf30b511c765.snappy.parquet, range: 0-1481, partition values: [empty row]
2017-05-04 08:53:44,351 WARN  [Executor task launch worker-33] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-5] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-12] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-8] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-15] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-16] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-30] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,353 WARN  [Executor task launch worker-36] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-25] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-29] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-27] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-26] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-17] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-34] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-14] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-18] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-21] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-1] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,357 WARN  [Executor task launch worker-10] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,357 WARN  [Executor task launch worker-32] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,356 WARN  [Executor task launch worker-31] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,356 WARN  [Executor task launch worker-28] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,355 WARN  [Executor task launch worker-3] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,355 WARN  [Executor task launch worker-13] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,355 WARN  [Executor task launch worker-0] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-35] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-7] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-38] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-6] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-39] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-20] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,354 WARN  [Executor task launch worker-4] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,361 WARN  [Executor task launch worker-9] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,360 WARN  [Executor task launch worker-37] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,360 WARN  [Executor task launch worker-2] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,358 WARN  [Executor task launch worker-23] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,358 WARN  [Executor task launch worker-24] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,358 WARN  [Executor task launch worker-22] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,361 WARN  [Executor task launch worker-11] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,361 WARN  [Executor task launch worker-19] hadoop.ParquetRecordReader (Log.java:error(193)) - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-28] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-7] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-30] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-24] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-36] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-3] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-9] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-33] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-19] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-20] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-34] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-25] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-32] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-15] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-29] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-26] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-31] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-14] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-4] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-2] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-23] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-13] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-38] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-21] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-16] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-39] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-1] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-11] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,380 INFO  [Executor task launch worker-22] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-12] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-27] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-6] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-37] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-18] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-10] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-0] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-8] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-17] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-35] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,376 INFO  [Executor task launch worker-5] parquet.ParquetReadSupport (Logging.scala:logInfo(54)) - Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 id;
  optional group point {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(id,IntegerType,true), StructField(point,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,true))
       
2017-05-04 08:53:44,436 INFO  [Executor task launch worker-3] codegen.CodeGenerator (Logging.scala:logInfo(54)) - Code generated in 30.486897 ms
2017-05-04 08:53:44,472 INFO  [Executor task launch worker-9] codegen.CodeGenerator (Logging.scala:logInfo(54)) - Code generated in 26.868059 ms
2017-05-04 08:53:44,481 INFO  [Executor task launch worker-16] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-19] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-33] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-8] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-28] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-0] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-29] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-35] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-10] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-27] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-4] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-9] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-20] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-30] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-17] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-26] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-1] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-32] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-14] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-34] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-39] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-24] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,483 INFO  [Executor task launch worker-23] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-18] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-31] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-7] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-22] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-11] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-12] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-6] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-3] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-5] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-36] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-25] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-13] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-38] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 0 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-15] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-2] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-21] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,482 INFO  [Executor task launch worker-37] hadoop.InternalParquetRecordReader (Log.java:info(151)) - RecordReader initialized will read a total of 1 records.
2017-05-04 08:53:44,493 INFO  [Executor task launch worker-15] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,496 INFO  [Executor task launch worker-34] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-2] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-18] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-26] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-7] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-5] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-20] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-37] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-13] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,497 INFO  [Executor task launch worker-21] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-31] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-36] executor.Executor (Logging.scala:logInfo(54)) - Finished task 22.0 in stage 81.0 (TID 219). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-33] executor.Executor (Logging.scala:logInfo(54)) - Finished task 31.0 in stage 81.0 (TID 228). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-29] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-25] executor.Executor (Logging.scala:logInfo(54)) - Finished task 24.0 in stage 81.0 (TID 221). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-38] executor.Executor (Logging.scala:logInfo(54)) - Finished task 15.0 in stage 81.0 (TID 212). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(54)) - Finished task 30.0 in stage 81.0 (TID 227). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-32] executor.Executor (Logging.scala:logInfo(54)) - Finished task 17.0 in stage 81.0 (TID 214). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-28] executor.Executor (Logging.scala:logInfo(54)) - Finished task 19.0 in stage 81.0 (TID 216). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(54)) - Finished task 20.0 in stage 81.0 (TID 217). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-24] executor.Executor (Logging.scala:logInfo(54)) - Finished task 35.0 in stage 81.0 (TID 232). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-39] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-10] hadoop.InternalParquetRecordReader (Log.java:info(151)) - at row 0. reading next block
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-12] executor.Executor (Logging.scala:logInfo(54)) - Finished task 29.0 in stage 81.0 (TID 226). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-22] executor.Executor (Logging.scala:logInfo(54)) - Finished task 18.0 in stage 81.0 (TID 215). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-30] executor.Executor (Logging.scala:logInfo(54)) - Finished task 16.0 in stage 81.0 (TID 213). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-11] executor.Executor (Logging.scala:logInfo(54)) - Finished task 36.0 in stage 81.0 (TID 233). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-4] executor.Executor (Logging.scala:logInfo(54)) - Finished task 34.0 in stage 81.0 (TID 231). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-9] executor.Executor (Logging.scala:logInfo(54)) - Finished task 32.0 in stage 81.0 (TID 229). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-3] executor.Executor (Logging.scala:logInfo(54)) - Finished task 33.0 in stage 81.0 (TID 230). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(54)) - Finished task 27.0 in stage 81.0 (TID 224). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-27] executor.Executor (Logging.scala:logInfo(54)) - Finished task 23.0 in stage 81.0 (TID 220). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-23] executor.Executor (Logging.scala:logInfo(54)) - Finished task 21.0 in stage 81.0 (TID 218). 1438 bytes result sent to driver
2017-05-04 08:53:44,499 INFO  [Executor task launch worker-6] executor.Executor (Logging.scala:logInfo(54)) - Finished task 28.0 in stage 81.0 (TID 225). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-8] executor.Executor (Logging.scala:logInfo(54)) - Finished task 25.0 in stage 81.0 (TID 222). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 22.0 in stage 81.0 (TID 219) in 221 ms on localhost (executor driver) (1/40)
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(54)) - Finished task 37.0 in stage 81.0 (TID 234). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-19] executor.Executor (Logging.scala:logInfo(54)) - Finished task 38.0 in stage 81.0 (TID 235). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-35] executor.Executor (Logging.scala:logInfo(54)) - Finished task 39.0 in stage 81.0 (TID 236). 1438 bytes result sent to driver
2017-05-04 08:53:44,500 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(54)) - Finished task 26.0 in stage 81.0 (TID 223). 1438 bytes result sent to driver
2017-05-04 08:53:44,502 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 31.0 in stage 81.0 (TID 228) in 219 ms on localhost (executor driver) (2/40)
2017-05-04 08:53:44,503 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 24.0 in stage 81.0 (TID 221) in 223 ms on localhost (executor driver) (3/40)
2017-05-04 08:53:44,503 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 30.0 in stage 81.0 (TID 227) in 221 ms on localhost (executor driver) (4/40)
2017-05-04 08:53:44,504 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 19.0 in stage 81.0 (TID 216) in 226 ms on localhost (executor driver) (5/40)
2017-05-04 08:53:44,504 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 15.0 in stage 81.0 (TID 212) in 228 ms on localhost (executor driver) (6/40)
2017-05-04 08:53:44,504 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 17.0 in stage 81.0 (TID 214) in 227 ms on localhost (executor driver) (7/40)
2017-05-04 08:53:44,504 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 20.0 in stage 81.0 (TID 217) in 226 ms on localhost (executor driver) (8/40)
2017-05-04 08:53:44,504 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 35.0 in stage 81.0 (TID 232) in 220 ms on localhost (executor driver) (9/40)
2017-05-04 08:53:44,505 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 29.0 in stage 81.0 (TID 226) in 222 ms on localhost (executor driver) (10/40)
2017-05-04 08:53:44,505 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 18.0 in stage 81.0 (TID 215) in 228 ms on localhost (executor driver) (11/40)
2017-05-04 08:53:44,505 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 16.0 in stage 81.0 (TID 213) in 229 ms on localhost (executor driver) (12/40)
2017-05-04 08:53:44,505 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 36.0 in stage 81.0 (TID 233) in 220 ms on localhost (executor driver) (13/40)
2017-05-04 08:53:44,506 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 34.0 in stage 81.0 (TID 231) in 221 ms on localhost (executor driver) (14/40)
2017-05-04 08:53:44,506 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 32.0 in stage 81.0 (TID 229) in 223 ms on localhost (executor driver) (15/40)
2017-05-04 08:53:44,506 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 33.0 in stage 81.0 (TID 230) in 223 ms on localhost (executor driver) (16/40)
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-31] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-5] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-21] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-20] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-18] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-26] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-10] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-7] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-15] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-39] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-13] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-37] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-2] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,507 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 23.0 in stage 81.0 (TID 220) in 228 ms on localhost (executor driver) (17/40)
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-29] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-39] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 10 ms. row count = 1
2017-05-04 08:53:44,507 INFO  [Executor task launch worker-34] compress.CodecPool (CodecPool.java:getDecompressor(181)) - Got brand-new decompressor [.snappy]
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-13] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 12 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-15] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 16 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-2] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 13 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-37] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 12 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-31] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 11 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-21] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 12 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-10] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 9 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-18] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 13 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-20] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 12 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-26] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 13 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 27.0 in stage 81.0 (TID 224) in 229 ms on localhost (executor driver) (18/40)
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-5] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 12 ms. row count = 1
2017-05-04 08:53:44,512 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 21.0 in stage 81.0 (TID 218) in 233 ms on localhost (executor driver) (19/40)
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-34] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 14 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-29] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 11 ms. row count = 1
2017-05-04 08:53:44,510 INFO  [Executor task launch worker-7] hadoop.InternalParquetRecordReader (Log.java:info(151)) - block read in memory in 13 ms. row count = 1
2017-05-04 08:53:44,513 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 28.0 in stage 81.0 (TID 225) in 232 ms on localhost (executor driver) (20/40)
2017-05-04 08:53:44,513 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 38.0 in stage 81.0 (TID 235) in 227 ms on localhost (executor driver) (21/40)
2017-05-04 08:53:44,513 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 39.0 in stage 81.0 (TID 236) in 227 ms on localhost (executor driver) (22/40)
2017-05-04 08:53:44,514 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 37.0 in stage 81.0 (TID 234) in 229 ms on localhost (executor driver) (23/40)
2017-05-04 08:53:44,514 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 25.0 in stage 81.0 (TID 222) in 234 ms on localhost (executor driver) (24/40)
2017-05-04 08:53:44,514 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 26.0 in stage 81.0 (TID 223) in 233 ms on localhost (executor driver) (25/40)
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-39] executor.Executor (Logging.scala:logInfo(54)) - Finished task 3.0 in stage 81.0 (TID 200). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-2] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 81.0 (TID 198). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-13] executor.Executor (Logging.scala:logInfo(54)) - Finished task 2.0 in stage 81.0 (TID 199). 1801 bytes result sent to driver
2017-05-04 08:53:44,546 INFO  [Executor task launch worker-37] executor.Executor (Logging.scala:logInfo(54)) - Finished task 10.0 in stage 81.0 (TID 207). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(54)) - Finished task 4.0 in stage 81.0 (TID 201). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-18] executor.Executor (Logging.scala:logInfo(54)) - Finished task 9.0 in stage 81.0 (TID 206). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-20] executor.Executor (Logging.scala:logInfo(54)) - Finished task 13.0 in stage 81.0 (TID 210). 1801 bytes result sent to driver
2017-05-04 08:53:44,546 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 3.0 in stage 81.0 (TID 200) in 276 ms on localhost (executor driver) (26/40)
2017-05-04 08:53:44,546 INFO  [Executor task launch worker-7] executor.Executor (Logging.scala:logInfo(54)) - Finished task 5.0 in stage 81.0 (TID 202). 1801 bytes result sent to driver
2017-05-04 08:53:44,546 INFO  [Executor task launch worker-34] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 81.0 (TID 197). 1801 bytes result sent to driver
2017-05-04 08:53:44,546 INFO  [Executor task launch worker-31] executor.Executor (Logging.scala:logInfo(54)) - Finished task 7.0 in stage 81.0 (TID 204). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-29] executor.Executor (Logging.scala:logInfo(54)) - Finished task 8.0 in stage 81.0 (TID 205). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-5] executor.Executor (Logging.scala:logInfo(54)) - Finished task 11.0 in stage 81.0 (TID 208). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-10] executor.Executor (Logging.scala:logInfo(54)) - Finished task 6.0 in stage 81.0 (TID 203). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-21] executor.Executor (Logging.scala:logInfo(54)) - Finished task 12.0 in stage 81.0 (TID 209). 1801 bytes result sent to driver
2017-05-04 08:53:44,545 INFO  [Executor task launch worker-26] executor.Executor (Logging.scala:logInfo(54)) - Finished task 14.0 in stage 81.0 (TID 211). 1801 bytes result sent to driver
2017-05-04 08:53:44,546 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 81.0 (TID 198) in 277 ms on localhost (executor driver) (27/40)
2017-05-04 08:53:44,547 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 2.0 in stage 81.0 (TID 199) in 277 ms on localhost (executor driver) (28/40)
2017-05-04 08:53:44,547 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 10.0 in stage 81.0 (TID 207) in 273 ms on localhost (executor driver) (29/40)
2017-05-04 08:53:44,547 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 4.0 in stage 81.0 (TID 201) in 276 ms on localhost (executor driver) (30/40)
2017-05-04 08:53:44,548 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 9.0 in stage 81.0 (TID 206) in 275 ms on localhost (executor driver) (31/40)
2017-05-04 08:53:44,548 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 13.0 in stage 81.0 (TID 210) in 273 ms on localhost (executor driver) (32/40)
2017-05-04 08:53:44,548 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 5.0 in stage 81.0 (TID 202) in 277 ms on localhost (executor driver) (33/40)
2017-05-04 08:53:44,548 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 7.0 in stage 81.0 (TID 204) in 276 ms on localhost (executor driver) (34/40)
2017-05-04 08:53:44,548 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 81.0 (TID 197) in 282 ms on localhost (executor driver) (35/40)
2017-05-04 08:53:44,549 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 8.0 in stage 81.0 (TID 205) in 276 ms on localhost (executor driver) (36/40)
2017-05-04 08:53:44,549 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 11.0 in stage 81.0 (TID 208) in 275 ms on localhost (executor driver) (37/40)
2017-05-04 08:53:44,549 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 6.0 in stage 81.0 (TID 203) in 277 ms on localhost (executor driver) (38/40)
2017-05-04 08:53:44,549 INFO  [task-result-getter-3] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 14.0 in stage 81.0 (TID 211) in 274 ms on localhost (executor driver) (39/40)
2017-05-04 08:53:44,549 INFO  [task-result-getter-1] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 12.0 in stage 81.0 (TID 209) in 275 ms on localhost (executor driver) (40/40)
2017-05-04 08:53:44,549 INFO  [task-result-getter-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 81.0, whose tasks have all completed, from pool 
2017-05-04 08:53:44,549 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 81 (collect at KMeansModel.scala:145) finished in 0.283 s
2017-05-04 08:53:44,550 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 46 finished: collect at KMeansModel.scala:145, took 0.294895 s
2017-05-04 08:53:44,552 INFO  [main] mapred.FileInputFormat (FileInputFormat.java:listStatus(249)) - Total input paths to process : 1
2017-05-04 08:53:44,556 INFO  [main] spark.SparkContext (Logging.scala:logInfo(54)) - Starting job: collect at KmeansMLlibTrain.scala:34
2017-05-04 08:53:44,556 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Got job 47 (collect at KmeansMLlibTrain.scala:34) with 2 output partitions
2017-05-04 08:53:44,556 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Final stage: ResultStage 82 (collect at KmeansMLlibTrain.scala:34)
2017-05-04 08:53:44,557 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Parents of final stage: List()
2017-05-04 08:53:44,557 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Missing parents: List()
2017-05-04 08:53:44,557 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting ResultStage 82 (MapPartitionsRDD[98] at map at KmeansMLlibTrain.scala:29), which has no missing parents
2017-05-04 08:53:44,558 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_124 stored as values in memory (estimated size 3.4 KB, free 363.5 MB)
2017-05-04 08:53:44,560 INFO  [dag-scheduler-event-loop] memory.MemoryStore (Logging.scala:logInfo(54)) - Block broadcast_124_piece0 stored as bytes in memory (estimated size 2.0 KB, free 363.5 MB)
2017-05-04 08:53:44,560 INFO  [dispatcher-event-loop-4] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added broadcast_124_piece0 in memory on 192.168.0.101:42487 (size: 2.0 KB, free: 364.3 MB)
2017-05-04 08:53:44,561 INFO  [dag-scheduler-event-loop] spark.SparkContext (Logging.scala:logInfo(54)) - Created broadcast 124 from broadcast at DAGScheduler.scala:996
2017-05-04 08:53:44,561 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Submitting 2 missing tasks from ResultStage 82 (MapPartitionsRDD[98] at map at KmeansMLlibTrain.scala:29)
2017-05-04 08:53:44,561 INFO  [dag-scheduler-event-loop] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Adding task set 82.0 with 2 tasks
2017-05-04 08:53:44,562 INFO  [dispatcher-event-loop-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 0.0 in stage 82.0 (TID 237, localhost, executor driver, partition 0, PROCESS_LOCAL, 6060 bytes)
2017-05-04 08:53:44,562 INFO  [dispatcher-event-loop-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Starting task 1.0 in stage 82.0 (TID 238, localhost, executor driver, partition 1, PROCESS_LOCAL, 6060 bytes)
2017-05-04 08:53:44,563 INFO  [Executor task launch worker-26] executor.Executor (Logging.scala:logInfo(54)) - Running task 0.0 in stage 82.0 (TID 237)
2017-05-04 08:53:44,563 INFO  [Executor task launch worker-21] executor.Executor (Logging.scala:logInfo(54)) - Running task 1.0 in stage 82.0 (TID 238)
2017-05-04 08:53:44,564 INFO  [Executor task launch worker-21] rdd.HadoopRDD (Logging.scala:logInfo(54)) - Input split: file:/usr/local/TrojanD/sample/MLlib.txt:1382329+1382330
2017-05-04 08:53:44,564 INFO  [Executor task launch worker-26] rdd.HadoopRDD (Logging.scala:logInfo(54)) - Input split: file:/usr/local/TrojanD/sample/MLlib.txt:0+1382329
2017-05-04 08:53:44,686 INFO  [Executor task launch worker-21] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_98_1 stored as values in memory (estimated size 952.4 KB, free 362.6 MB)
2017-05-04 08:53:44,687 INFO  [dispatcher-event-loop-15] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_98_1 in memory on 192.168.0.101:42487 (size: 952.4 KB, free: 363.4 MB)
2017-05-04 08:53:44,692 INFO  [Executor task launch worker-26] memory.MemoryStore (Logging.scala:logInfo(54)) - Block rdd_98_0 stored as values in memory (estimated size 952.4 KB, free 361.6 MB)
2017-05-04 08:53:44,692 INFO  [dispatcher-event-loop-11] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Added rdd_98_0 in memory on 192.168.0.101:42487 (size: 952.4 KB, free: 362.5 MB)
2017-05-04 08:53:44,703 INFO  [Executor task launch worker-21] executor.Executor (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 82.0 (TID 238). 770294 bytes result sent to driver
2017-05-04 08:53:44,708 INFO  [Executor task launch worker-26] executor.Executor (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 82.0 (TID 237). 770294 bytes result sent to driver
2017-05-04 08:53:44,722 INFO  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 1.0 in stage 82.0 (TID 238) in 160 ms on localhost (executor driver) (1/2)
2017-05-04 08:53:44,727 INFO  [task-result-getter-0] scheduler.TaskSetManager (Logging.scala:logInfo(54)) - Finished task 0.0 in stage 82.0 (TID 237) in 165 ms on localhost (executor driver) (2/2)
2017-05-04 08:53:44,727 INFO  [task-result-getter-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(54)) - Removed TaskSet 82.0, whose tasks have all completed, from pool 
2017-05-04 08:53:44,727 INFO  [dag-scheduler-event-loop] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - ResultStage 82 (collect at KmeansMLlibTrain.scala:34) finished in 0.166 s
2017-05-04 08:53:44,728 INFO  [main] scheduler.DAGScheduler (Logging.scala:logInfo(54)) - Job 47 finished: collect at KmeansMLlibTrain.scala:34, took 0.171618 s
2017-05-04 08:53:44,775 INFO  [dispatcher-event-loop-25] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_124_piece0 on 192.168.0.101:42487 in memory (size: 2.0 KB, free: 362.5 MB)
2017-05-04 08:53:44,776 INFO  [dispatcher-event-loop-17] storage.BlockManagerInfo (Logging.scala:logInfo(54)) - Removed broadcast_123_piece0 on 192.168.0.101:42487 in memory (size: 7.0 KB, free: 362.5 MB)
Spark MLlib K-means clustering test finished.
2017-05-04 08:53:44,806 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(54)) - Invoking stop() from shutdown hook
2017-05-04 08:53:44,813 INFO  [Thread-2] server.ServerConnector (AbstractConnector.java:doStop(306)) - Stopped ServerConnector@1a38ba58{HTTP/1.1}{0.0.0.0:4040}
2017-05-04 08:53:44,816 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@69c43e48{/stages/stage/kill,null,UNAVAILABLE}
2017-05-04 08:53:44,817 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@3caa4757{/jobs/job/kill,null,UNAVAILABLE}
2017-05-04 08:53:44,817 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@2f40a43{/api,null,UNAVAILABLE}
2017-05-04 08:53:44,817 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@7cd1ac19{/,null,UNAVAILABLE}
2017-05-04 08:53:44,817 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@5851bd4f{/static,null,UNAVAILABLE}
2017-05-04 08:53:44,818 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@46cf05f7{/executors/threadDump/json,null,UNAVAILABLE}
2017-05-04 08:53:44,818 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@29caf222{/executors/threadDump,null,UNAVAILABLE}
2017-05-04 08:53:44,818 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@1d572e62{/executors/json,null,UNAVAILABLE}
2017-05-04 08:53:44,818 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@2b46a8c1{/executors,null,UNAVAILABLE}
2017-05-04 08:53:44,818 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@22db8f4{/environment/json,null,UNAVAILABLE}
2017-05-04 08:53:44,819 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@581d969c{/environment,null,UNAVAILABLE}
2017-05-04 08:53:44,819 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@30c0ccff{/storage/rdd/json,null,UNAVAILABLE}
2017-05-04 08:53:44,819 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@37d3d232{/storage/rdd,null,UNAVAILABLE}
2017-05-04 08:53:44,819 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@4593ff34{/storage/json,null,UNAVAILABLE}
2017-05-04 08:53:44,820 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@27f9e982{/storage,null,UNAVAILABLE}
2017-05-04 08:53:44,820 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@2d35442b{/stages/pool/json,null,UNAVAILABLE}
2017-05-04 08:53:44,820 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@ecf9fb3{/stages/pool,null,UNAVAILABLE}
2017-05-04 08:53:44,820 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@329a1243{/stages/stage/json,null,UNAVAILABLE}
2017-05-04 08:53:44,820 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@2575f671{/stages/stage,null,UNAVAILABLE}
2017-05-04 08:53:44,821 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@1fc793c2{/stages/json,null,UNAVAILABLE}
2017-05-04 08:53:44,821 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@433ffad1{/stages,null,UNAVAILABLE}
2017-05-04 08:53:44,821 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@72bca894{/jobs/job/json,null,UNAVAILABLE}
2017-05-04 08:53:44,821 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@503d56b5{/jobs/job,null,UNAVAILABLE}
2017-05-04 08:53:44,822 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@324dcd31{/jobs/json,null,UNAVAILABLE}
2017-05-04 08:53:44,822 INFO  [Thread-2] handler.ContextHandler (ContextHandler.java:doStop(865)) - Stopped o.s.j.s.ServletContextHandler@39c11e6c{/jobs,null,UNAVAILABLE}
2017-05-04 08:53:44,824 INFO  [Thread-2] ui.SparkUI (Logging.scala:logInfo(54)) - Stopped Spark web UI at http://192.168.0.101:4040
2017-05-04 08:53:44,848 INFO  [dispatcher-event-loop-28] spark.MapOutputTrackerMasterEndpoint (Logging.scala:logInfo(54)) - MapOutputTrackerMasterEndpoint stopped!
2017-05-04 08:53:44,864 INFO  [Thread-2] memory.MemoryStore (Logging.scala:logInfo(54)) - MemoryStore cleared
2017-05-04 08:53:44,865 INFO  [Thread-2] storage.BlockManager (Logging.scala:logInfo(54)) - BlockManager stopped
2017-05-04 08:53:44,867 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(54)) - BlockManagerMaster stopped
2017-05-04 08:53:44,870 INFO  [dispatcher-event-loop-39] scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint (Logging.scala:logInfo(54)) - OutputCommitCoordinator stopped!
2017-05-04 08:53:44,876 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(54)) - Successfully stopped SparkContext
2017-05-04 08:53:44,877 INFO  [Thread-2] util.ShutdownHookManager (Logging.scala:logInfo(54)) - Shutdown hook called
2017-05-04 08:53:44,878 INFO  [Thread-2] util.ShutdownHookManager (Logging.scala:logInfo(54)) - Deleting directory /tmp/spark-c0c6e657-a403-4866-9dfd-f94efbc27ff4
